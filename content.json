{"pages":[],"posts":[{"title":"抽象代数续论笔记 01 群、幺半群、可解群以及一些例子","text":"因为这门课是续论课，所以对于很多基本的概念性内容都报以回顾性的态度，在这里也同样会过得很快，简单的证明也会略过。第一次课回顾了群的一些基本概念，给出了一些例子；定义了可解群且给出了一个矩阵群中的例子。 这门课的思路比较混乱，因为是一门拾遗类型的课程，主要是起到衔接的作用，因此这里的笔记也不算太有条理。 基本定义和例子我们称满足以下三条公理的带有二元运算的集合 (G, \\cdot) 为一个群： 二元运算满足结合律； 存在单位元； 每个元素都存在逆元； 如果只满足上面两条公理，则称其为幺半群（monoid）。容易表明，幺半群中的单位元具备唯一性。根据二元运算是否满足交换律，我们将幺半群分为交换的与非交换的；如果幺半群 $M$ 的一个子集 $S$ 含有单位元并对二元运算封闭，则称其为一个子幺半群（submonoid），子群的定义与之类似。 很容易给出子幺半群的一些例子，例如，线性空间中 $V$ 上的自同构群 $\\mathrm{Aut}(V)$ （也记作一般线性群 $GL(V)$）是其自同态幺半群 $\\mathrm{End}(V)$ 的一个子幺半群。 我们称幺半群间的映射 $\\varphi: M_1 \\to M_2$ 为一个幺半群同态，若其保乘法、保单位元；若 $M_1$ 和 $M_2$ 都是群，也将其称为一个群同态。 一个例子是域 $K$ 上的线性空间 $V$ 的自同态群 $\\mathrm{End}V$ 到视作幺半群的 $(K, \\cdot)$ 的一个幺半群同态行列式运算 $\\det$。注意，这里的 $(K, \\cdot)$ 不是群，因为 $0$ 没有乘法逆元。 我们可以定义一个群结构 $M(S, G)$，其元素是从非空集合 $S$ 到群 $G$ 的映射全体，对于任意 $f, g \\in M(S, G)$，我们定义 $(fg)(a) = f(a)g(a)$，称其为 pointwise multiplication。当然，它的乘法单位元就是将 $S$ 中的所有元素映到 $G$ 的单位元，映射 $f$ 的逆元定义为 $(f^{-1})(a) = (f(a))^{-1}$。 这个定义是为了什么引入的来着？ 置换群也是一类很常见的群，设 $S$ 为一有限集合，记 $\\mathrm{Perm}(S) = \\{f: S \\to S \\vert f 是双射\\}$。其中的复合、单位元和逆元的定义都是简单的。很显然，若 $G$ 为一群，$\\mathrm{Aut} (G)$ 是 $\\mathrm{Perm} (G)$ 的子群。 对域 $K$ 上的线性空间 $V$，我们显然可以定义一个群同构 $GL(n, K) \\to GL(V)$，其中 $n = \\dim_KV$。 我们可以建立正多边形的自同构，设 $A_n$ 为正 $n$ 边形，我们记 $\\mathrm{Aut}(A_n) = \\{\\varphi \\vert \\varphi(A_n) = A_n, \\varphi 为等距映射\\}$，我们称这样的群为二面体群，记作 $D_{2n}$，有些书也会记作 $D_n$。任何 $n \\leqslant 3$ 的二面体群 $D_{n}$ 都可表成 $\\langle \\sigma \\tau \\vert \\sigma^n = 1, \\tau^2 = 1, \\tau \\sigma \\tau^{-1} = \\sigma^{-1}\\rangle$ 的形式，称为 $n$ 阶二面体群。 类似地我们可以定义 $\\mathbb{R}^2$ 上的等距同构，对于一个距离 $d$，我们记 $\\mathrm{Aut}(\\mathbb{R^2}, d)$ 为 $\\mathbb{R}$ 上的等距同构全体。我们也已经知道，它可以被写成旋转群 $O(\\mathbb R^2)$ 和平移群 $T(\\mathbb R^2)$ 的半直积（semi-direct product）。 下一个例子是关于多项式环 $K[x, y]$ 上的自同构群。设 $\\varphi: K[x, y] \\to K[x, y]$ 是一个线性保持乘法的双射，将 $x$ 映到 $f(x, y)$, 将 $y$ 映到 $g(x, y)$，则一个将 $h(x, y)$ 映到 $h(f, g)$ 的映射是一个自同态。它是否是一个自同构是一个开放问题，一般称为 Jacobi 猜想或者 Keller 问题。 很容易定义群的直积的概念，给定群 $G, H$，称它们的直积为一个笛卡尔积 $G \\times H$，其上的二元运算定义为每一个分量分别做运算。群的生成元的定义在此也不再赘述。 接下来考虑一个很常见的群，四元数群。其中有八个元素 $\\{\\pm 1, \\pm i, \\pm j, \\pm k\\}$。其乘法定义为： ij=k=-ji, jk=i=-kj, ki=j=-ik, i^2=j^2=k^2=-1它是八阶群中的一个，容易证明八阶群一共有五个，分别是 $C_8, C_2 \\times C_4, C_2^3, D_8$ 和四元数群。 四元数群在 $\\mathbb R$ 上张成一个四维线性空间，其中 $1, i, j, k$ 构成了标准正交基，四元数群是其中单位球面群的子群。其上的实内积定义为 $(a, b) = \\Re(a\\bar b)$，由此内积定义的范数是可乘的。我们在后面还会再次碰到四元数群。 我们定义一个群 $G$ 的内自同构群 $\\mathrm{Inn}(G)$。首先定义 $G$ 上关于元素 $a$ 的共轭，记作 $C_a: G \\to G$，它将每个元素 $g$ 映到 $aga^{-1}$。容易证明，$\\varphi: G \\to \\mathrm{Aut}(G)$ 为一个同态，记 $\\mathrm{Inn}(G) = \\mathrm{Im}(G)$，不难证明其为 $\\mathrm{Aut}(G)$ 的正规子群。同样容易表明，如果 $G$ 是交换群，那么 $\\mathrm{Inn}(G)$ 为平凡群。 自同态的概念不再赘述，可以观察到以下结果：对于有理数加群 $\\mathbb{Q}$，把 $a$ 映到 $na$ 的映射 $\\varphi_n$ 是一个自同构，而对于整数加群 $\\mathbb Z$，这只是一个自同态。 考虑投影 $P_1: G_1 \\times G_2 \\to G_1$ 和 $P_2: G_1 \\times G_2 \\to G_2$，很显然，它们都是满射，而且它们的核 $\\mathrm{Ker} P_i \\simeq G_{2-i}$。 在这里可以补叙 Goursat 定理，它出现在了作业题中（Algebra, S. Lang, p75 Ex. 5） 设 $G$，$G’$ 为群，$H$ 为 $G \\times G’$ 的子群，且满足投影 $P_1: H \\to G$ 和 $P_2: H \\to G’$ 都是满射。设 $N = \\mathrm{Ker}(P_2), N’ = \\mathrm{Ker}(P_1)$，容易表明，$N, N’$ 分别同构于 $G, G’$ 的一个正规子群，则 $H$ 在 $G/N \\times G’/N’$ 中的图像是 $G/N$ 到 $G’/N’$ 的一个同构。 接下来我们给出一个类似于半直积的构造的命题：设 $G$ 为一群，$H, K$ 为其子群，$H \\cap K = \\{e\\}$，$HK = G$，且 $\\forall x \\in H, y \\in K, xy=yx$，则 $\\varphi: H \\times K \\to G$，$\\varphi(x, y) = xy$ 为一个同构。 首先证明它是一个群同态。很显然：\\begin{align}\\varphi(x_1, y_1)\\varphi(x_2, y_2) = x_1y_1x_2y_2 =x_1x_2y_1y_2 =\\varphi(x_1x_2, y_1y_2)\\end{align}这里利用了上面给出的可交换性； 它是满射直接由 $HK=G$ 得出，然后表明它是单射，考虑它的核中的元素 $(x, y)$：\\begin{align}\\varphi(x, y) = xy = e \\Rightarrow x=y^{-1} \\in H \\cap K \\Rightarrow x=y=e \\Rightarrow \\mathrm{Ker}(\\varphi) = {e}\\end{align} 由此，$\\varphi$ 是一个满射。 有一个看起来比较优雅的推论：设 $H \\triangleleft G, K \\triangleleft G, H \\cap K = \\{e\\}$，则 $H \\times K \\to G$ 为一个嵌入。 如上我们只要证明交换性：\\begin{align}hkh^{-1}k^{-1} = (hkh^{-1})k^{-1} = h(kh^{-1}k^{-1}) \\in H \\cap K \\Rightarrow hkh^{-1}k^{-1} = e \\Rightarrow hk=kh\\end{align} 这个证明很自然地体现了一个问题，即 $aba^{-1}b^{-1}$ 事实上刻画了一个群的“交换性”。延续这个思路可以引入换位子群（或导群）的概念，记 $G’ = \\{aba^{-1}b^{-1} \\mid a, b \\in G\\}$。 首先，可以表明它是群 $G$ 的一个正规子群：\\begin{align}\\forall a \\in G’, g \\in G, gag^{-1} = (gag^{-1}a^{-1})a \\in G’\\end{align} 接下来，很容易表明，$G/G’$ 是一个交换群：\\begin{align}\\forall gG’, hG’ \\in G/G’, (hg)^{-1}(gh) = g^{-1}h^{-1}gh \\in G’ \\Rightarrow ghG’ = hgG’\\end{align} 最后，读者当不难自证，设 $N \\triangleleft G$，若 $G/N$ 交换，则 $G’ \\subseteq N$。 这个群在表示论和 Galois 理论等方面都有应用，在此稍作叙述。 接下来对于陪集、商群、指标和阶的定义都直接略去，Lagrange 定理读者应当也已经熟悉。正规子群的交和乘仍然是正规子群，这个结论也不再证明。 设 $S$ 为群 $G$ 的一个子集。称 $N_G(S) = \\{g \\in G \\mid gSg^{-1} = S\\}$ 为群 $G$ 对于集合 $S$ 的正规化子（Normalizer）。不难证明，$\\langle S \\rangle \\triangleleft N_G(S) \\leqslant G$。称 $C_G(S) = \\{g \\in G \\mid ga = ag, \\forall a \\in S\\}$ 为群 $G$ 对集合 $S$ 的中心化子，同样不难证明，$C_G(S) \\leqslant G$。称 $C_G(G)$ 为群的中心（center）。 称 $x$ 和 $y$ 关于 $H$ 同余（congruent），若 $xH = yH$，记作 $x \\equiv y\\ (\\mathrm{mod}\\ H)$。 我们称如下群与态射的序列为一个正合列（exact sequence）\\begin{align}G_1 \\stackrel{f_1}{\\longrightarrow} G_2 \\stackrel{f_2}{\\longrightarrow} \\cdots \\stackrel{f_{n-1}}{\\longrightarrow} G_n\\end{align}若 $\\forall 1 &lt; i &lt; n-2, \\mathrm{Im} f_i = \\mathrm{Ker} f_{i+1}$。一个形如 1 \\rightarrow G' \\rightarrow G \\rightarrow G'' \\rightarrow 1的正合列被称为一个短正合列。 事实上，我们会发现，在一个范畴中的对象有： 1 \\rightarrow \\mathrm{Ker}\\ f \\rightarrow A \\stackrel{f}{\\longrightarrow} B \\rightarrow \\mathrm{Coker}\\ f \\rightarrow 1是一种非常常见的现象。当然，在 $\\mathsf{Grp}$ 中，由于只有 $\\mathrm{Im}\\ f \\triangleleft B$ 才能考虑 $\\mathrm{Coker}\\ f$，所以在这里它不完全成立。 正合列是同调代数研究的中心问题。 一个很显然的短正合列是这样的： 1 \\rightarrow H \\hookrightarrow G \\stackrel{\\mathrm{can.}}{\\longrightarrow} G/H \\rightarrow 1我们应当已经很熟悉群的同态基本定理，设 $f: G \\to G’$ 为一群同态，$N \\triangleleft G$，则存在 $f: G/N \\to G’$ 使得 $\\varphi \\circ f = f$，其中 $\\varphi$ 为典范同态。它的一个典型应用如下图：$\\require{AMScd}$\\begin{CD}1 @&gt;&gt;&gt; H @&gt;&gt;&gt; G @&gt;&gt;&gt; G/H @&gt;&gt;&gt; 1\\\\@. @VVV @VVV @VVV @.\\\\1 @&gt;&gt;&gt; H/K @&gt;&gt;&gt; G/K @&gt;&gt;&gt; G/H @&gt;&gt;&gt; 1\\end{CD}其中 $K \\triangleleft H \\triangleleft G$，每行都是一个短正合列。 其他同构定理在此不再叙述，可参考 Algebra, S. Lang p16 起的内容。 顺便给出两个命题：正规子群在群同态下的原像是正规的；正规子群在满同态下的像是正规的，这其实是上面的 remark 中 cokernel 存在性问题的另一种描述。 群塔和可解群我们称以下一个群序列为一个群塔（tower of groups）： G = G_0 \\supset G_1 \\supset \\cdots \\supset G_m称其为一个正规群塔，若 $G_i \\triangleright G_{i+1}$，称其为一个交换群塔，若它在正规的同时还满足 $G_i/G_{i+1}$ 是交换的。同理可以定义循环群塔。 若 $G$ 为一个群，存在一个交换群塔： G = G_0 \\supset G_1 \\supset \\cdots \\supset G_m = 1则称 $G$ 是可解群（solvable group）。可以证明，若 $G$ 可解，则它的子群和商群均可解；若 $H \\triangleleft G$，$G/H$ 可解，则 $G$ 可解。 接下来我们在矩阵群中给出可解群的一个例子。设 $T=T(n, K)$ 为 $n$ 维方阵在域 $K$ 的上三角矩阵群，$D=D(n, K)$ 为域 $K$ 上对角线不为 $0$ 的 $n$ 维对角方阵群，$N(n, K)$ 维对角线下方及对角线上为 $0$ 的幂零矩阵的加法群。则可以很容易构造一个从 $T(n, K)$ 到 $D(n, K)$ 的满同态，即只取其对角线上的元素。这个群同态的核显然就是 $U = I + N$，其中 $I$ 为单位矩阵。 记 $N^r(n, K)$ 为幂零矩阵的 $r$ 次方的加法群，$U_r = I + N^r(n, K)$，注意到 $U_i$ 构成了 $U$ 的一个正规群塔，而 $U$ 是 $T$ 的正规子群，$T/U=D$ 为交换群，所以有交换群塔： T \\supset U = U_1 \\supset U_2 \\supset \\cdots \\supset U_n = \\{I\\}故 $T$ 为可解群。","link":"/blog/39b94080/"},{"title":"GDB 源码分析 01：函数前导代码分析（上）","text":"代码：gdb/prologue-value.h，gdb/prologue-value.c 函数前导代码其实很简单也很固定，它主要就是进入一个函数时进行的代码操作，用来建立栈帧，并为临时变量开好空间。这里展示一个 x86 汇编的最简单情形： 123push ebpmov ebp, espsub esp, N 很简单，很友好，不是吗？ 那么为什么要对这块代码进行重点分析呢？一方面，这是一个函数开头的部分，可以用来分析这个函数调用栈的情况，并分析这个函数所使用的临时变量地址；另一方面，虽然看起来这段代码很可爱，但是随着编译器的复杂化和在调度指令时日益激进的策略，它往往会变得面目全非。但无论如何，这段代码总归是相对简单的部分。 事实上，在现代的 gcc 编译器中往往会给出调用栈信息（call frame information, CFI），其中描述了如何寻找栈的基地址和存储的寄存器等信息，但是这些信息并不总是存在。如果它们不存在，我们就必须采用一些策略来试图解读这些信息。为了解读这些信息，我们就采用一种对指令“抽象解读”的策略，也就是下面将介绍的模糊计算策略。 值的模糊计算在 prologue-value.h 中，定义了如下结构： 12345enum prologue_value_kind { pvk_unknown, pvk_constant, pvk_register}; 这里所描述的是一个 prologue 值的类型，分别表示未知、常数和寄存器，它决定了在下面的结构体中，后面两个参数应该如何解读。 123456struct prologue_value { enum prologue_value_kind kind; int reg; CORE_ADDR k;};typedef struct prologue_value pv_t; 如果类型是 pvk_unknown，那么后面两个参数都没有意义，毕竟我们对它一无所知； 如果类型是 pvk_constant，这表明这个值就是常数，记录在 k 中。注意，所谓的 CORE_ADDR 实际上就是一个 uint64_t； 如果类型是 pvk_register，这表明寄存器 reg 所对应的原始值为 初始值 + k ，其中 reg 为 GDB 标定的寄存器编号，是为了防止不同架构带来的跨平台问题。在开始分析之前，所有寄存器都会被标为 {pvk_register, reg, 0} 每个类型都有相关的初始化函数，它们实现起来并不困难： 1234567891011121314151617181920pv_t pv_unknown(void) { pv_t v = {pvk_unknown, 0, 0}; return v;}pv_t pv_constant(CORE_ADDR k) { pv_t v; v.kind = pvk_constant; v.reg = -1; v.k = k; return v;}pv_t pv_register(int reg, CORE_ADDR k) { pv_t v; v.kind = pvk_register; v.reg = reg; v.k = k; return v;} 随后我们要对这些值进行“保守估计”，保守估计的意思是说，我们会把一个值正确估计或者设为未知，但不能出现错误的估计。对应于这种估计方式的逻辑变量是这样的： 12345enum pv_boolean { pv_maybe, pv_definite_yes, pv_definite_no}; 然后通过看相关的函数来理解其原理： 1234567static void constant_last(pv_t *a, pv_t *b) { if (a-&gt;kind == pvk_constant &amp;&amp; b-&gt;kind != pvk_constant) { pv_t temp = *a; *a = *b; *b = temp; }} 这个函数就是个辅助函数。如果前者是常数而后者不是常数，对两者做个交换。这个函数在后面只是用来减少需要处理的情况总数的。接下来是对一大批寄存器运算的模拟。 1234567891011121314pv_t pv_add(pv_t a, pv_t b) { constant_last(&amp;a, &amp;b); if (a.kind == pvk_register &amp;&amp; b.kind == pvk_constant) { return pv_register(a.reg, a.k + b.k); } else if (a.kind == pvk_constant &amp;&amp; b.kind == pvk_constant) { return pv_constant(a.k + b.k); } else { return pv_unknown(); }}pv_t pv_add_constant(pv_t v, CORE_ADDR k) { return pv_add(v, pv_constant(k));} 首先实现加法的计算。这里的过程看起来很简单：两个常数可以加，寄存器可以加上常数，但其他的加法都返回未知。如果要处理常数加法，那就将常数转换成对应的类型再相加。这里不妨停一下思考两个问题： 为什么寄存器和寄存器不能相加？ 为什么常数加法的情况在两个函数中都进行了处理？ 这里我们需要注意，我们并不会实际执行程序，只是取其中的一个片段进行分析。在分析这个片段的时候，寄存器的初始值是未知的。因此，我们总共知道的就是，某个寄存器相比其进入这个片段之前多了或者少了多少。如果将寄存器与寄存器相加，那么势必要用到初始值，也就是说，它的结果对我们来说是未知的。 对于第二个问题，需要注意，加常数的情况并不仅仅存在于显式的指令编码中。例如，在下面将读到的 logical_and 函数中，如果出现与常数 0 做 and 操作，这个寄存器的值就会变成常数 0 ，于是，当它与另一个寄存器相加时，就会进入寄存器与常数相加的情形。 接下来按照这个思路，可以很简单的实现减法和逻辑与运算，代码如下： 1234567891011121314151617181920212223242526272829pv_t pv_subtract(pv_t a, pv_t b) { constant_last(&amp;a, &amp;b); if (a.kind == pvk_constant &amp;&amp; b.kind == pvk_constant) { return pv_constant(a.k - b.k); } else if (a.kind == pvk_register &amp;&amp; b.kind == pvk_constant) { return pv_register(a.reg, a.k - b.k); } else if (a.kind == pvk_register &amp;&amp; b.kind == pvk_register &amp;&amp; a.reg == b.reg) { return pv_constant (a.k - b.k); } else { return pv_unknown(); }}pv_t pv_logical_and(pv_t a, pv_t b) { constant_last(&amp;a, &amp;b); if (a.kind == pvk_constant &amp;&amp; b.kind == pvk_constant) { return pv_constant(a.k &amp; b.k); } else if (b.kind == pvk_constant &amp;&amp; b.k == 0) { return pv_constant(0); } else if (b.kind == pvk_constant &amp;&amp; b.k == ~(CORE_ADDR) 0) { return a; } else if (a.kind == pvk_register &amp;&amp; b.kind == pvk_register &amp;&amp; a.reg == b.reg &amp;&amp; a.k == b.k) { return a; } else { return pv_unknown(); }} 这两段的代码逻辑也相当清晰，如果理解了什么叫做保守估计，那么应该也能很轻松地读懂。主要的难点在于理解什么情况下可以对这个操作做出准确的估计，什么时候不能。基于这些计算方式，我们就可以基本上知道某段代码结束之后，能得到怎样的结果了。 这里需要注意，如果出现了除算术指令之外的其他指令，其结果就基本上全是 pvk_unknown 了。所以，这里所做的只是一个前导代码分析，因为函数的前导代码往往不会包含太复杂的东西。 对模糊值做出判断当我们知道一段代码运行之后的结果之后，我们就可以尝试确定一些东西了，比如某个对象是否落在一个数组里。但在进行这些判断之前，先要有三个最普通的判断函数：判断某个值是否是常数、是否是某个寄存器、是否是某寄存器 初始值 + k 的结果，这三个函数将成为后面应用的基石。 1234567891011int pv_is_constant(pv_t a) { return (a.kind == pvk_constant);}int pv_is_register(pv_t a, int r) { return (a.kind == pvk_register &amp;&amp; a.reg == r);}int pv_is_register_k(pv_t a, int r, CORE_ADDR k) { return (a.kind == pvk_register &amp;&amp; a.reg == r &amp;&amp; a.k == k);} 接下来我们要开始考虑，如何判断一个对象是否落在某个数组里。很显然，我们需要考虑如何描述一个对象和如何描述一个数组。我们需要的参数有： 描述一个对象在内存中的位置和长度，pv_t addr 和 CORE_ADDR size； 描述一个数组在内存里的位置和长度，pv_t array_addr 和 array_len （数组长度），以及 CORE_ADDR elt_size （数组内每个元素的大小）； 返回值，是不是指向某个完整元素，以及如果指向某个完成元素，其对应的指标 int *i。 在开始考虑如何写这个函数之前，我们需要明确地指出这个函数的功能以及它的返回类型。它的返回类型是 enum pv_boolean 型： 如果这个对象一定落在数组中，那么返回 pv_definite_yes ，并把 I； 如果这个对象一定不落在数组中，那么返回 pv_definite_no； 如果不能判断、或者不指向完整的数组元素，那么返回 pv_maybe 。 好，接下来可以开始写这个函数了： 1234567891011121314151617enum pv_boolean pv_is_array_ref(pv_t addr, CORE_ADDR size, pv_t array_addr, CORE_ADDR array_len, CORE_ADDR elt_size, int *i) { pv_t offset = pv_subtract(addr, array_addr); if (offset.kind == pvk_constant) { if (offset.k &lt;= -size &amp;&amp; offset.k &gt;= array_len * elt_size) { return pv_definite_no; } else if (offset.k % elt_size != 0 || size != elt_size) { return pv_maybe; } else { *i = offset.k / elt_size; return pv_definite_yes; } } else { return pv_maybe; }} 这里的逻辑其实很简单：如果对象的起始地址和数组起始地址不能相减或相减不为常数，我们都不能严格做出判断。然后我们考虑这样的情形： 这里就是第一个比较的来源：只有当 offset &lt;= -size 时，才能明确判断说这个元素不在数组内，同样的，第二个比较发生在右边。然后检查是否对齐。如果不对齐，那只能是 pv_maybe。只有当对齐且元素大小等于数组元素大小，且偏移量落在数组之内时，才会返回 pv_definite_yes 。 我们上面的这些工作都只是完成了这个模块的第一部分。为了真正让这个模块得以被使用，还需要具备分析一个内存块的内容的能力。这就是第二部分将要讨论的内容。","link":"/blog/5ad3ba87/"},{"title":"抽象代数续论笔记 02 一些可解群、有限单群分类与范畴","text":"第二次课延续了上一次课的内容，首先介绍了一些关于可解群的结论和有限单群分类的大致思路，然后开始范畴论部分，这一部分总体上维持了较为完整的框架，还是比较清晰的。 如果有时间可能补一个 1.5 期，用来介绍一下循环群，Sylow 定理及一些简单的内容，并且添加一些关于群结构描述的部分，但是现在先把这些写完吧。 几类可解群和一些判定定理可以表明，所有阶为 $p^nq^m$，其中 $p, q$ 为素数都为可解群（Burnside 定理）。这个定理的证明在后面我如果写群表示的专题可能会以表示论的方式给出，纯粹群论的方法处理它是较为困难的。另外，甚至可以表明，所有阶为奇数的群都为可解群（Feit-Thompson 定理），这个定理的证明有些复杂，也不在此详述。 换位子群（commutator group）的内容在上一次课的注释中已经叙述，单群（simple group） 的定义为非平凡的，没有非平凡正规子群的群。我们可以很容易证明，交换单群同构于循环群 $C_p$，其中 $p$ 为素数；若 $G$ 为非交换的单群，则 $G’ = G$，$G$ 是一个不可解的群。 于是我们发现，对于非交换单群 $G$，它的阶中至少有三个素因子（因为 Burnside 定理），而且其中有一个是 2（因为 Feit-Thompson 定理），所以可以表明，交错群 $A_5$ 是阶数最低的非交换单群。 可以表明，所有 $n \\geqslant 5$ 的交错群 $A_n$ 都是非交换的单群，这预示着 $n \\geqslant 5$ 的 $S_n$ 都是不可解的。 这种对有限单群结构的考量使得我们走向了有限单群分类定理，在下面我们对一些结论进行简要叙述。 有限单群分类定理可以表明，所有有限单群都等价于一下四类群中的一种： 三类无限群列，素数阶循环群、五阶及以上的交错群和李型单群； 26 个散在单群； （可以被归在散在单群或李型单群中的）$^2F_4(2)’$，有时被称为 Tits 群。 这些群中，素数阶循环群和五阶及以上的交错群在前面已经叙述过了，最早构造出的形式为 $PSL(2, F)$（有少数例外），是在 1830 年左右由 Galois 给出的。这给出了一系列通过“商掉中心”的方法构造出来的经典李型单群。 散在单群（sporadic group）的描述比较复杂，其中最著名的是 Fischer-Griess 魔群（Monster group）和子魔群（baby Monster group），具备非常惊人的阶数；另外的群中有 18 个是魔群或子魔群的子群，例如 Mathieu 群（$M_{11}, M_{12}, M_{22}, M_{23}, M_{24}$，它们都是 $M_{24}$ 的子群）；剩下六个被戏称为贱民（pariahs），主要分布在 Janko 群中（$J_1, J_3, J_4$）和 Lyons 群、O’Nan 群、Rudvalis 群，它们中最小的是 $J_1$，阶为 175560。 它们的大致关系可以参见 Wikipedia，如下图： Jordan-Hölder 定理回到单群的问题，我们可以证明 Jordan-Hölder 定理，如果 $G$ 有一个群塔使得 $G_i / G_{i+1}$ 为单群，那么这个群塔在等价意义上是唯一的。同样不难表明，任意一个有限群都存在一个有限长的正规群塔使得 $G_i / G_{i+1}$ 为单群，那么它在等价意义上是唯一的。 这些定理的证明参见 Algebra S. Lang，在这里不再赘述。 半直积上次课已经提到了半直积，这次我们将其规范化一下。若 $N \\triangleleft G, H \\subset G, N \\cap H = \\{e\\}, NH=G$，则称 $G$ 为 $N$ 和 $H$ 的半直积。 我们还可以给出一个等价描述。设存在正合列：$\\require{AMScd}$\\begin{CD}1 @&gt;&gt;&gt; H @&gt;&gt;&gt; G @&gt;\\varphi&gt;&gt; K @&gt;&gt;&gt; 1\\end{CD}并存在一个同态 $\\psi: K \\to G$ 使得 $\\varphi \\circ \\psi = \\mathrm{id}_K$，则称 $G$ 为 $N$ 和 $K$ 的半直积。 用一张图可以比较清晰地表明这里面的关系： 在作业题中还给出了另外一种描述，参考 Algebra S. Lang p76 Ex. 12 范畴接下来才是这节课的正题。我们称一个范畴 $\\mathcal{C}$ 为带有以下资料的东西：一些对象 $\\mathsf{Ob}(\\mathcal{C})$ 与任意两对象 $A, B \\in \\mathsf{Ob}(\\mathcal{C})$ 之间的态射 $\\mathrm{Mor}_\\mathcal C(A, B)$。其中态射间可以作复合运算，且满足以下三条公理： $\\mathrm{Mor}_\\mathcal C(A, B)$ 与 $\\mathrm{Mor}_\\mathcal C(A’, B’)$ 交为空集，除非 $A = A’ \\wedge B = B’$； 态射之间满足结合律，即若存在\\begin{CD}A @&gt;f&gt;&gt; B @&gt;g&gt;&gt; C @&gt;h&gt;&gt; D\\end{CD}则 $h \\circ (g \\circ f) = (h \\circ g) \\circ f$； 在 $\\mathrm{Mor}_\\mathcal C(A, A)$ 中存在单位态射 $\\mathrm{id}_A$，满足：\\begin{CD}A @&gt;\\text{id}_A&gt;&gt; A @&gt;f&gt;&gt; B\\end{CD}中 $f\\circ \\mathrm{id}_A = f$，\\begin{CD}B @&gt;g&gt;&gt; A @&gt;\\text{id}_A&gt;&gt; A\\end{CD}中 $\\mathrm{id}_A \\circ g = g$。 接下来我们给出一（da）些（liang）范畴的例子。一个最普通的范畴是 $\\mathsf{Set}$，其中的对象为集合，态射为映射；同理，还有 $\\mathsf{Mon}$ 和 $\\mathsf{Grp}$，其对象分别为幺半群和群，态射分别为幺半群同态和群同态；$\\mathcal{C}^0$ 的对象为 $\\mathbb{R}$ 上的开区间，态射为连续映射；$\\mathcal{C}^{\\infty}$ 的对象为光滑流形，态射为光滑映射。 $\\mathsf{Ab}$ 的对象为交换群，态射为群同态。注意这里有一个比较有趣的结构，考虑下图： \\begin{xy} \\xymatrix { U \\ar@/_/[ddr]_y \\ar@{.>}[dr]|{\\langle x,y \\rangle} \\ar@/^/[drr]^x \\\\ & X \\times_Z Y \\ar[d]^q \\ar[r]_p & X \\ar[d]_f \\\\ & Y \\ar[r]^g & Z } \\end{xy} 不能画斜线和多个箭头好烦，AMScd 就是个垃圾 注意到，如果取 $f + f’$ 为对应元素在交换群 $B$ 中的加法，$\\mathrm{Mor}(A, B)$ 有一个","link":"/blog/c1a66891/"},{"title":"GDB 源码分析 02：函数前导代码分析（中）","text":"代码：gdb/prologue-value.h，gdb/prologue-value.c 上一次，我们考虑了如何对一个值进行模糊分析，接下来，我们要对一片内存的值做出模糊的估计。在有了这种估计方式之后，我们就能够将其进行真正的应用了。 首先考虑一个问题：如何记录一个内存中的数据？不要忘记，我们需要分析一个函数的前导代码，而前导代码往往做的是栈操作。那么，既然我们不能预先知道栈地址的值，我们自然就得以一个寄存器作为基地址，那么我们需要存储的就是某个地址距离基地址寄存器的偏移量；同时，内存中的对象并不自带类型，所以需要长度作为其标识；最后，我们还要记下它的值。这就是下面这个结构体想要做的事情： 123456struct pv_area::area_entry { struct area_entry *prev, *next; CORE_ADDR offset; CORE_ADDR size; pv_t value;} 注意到它有两个指向前后的指针。事实上，它构成了一个环形双向链表，在后面的分析中，我们还会看到，它是以偏移量的大小进行排序的。然后就是把这个双向链表包装成一个类： 123456class pv_area {private: int m_base_reg; CORE_ADDR m_addr_mask; struct area_entry *m_entry;} 其中除了指向这样一个链表的指针之外，还有基地址寄存器的寄存器号，以及一个地址的掩码。看到后面之后会更容易理解这个掩码的目的，在这里我们只需要简单地认为，我们的地址空间是一个环形，前后相连即可。 接下来看类里面的三个私有成员函数： 1234567891011void pv_area::clear_entries() { struct area_entry *e = m_entry; if (e) { do { struct area_entry *next = e.next; xfree(e); e = next; } while (e != m_entry); m_entry = 0; }} 这个函数用来清空双向链表。其中比较需要注意的就是 xfree 这个函数，事实上这个函数和内存保护有关，在读到这里的时候，我们不妨将其当成一个简单的 free 函数理解。 12345678910111213141516struct pv_area::area_entry *pv_area::find_entry(CORE_ADDR offset) { struct area_entry *e = m_entry; if (!e) { return 0; } while (((e-&gt;next-&gt;offset - offset) &amp; m_addr_mask) &lt; ((e-&gt;offset - offset) &amp; m_addr_mask)) { e = e-&gt;next; } while (((e-&gt;prev-&gt;offset - offset) &amp; m_addr_mask) &lt; ((e-&gt;offset - offset) &amp; m_addr_mask)) { e = e-&gt;prev; } m_entry = e; return e;} 这个函数用来找到在 offset 之后的第一项。如果本身这片区域一项也没有，那么就自然地返回一个空指针。而如果 offset 中一项也没有，那当然就会返回较为靠前的一项。这里需要澄清的是，因为 m_entry 指向的是环形链表中的任意一项，往后扫描和往前扫描都是必须的。最后一步将 m_entry 设为 e 就是出于局部性的假定，假定最近被访问的这项很可能再次被访问，从而提高整个数据结构的搜索效率。 在此不妨停下来思考一个问题：哪怕没有掩码的问题，是否有可能直接写成 e-&gt;next-&gt;offset &lt; e-&gt;offset ？答案是否定的，原作者在注释中特别表明了这一点： 123456/*Note that, even setting aside the addr_mask stuff, we must not simplify this, in high school algebra fashion, to (e-&gt;next-&gt;offset &lt; e-&gt;offset), because of the way &lt; interacts with wrap-around. We have to subtract offset from both sides to make sure both things we're comparing are on the same side of the discontinuity. */ 解释很清楚，在这也就不翻译了。 第三个函数就是一个判断 offset 处 size 大小的一项是否与某个 entry 重合，非常简单，直接贴代码不解释： 1234int pv_area::overlaps(struct area_entry *entry, CORE_ADDR offset, CORE_ADDR size) { return (((entry-&gt;offset - offset) &amp; m_addr_mask) &lt; size || ((offset - entry-&gt;offset) &amp; m_addr_mask) &lt; entry-&gt;size);} 接下来观察一下公有的成员函数，首先来看构建过程： 123pv_area::pv_area(int base_reg, int addr_bit): m_base_reg(base_reg), m_addr_mask(((((CORE_ADDR) 1) &lt;&lt; (addr_bit - 1)) - 1) &lt;&lt; 1) | 1), m_entry(nullptr){} 非常简单的构造函数，但是这里我们就可以重新解释一下地址掩码的含义了。当我们有一个 32 bit 的地址时，自然而然地我们只希望其比较 32 bit，但是寄存器又是可以有 64 bit 的：也就是说，有一部分是需要被抛弃的。 一个很细节的问题是，为什么不把这个表达式写成 ((CORE_ADDR) 1 &lt;&lt; addr_bit) - 1 ？注意，移位的位数与类型的宽度相同时，得到的结果是未定义的。如果采取上面代码的形式，就一定能够保证得到正确的结果。 析构函数非常简单： 123pv_area::~pv_area() { clear_entries();} 然后用了一个有趣的小技巧取消了默认析构函数和 operator=： 1DISABLE_COPY_AND_ASSIGN(pv_area); 宏定义如下，这是为了与不同版本的 c++ 标准兼容： 123456789#if defined(__cplusplus) &amp;&amp; __cplusplus &gt;= 201103#define DISABLE_COPY_AND_ASSIGN(TYPE) \\ TYPE (const TYPE&amp;) = delete; \\ void operator= (const TYPE &amp;) = delete #else#define DISABLE_COPY_AND_ASSIGN(TYPE) \\ TYPE (const TYPE&amp;); \\ void operator= (const TYPE &amp;)#endif 接下来我们就要正式进行模拟存取了。下面这个成员函数用来将一个 size 大小的值 value 写入到 addr 处： 123456789101112131415161718192021222324252627282930313233void pv_area::store(pv_t addr, CORE_ADDR size, pv_t value) { if (store_would_trash(addr)) { clear_entries(); } else { CORE_ADDR offset = addr.k; struct area_entry *e = find_entry(offset); while (e &amp;&amp; overlaps(e, offset, size)) { struct area_entry *next = (e-&gt;next == e) ? 0 : e-&gt;next; e-&gt;prev-&gt;next = e-&gt;next; e-&gt;next-&gt;prev = e-&gt;prev; xfree(e); e = next; } m_entry = e; } if (value.kind == pvk_unknown) { return; } else { CORE_ADDR offset = addr.k; struct area_entry *e = XNEW(struct area_entry); e-&gt;offset = offset; e-&gt;size = size; e-&gt;value = value; if (m_entry) { e-&gt;prev = m_entry-&gt;prev; e-&gt;next = m_entry; e-&gt;prev-&gt;next = e-&gt;next-&gt;prev = e; } else { e-&gt;prev = e-&gt;next = e; m_entry = e; } }} 这个函数稍微有点长，但是思路其实挺简单的： 如果存储操作会使得我现在所有的表项都被无效化，那么直接先把表清掉； 否则的话，清掉所有与这个值重合的项，因为这些项都会被覆写掉； 然后，如果我需要存的值未知，那就直接返回； 如果我需要存的值已知，那就创建出这一项并把它插进去。 XNEW(T) 事实上拓展到了泛型 xnew&lt;T&gt;() ，这个函数和前面的 xfree 一样，只需要当成一个 new 就行，暂时不需要深究；接下来需要考虑的是，什么时候我可能无效化整个表： 12345bool pv_area::store_would_trash(pv_t addr) { return (addr.kind == pvk_unknown || addr.kind == pvk_constant || (addr.kind == pvk_register &amp;&amp; addr.reg != m_base_reg));} 事实上说这种情况可能无效化整个表是不准确的。准确地说，是我们不能确定这次存储和我们现在的表有什么关系，为了方便起见，我们就直接认为这整个表的分析是不可能的，因而就将其抛弃了。这个函数还会被在其他地方调用，比如，考虑取一个值的问题： 12345678910111213pv_t pv_area::fetch(pv_t addr, CORE_ADDR size) { if (!m_entry || store_would_trash(addr)) { return pv_unknown(); } else { CORE_ADDR offset = addr.k; struct area_entry *e = find_entry(offset); if (e-&gt;offset == offset &amp;&amp; e-&gt;size == size) { return e-&gt;value; } else { return pv_unknown(); } }} 很显然，这个函数的核心还是保守估计。需要注意的是第一行判断，如果表里没有内容，或者这个地址和表无关，那就返回未知。这里，“和表无关”才是上面那个函数真正的语义。 当然，我们可以找找我们的表中有没有关于某个寄存器值的信息： 123456789101112131415161718bool pv_area::find_reg(struct gdbarch *gdbarch, int reg, CORE_ADDR *offset_p) { struct area_entry *e = m_entry; if (e) { do { if (e-&gt;value.kind == pvk_register &amp;&amp; e-&gt;value.reg == reg &amp;&amp; e-&gt;value.k == 0 &amp;&amp; e-&gt;size == register_size(gdbarch, reg)) { if (offset_p) { *offset_p = e-&gt;offset; } return true; } e = e-&gt;next; } while (e != m_entry); } return false;} 关于 gdbarch 结构体，现在只要知道它记录了架构信息即可，register_size 则返回某种架构中某个寄存器的大小。这个函数也没什么特别的，就是一个遍历操作。 接下来有一个稍微看起来复杂点但功能很简单的函数，也是这个类的最后一个函数： 12345678910111213141516void pv_area::scan(void (*func)(void *closure, pv_t addr, CORE_ADDR size, pv_t value), void *closure) { struct area_entry *e = m_entry; pv_t addr; addr.kind = pvk_register; addr.reg = m_base_reg; if (e) { do { addr.k = e-&gt;offset; func(closure, addr, e-&gt;size, e-&gt;value); e = e-&gt;next; } while (e != m_entry); }} 循环遍历整个表，对表中的每一项做一个自定义操作。函数指针使得整个函数看起来有些抽象，但也不难明白它的意思。 现在，我们已经获得了对一块内存区域的数据进行分析的工具，接下来，我们可以利用这些工具进行分析了。下一次将针对两种架构的整个前导代码分析过程做出解释，在那里，我们可以看见这些工具都是怎样被应用的，又能给出怎样的结果。","link":"/blog/3d571260/"},{"title":"局部哈密顿问题的复杂性 01","text":"原文为：arXiv:quant-ph/0406180 这篇文章挺难读的，但是也很有意思。大概会分成四次把解读更新完，其实原文在大部分地方已经讲的很清楚了，但是一些计算过程可能需要澄清，毕竟文中的证明有些给的实在简洁。 这篇文章主要讨论的问题是 $\\text{2-}\\large\\text{L}\\small{\\text{OCAL }}\\large{\\text{H}}\\small{\\text{AMILTONIAN}}$ 问题的复杂度。作为第一部分这里主要把论文的前四节的比较复杂的证明过了一遍，主要是投影引理和对 Kitaev 构造的重述，虽然还没进入正题但也并不算简单，还是很有仔细研究的价值的。 基本定义首先，我们快速回顾一下经典计算理论中复杂度类 $\\mathsf{NP}$ 的量子版本，即 $\\mathsf {QMA}$ 的概念： 给定 $\\varepsilon = \\varepsilon(|x|) = 2^{-\\Omega(|x|)}$，其中 $|x|$ 为字符串 $x$ 的长度。称一个承诺问题 $L = (L_{yes}, L_{no})$ 在类 $\\mathsf{QMA}$ 中，若存在一个量子的、多项式时间的验证者 $V$ 和一个多项式 $p$，使得： $\\forall x \\in L_{yes}, \\exists |\\xi\\rangle \\in \\mathcal{B}^{\\otimes p(|x|)}, \\mathbf{P}(V(|x\\rangle, |\\xi\\rangle) = 1) \\geqslant 1 - \\varepsilon$ $\\forall x \\in L_{no}, \\exists |\\xi\\rangle \\in \\mathcal{B}^{\\otimes p(|x|)}, \\mathbf{P}(V(|x\\rangle, |\\xi\\rangle) = 1) \\leqslant \\varepsilon$ 其中 $\\mathcal B$ 指描述一个量子比特的 Hilbert 空间。 注意到，只需要取 $\\varepsilon \\leqslant 1/3$，事实上也就够了，它们产生的复杂度类都是等价的。 在经典计算复杂度理论中，Cook-Levin 定理表明， $\\text{SAT}$ 问题是 $\\mathsf{NP}$-完备的。在 $\\mathsf{QMA}$ 中，我们取局部哈密顿问题 $\\large\\text{L}\\small{\\text{OCAL }}\\large{\\text{H}}\\small{\\text{AMILTONIAN}}$ 来作为其等价物。这个问题定义如下： 我们称一个算子 $H: \\mathcal B^{\\otimes n} \\rightarrow \\mathcal B^{\\otimes n}$ 是 $k$-局部哈密顿（$k$-local Hamiltonian），若 $H = \\sum_{j=1}^rH_j$，其中每个 $H_i$ 都是在最多 $k$ 个量子比特上的哈密顿算子。 在 $n$ 个量子比特上，给定一个 $k$-局部哈密顿 $H = \\sum_{j=1}^r H_j$，其中 $r=\\text{poly}(n)$。每个 $H_j$ 的范数 $||H_j|| \\leqslant \\text{poly}(n)$，其中的每个元素也可以被 $\\text{poly}(n)$ 个比特表示出来。有两个输入 $a, b$ 满足 $a &lt; b$，我们需要判断， $H$ 的最小特征值至多为 $a$ 还是大于 $b$。 在下文中，我们记 $H$ 的最小特征值为 $\\lambda(H)$；若 $\\Pi$ 为 $\\mathcal{B}^{\\otimes n}\\rightarrow \\mathcal{S}$ 的投影，则称 $\\Pi H\\Pi$ 为 $H$ 在子空间 $\\mathcal S$ 上的限制，记作 $H\\vert_\\mathcal S$。 投影引理（Projection Lemma）这个引理的核心意义就在于通过局部哈密顿近似一个全局哈密顿。考虑 Hilbert 空间 $\\mathcal H$，$H_1$ 是其上的一个哈密顿，对于一个子空间 $\\mathcal S \\subseteq \\mathcal H$ ，取一个哈密顿 $H_2$，使得 $\\mathcal S$ 是其特征值 $0$ 的特征子空间，且 $\\lambda(H_2|_\\mathcal {S^{\\perp}}) \\gg ||H_1||$ 。考虑 $H = H_1 + H_2$，投影引理表明，$\\lambda(H)$ 非常接近于 $\\lambda(H_1\\vert_\\mathcal S)$。 我们称哈密顿 $H$ 的惩罚值（penalty value）为 $\\min_{x \\in \\mathcal H, ||x|| = 1}\\langle x | H | x \\rangle$。 直觉地，我们发现，$H_2$ 对于那些在 $\\mathcal S^{\\perp}$ 上有分量的向量给出了非常大的惩罚值，因此，$\\lambda(H)$ 对应的特征向量一定接近于 $\\mathcal S$，也就是接近于 $H_1|_\\mathcal S$ 的特征向量。 接下来给出一个形式化的描述： 令 $H = H_1 + H_2$ 为 Hilbert 空间 $\\mathcal H = \\mathcal S + \\mathcal S^\\perp$ 上的两个哈密顿之和，哈密顿 $H_2$ 的零特征空间为 $S$，而其在 $S^\\perp$ 上的特征向量对应的特征值至少为 $J &gt; 2||H_1||$，于是： \\lambda(H_1\\vert_\\mathcal S) - \\frac{||H_1||^2}{J - 2||H_1||} \\leqslant \\lambda(H) \\leqslant \\lambda(H_1\\vert_\\mathcal{S})注意到，如果 $J \\geqslant 8||H_1||^2 + 2||H_1|| = \\text{poly}(||H_1||)$，则我们有 $\\lambda(H_1\\vert_\\mathcal S) - 1/8 \\leqslant \\lambda(H) \\leqslant\\lambda(H_1\\vert_\\mathcal S)$。 这个引理的证明非常简单。首先表明 $\\lambda(H) \\leqslant \\lambda(H_1\\vert_\\mathcal S)$ ：令 $|\\eta\\rangle \\in \\mathcal S$ 为 $H_1|_\\mathcal S$ 对应 $\\lambda(H_1|_\\mathcal S)$ 的特征向量，则 \\langle \\eta|H|\\eta \\rangle = \\langle \\eta|H_1|\\eta \\rangle + \\langle \\eta|H_2|\\eta\\rangle = \\lambda (H_1\\vert_\\mathcal S)因此 $\\lambda(H_1|_\\mathcal S)$ 也是 $H$ 的特征值，$\\lambda(H) \\leqslant \\lambda(H_1|_\\mathcal S)$。 接下来证明 $\\lambda(H)$ 的下界：将单位向量 $|v\\rangle \\in \\mathcal H$ 分解成 $|v\\rangle = \\alpha_1|v_1\\rangle + \\alpha_2 |v_2\\rangle$，其中 $|v_1\\rangle$ 和 $|v_2\\rangle$ 分别为 $\\mathcal S$ 和 $\\mathcal S^\\perp$ 中的两个单位向量，$\\alpha_1, \\alpha_2$ 为非负实数且平方和为 $1$ 。令 $K = ||H||$，则： \\begin{aligned}\\langle v|H|v \\rangle &amp;\\geqslant \\langle v|H_1|v\\rangle + J\\alpha_2^2\\hfill\\newline&amp;= \\alpha_1^2\\langle v_1|H|v_1\\rangle + 2\\alpha_1\\alpha_2 \\text{Re} \\langle v_1 |H_1|v_2\\rangle + \\alpha_2^2\\langle v_2|H_1|v_2\\rangle + J\\alpha_2^2\\hfill\\newline&amp;= (1 - \\alpha_2^2)\\langle v_1|H|v_1\\rangle + 2\\alpha_1\\alpha_2 \\text{Re} \\langle v_1 |H_1|v_2\\rangle + \\alpha_2^2\\langle v_2|H_1|v_2\\rangle + J\\alpha_2^2\\hfill\\newline&amp;\\geqslant \\langle v_1|H|v_1\\rangle - K\\alpha_2^2 - 2K\\alpha_2 - K \\alpha_2^2 + J \\alpha_2\\hfill\\newline&amp;= \\langle v_1|H|v_1\\rangle + (J - 2K)\\alpha_2^2 - 2K\\alpha_2 \\hfill\\newline&amp;\\geqslant \\lambda(H_1|_\\mathcal S) + (J - 2K)\\alpha_2^2 - 2K\\alpha_2\\hfill\\newline&amp;\\geqslant \\lambda(H_1|_\\mathcal S) - \\frac{K^2}{J - 2K}\\end{aligned} Kitaev 构造 这里会经常用到一个非常好用的性质，对于张量积和矩阵乘法，有： (A\\otimes B)(C\\otimes D) = AB\\otimes CD直觉上看，这很好理解，当然也是有条件的。这里懒得复述其条件和证明了，反正在这都能用。 在这一节中，作者尝试应用上述投影引理重述 Kitaev 关于 $O(\\log n)\\text{-}\\large\\text{L}\\small{\\text{OCAL }}\\large{\\text{H}}\\small{\\text{AMILTONIAN}}$ 是 $\\mathsf{QMA}$ 完全的证明。也就是说，需要表明，所有 $\\mathsf{QMA}$ 中的问题都可以被多项式地规约到 $O(\\log n)\\text{-}\\large\\text{L}\\small{\\text{OCAL }}\\large{\\text{H}}\\small{\\text{AMILTONIAN}}$。 我们考虑验证者 $V_x = V(|x\\rangle, \\cdot) = U_T\\cdots U_1$，其中 $T = \\text{poly}(|x|)$ 作用在 $N = \\text{poly}(|x|)$ 个量子比特上。最开始前面的 $m = p(|x|)$ 个量子比特包含被给出的证明，后面的 $N - m$ 个辅助量子比特被初始化为 $0$ ，而电路的最终结果被放在第一个量子比特上。 我们构造一个作用在 $n = N + \\log(T+1)$ 个量子比特上哈密顿 $H$ ，其中前面 $N$ 个量子比特表征计算，最后的 $\\log(T + 1)$ 个量子比特表征时钟 $0, \\cdots, T$ 的可能值，使得： H = H_{out} + J_{in}H_{in} + J_{prop}H_{prop}其中 $J_{in}$ 和 $J_{prop}$ 为 $N$ 为变量的大多项式，在后面将详细讲述其构造，而其他参量构造为：\\begin{aligned}H_{in} \\hfill&amp;= \\sum\\limits^N_{i=m+1} |1\\rangle\\langle1|_i\\otimes|0\\rangle\\langle0|\\hfill\\\\H_{out} \\hfill &amp;= (T+1)|0\\rangle\\langle0|_1 \\otimes |T\\rangle\\langle T|\\hfill\\\\H_{prop} \\hfill &amp;= \\sum\\limits_{t=1}^T H_{prop, t}\\hfill\\\\H_{prop, t} \\hfill &amp;= \\frac 1 2 \\big(I \\otimes |t\\rangle\\langle t| + I \\otimes |t-1\\rangle\\langle t-1| - U_t \\otimes |t\\rangle\\langle t-1| - U_t^\\dagger|t-1\\rangle\\langle t| \\big)\\end{aligned}其中 $|\\alpha\\rangle\\langle\\alpha|_i$ 表示在第 $i$ 个量子比特为 $|\\alpha\\rangle$ 的子空间上的投影。上面的张量积中，第一部分都是在 $N$ 个量子比特的空间上的作用，第二部分则是为了处理时钟量子比特，$U_t$ 和 $U_t^\\dagger$ 作用的量子比特与在原来的电路中完全相同。直观地讲，现在构造的这一系列哈密顿量中： $H_{in}$ 检查我们的输入值是正确的，也就是说，后面的 $N - m$ 个量子比特确实被初始化为 $0$； $H_{out}$ 检查表征结果的输出位； $H_{prop}$ 检查我们的结果确实是按照原来电路的状态转移方式得到的。 由于我们只有 $\\log(T + 1) = O(\\log n)$ 个时钟量子比特，所以这些哈密顿算子都是 $O(\\log n)$-局部的。接下来我们表明任何一个 $\\mathsf{QMA}$ 中的问题都能规约为对上述 $H$ 的 $O(\\log n)\\text{-}\\large\\text{L}\\small{\\text{OCAL }}\\large{\\text{H}}\\small{\\text{AMILTONIAN}}$ 问题： 如果电路 $V_x$ 对某个输入 $|\\xi, 0\\rangle$ 的接受概率大于 $1-\\varepsilon$，那么哈密顿 $H$ 有一个小于 $\\varepsilon$ 的特征值；如果它对所有输入 $|\\xi, 0\\rangle$ 的接受概率小于 $\\varepsilon$，那么哈密顿 $H$ 的所有特征值均大于 $3/4 - \\varepsilon$。 这个引理的前半部分非常好证明，只需要取 |\\eta\\rangle = \\frac{1}{\\sqrt{T+1}} \\sum\\limits^T_{t=0}U_t\\cdots U_1 |\\xi, 0\\rangle \\otimes |t\\rangle接下来就可以表明： \\langle \\eta|H_{prop}|\\eta \\rangle = 0记 $\\eta_j = U_j\\cdots U_1|\\xi, 0\\rangle \\otimes |j\\rangle$，注意到： \\langle \\eta|H_{prop}|\\eta\\rangle = \\sum\\limits_{t=1}^t \\langle\\eta|H_{prop, t}|\\eta\\rangle再次逐项展开，得： \\langle \\eta|(\\frac 1 2 I \\otimes |t\\rangle\\langle t|)|\\eta\\rangle = \\frac {1} {2(T+1)} \\langle \\sum\\limits_{t=0}^T \\eta_t |(I\\otimes |t\\rangle\\langle t|)|\\sum\\limits_{t=0}^T \\eta_t\\rangle这里如果将 $\\eta_t$ 再展开，就会出现交叉项，首先考虑能不能消除它：\\begin{aligned}\\langle \\eta_i | (I \\otimes |t\\rangle \\langle t|) | \\eta_j\\rangle &amp;= (U_j \\cdots U_1 |\\xi, 0\\rangle \\otimes |j\\rangle)^\\dagger(I\\otimes|t\\rangle\\langle t |)(U_i \\cdots U_1 |\\xi, 0\\rangle \\otimes |i\\rangle)\\hfill\\\\&amp;= (U_j \\cdots U_1 |\\xi, 0\\rangle)^\\dagger I (U_i \\cdots U_1 |\\xi, 0\\rangle) \\otimes (\\langle i|t\\rangle\\langle t|j\\rangle)\\hfill\\\\&amp;= (U_j \\cdots U_1 |\\xi, 0\\rangle)^\\dagger I (U_i \\cdots U_1 |\\xi, 0\\rangle) \\otimes 0\\hfill\\\\&amp;= 0\\hfill\\end{aligned}那么非交叉项呢？事实上按照上面的式子，我们已经表明，所有这些和式中只会剩下一项：\\begin{aligned}\\langle \\eta|(I \\otimes |t\\rangle\\langle t|)|\\eta\\rangle &amp;= (U_t \\cdots U_1 |\\xi, 0\\rangle)^\\dagger I (U_t \\cdots U_1 |\\xi, 0\\rangle) \\otimes (\\langle t|t\\rangle\\langle t|t\\rangle) \\hfill\\\\&amp;= I \\otimes 1\\hfill\\\\&amp;= I\\hfill\\end{aligned}第二项如法炮制：\\begin{aligned}\\langle \\eta|(I \\otimes |t-1\\rangle\\langle t-1|)|\\eta\\rangle &amp;= (U_{t-1} \\cdots U_1 |\\xi, 0\\rangle)^\\dagger I (U_{t-1} \\cdots U_1 |\\xi, 0\\rangle) \\otimes (\\langle t-1|t-1\\rangle\\langle t-1|t-1\\rangle)\\hfill\\\\&amp;= I \\otimes 1\\hfill\\\\&amp;= I\\hfill\\end{aligned}后面两项同理展开，不过需要注意左边的不再是 $I$ 了：\\begin{aligned}\\langle \\eta|(U_t \\otimes |t\\rangle\\langle t -1 |)|\\eta\\rangle &amp;= \\langle \\eta_{t}|(U_t \\otimes |t\\rangle\\langle t-1|)|\\eta_{t-1}\\rangle\\hfill\\\\&amp;= (U_{t} \\cdots U_1 |\\xi, 0\\rangle)^\\dagger U_t (U_{t-1} \\cdots U_1 |\\xi, 0\\rangle) \\otimes (\\langle t|t\\rangle\\langle t-1|t-1\\rangle)\\hfill\\\\&amp;= I \\otimes1\\hfill\\\\&amp;= I\\hfill\\\\\\langle \\eta|(U_t \\otimes |t -1\\rangle\\langle t |)|\\eta\\rangle &amp;= \\langle \\eta_{t-1}|(U_t^\\dagger \\otimes |t-1\\rangle\\langle t|)|\\eta_{t}\\rangle\\hfill\\\\&amp;= (U_{t-1} \\cdots U_1 |\\xi, 0\\rangle)^\\dagger U_t^\\dagger (U_{t} \\cdots U_1 |\\xi, 0\\rangle) \\otimes (\\langle t-1|t-1\\rangle\\langle t|t\\rangle)\\hfill\\\\&amp;= I \\otimes1\\hfill\\\\&amp;= I\\hfill\\\\\\end{aligned}于是我们成功地表明了 $\\langle \\eta|H_{prop}|\\eta \\rangle = 0$。 接下来的式子就简单了：\\begin{aligned}\\langle \\eta|H_{in}|\\eta \\rangle = 0, \\langle \\eta|H_{out}|\\eta \\rangle &lt; \\varepsilon\\end{aligned}合起来就有： \\lambda(H) \\leqslant\\langle \\eta|H|\\eta \\rangle = \\langle \\eta|H_{out}|\\eta \\rangle < \\varepsilon嗯，确实非常好证明，容易到原作者都没有仔细写。接下来的第二部分会更加复杂一些：假设 $V_x$ 对于所有输入 $|\\xi, 0\\rangle$ 的接受概率都小于 $|\\varepsilon\\rangle$ ，令 $\\mathcal S_{prop}$ 为 $H_{prop}$ 的基态空间，很显然，它是 $2^N$ 维的，它的一组基是： |\\eta_i\\rangle = \\frac {1}{\\sqrt{T + 1}} \\sum\\limits^{T}_{t=0} U_t\\cdots U_1|i\\rangle \\otimes |t\\rangle其中 $|i\\rangle$ 表示 $N$ 个进入计算的量子比特，它们的特征值为 $0$，$\\mathcal S_{prop}$ 事实上就表征了正确的状态转移方式。接下来我们要对这个子空间应用可爱的投影引理，为了做到这一点，首先需要标明 $J_{prop}H_{prop}$ 对处于 $\\mathcal{S}_{prop}^\\perp$ 中的状态给出了一个 $\\text{poly}(N)$ 的惩罚值，也就是说，$H_{prop}$ 中最小的非零特征值反比于某个关于 $N$ 的多项式。于是，我们给出以下命题： $\\exists c &gt; 0$，使得 $H_{prop}$ 的最小非零特征值至少为 $c/T^2$。 这个命题的证明也不太复杂，首先构造一个基的变换： W = \\sum\\limits_{t=0}^T U_t\\cdots U_1 \\otimes |t\\rangle\\langle t|将其应用到 $H_{prop}$ 上： W^\\dagger H_{prop}W = \\sum\\limits_{t=1}^T I \\otimes \\frac 1 2(|t\\rangle\\langle t| + |t-1\\rangle\\langle t-1| - |t\\rangle\\langle t-1| - |t-1\\rangle\\langle t|)这里的计算方式和上面的证明很像，就不再重复了。当然，这个变换并不会改变 $H_{prop}$ 的特征谱，但它成功地将我们的哈密顿块对角化了： W^\\dagger H_{prop}W = I \\otimes \\begin{bmatrix} \\frac 1 2 & -\\frac 1 2 & 0 &&&\\cdots& 0\\\\ -\\frac 1 2 & 1 & -\\frac 1 2 & 0 & \\ddots && \\vdots\\\\ 0 & -\\frac 1 2 & 1 & -\\frac 1 2 & 0 & \\ddots &\\vdots \\\\ &\\ddots&\\ddots&\\ddots&\\ddots&\\ddots&\\vdots\\\\ \\vdots&&0 &-\\frac 1 2 & 1 & -\\frac 1 2 & 0 \\\\ &&&0 &-\\frac 1 2 & 1 & -\\frac 1 2 \\\\ 0 &&\\cdots&& 0 &-\\frac 1 2 &\\frac 1 2 \\end{bmatrix}漂亮，这样我们就可以开始估计 $(T+1) \\times (T+1)$ 小矩阵的特征值了。作者在这里说，使用“标准方法”即可，本来打算用 Givens 变换求解，但是算着实在烦人，于是在这里给出一个（看上去）稍微简单点的思路： 考虑到第一行和最后一行破坏了这个矩阵的美感，我们先把它们无视掉，来看一看中间这个矩阵： \\begin{bmatrix} 1 & -\\frac 1 2 & 0 &&&\\cdots& 0\\\\ -\\frac 1 2 & 1 & -\\frac 1 2 & 0 & \\ddots && \\vdots\\\\ 0 & -\\frac 1 2 & 1 & -\\frac 1 2 & 0 & \\ddots &\\vdots \\\\ &\\ddots&\\ddots&\\ddots&\\ddots&\\ddots&\\vdots\\\\ \\vdots&&0 &-\\frac 1 2 & 1 & -\\frac 1 2 & 0 \\\\ &&&0 &-\\frac 1 2 & 1 & -\\frac 1 2 \\\\ 0 &&\\cdots&& 0 &-\\frac 1 2 & 1 \\end{bmatrix}这个矩阵是个 Toeplitz 矩阵，其特征值还是比较容易获得的：考虑其特征多项式为 $\\varphi_n(\\lambda)$ ，若其为一个 $n \\times n$ 矩阵，则可以很容易地得到： \\varphi_n(\\lambda) = (1-\\lambda)\\varphi_{n-1}(\\lambda) - \\frac 1 4 \\varphi_{n-2}(\\lambda)然后观察一下这个式子，考虑到： \\varphi_0(\\lambda) = 1, \\varphi_1(\\lambda) = 1-\\lambda发现它长得和第二类切比雪夫多项式的递推式长得有点神似： \\varphi_0(x) = 1, \\varphi_1(x) = 2x, \\varphi_n(x) = 2x\\varphi_{n-1}(\\lambda) - \\varphi_{n-2}(\\lambda)那么我们可以考虑折腾一下这个矩阵，让它能够变成切比雪夫多项式的样子，只要把 $-\\frac 1 2$ 这个系数提出来就可以了。这样的话得到的多项式就是： \\varphi_0 (\\lambda) = 1, \\varphi_1(\\lambda) = 2-\\lambda, \\varphi_n(\\lambda) = (2-\\lambda)\\varphi_{n-1}(\\lambda) - \\varphi_{n-2}(\\lambda)这下子我们就大功告成，只欠换元，令 $2x = 2-\\lambda$，于是 $\\phi_n(x) = \\varphi_n(\\lambda) = U_n(x)$。 接下来的事情就好办了，众所周知，第二类切比雪夫多项式的表达式是： U_n = \\frac{\\sin((n+1)\\cos^{-1}x)}{\\sin(\\cos^{-1}x)}, |x| \\leqslant 1它的根就是： \\cos\\frac{k\\pi}{n+1}, k = 1, 2, \\cdots, n所以我们求出现在这个矩阵的特征值为： 1-\\cos\\frac{k\\pi}{n+1}, k = 1, 2, \\cdots, n接下来我们考虑按照和最后一行展开原来的矩阵，新的特征多项式为： \\psi(\\lambda) = (\\frac 1 2-\\lambda)\\xi(\\lambda) - \\frac 1 4 \\xi\\prime(\\lambda)其中：\\begin{aligned}\\xi(\\lambda) &amp;= (\\frac 1 2 - \\lambda)\\varphi_{T-1}(\\lambda) - \\frac 1 4 \\varphi_{T-2}(\\lambda)\\hfill\\\\\\xi\\prime(\\lambda) &amp;= (\\frac 1 2 - \\lambda)\\varphi_{T-2}(\\lambda) - \\frac 1 4 \\varphi_{T-3}(\\lambda)\\hfill\\\\\\end{aligned}嗯，看上去很丑。整理一下，就有： \\psi(\\lambda) = (\\frac 1 2 - \\lambda)^2\\varphi_{T-1}(\\lambda) - \\frac 1 2(\\frac 1 2 - \\lambda)\\varphi_{T-2}(\\lambda)+ \\frac 1 {16} \\varphi_{T-3}(\\lambda)看上去似乎就没什么办法了，但是别忘了我们有 $\\varphi_n(\\lambda)$ 的递推公式，现在这是我们最后的希望了：\\begin{aligned}\\psi(\\lambda) &amp;= (\\frac 1 2 - \\lambda)^2\\varphi_{T-1}(\\lambda) - \\frac 1 2(\\frac 1 2 - \\lambda)\\varphi_{T-2}(\\lambda) - \\frac 1 4 \\varphi_{T-1}(\\lambda) + \\frac{1-\\lambda}4 \\varphi_{T-2}(\\lambda)\\hfill\\\\&amp;= (\\lambda^2-\\lambda) \\varphi_{T-1}(\\lambda) + \\frac{\\lambda}{4}\\varphi_{T-2}(\\lambda)\\hfill\\\\&amp;= -\\lambda((1-\\lambda)\\varphi_{T-1}(\\lambda) - \\frac 1 4\\varphi_{T-2}(\\lambda))\\hfill\\\\&amp;= -\\lambda\\varphi_{T}(\\lambda)\\hfill\\end{aligned}诶嘿，这下我们就发现了，原来加边矩阵的特征值和没加边的情形是一样的。所以，特征值中的较小值就是： 1 - \\cos\\frac{\\pi}{T+1} \\geqslant \\frac c {T^2}这个不等式挺显然的，就不做证明了。 好，准备工作做完了，接下来我们就要应用我们的投影引理了。上面表明了，$J\\geqslant cJ_{prop}/T^2$。我们取： H_1 = H_{out} + J_{in}H_{in}， H_2 = J_{prop}H_{prop}只需要令 $J_{prop} = JT^2/c=\\text{poly}(n)$，那么 $\\lambda(H) \\geqslant \\lambda(H_1|_\\mathcal {S_{prop}}) - \\frac 1 8$。接下来我们考察 $\\lambda(H_1|_\\mathcal {S_{prop}})$。 考虑 $\\mathcal{S}_{in} \\subset \\mathcal{S}_{prop}$ 为 $H_{in}|_{\\mathcal S_{prop}}$ 的基态空间，很显然，它也是一个 $2^m$ 维子空间，其基为 |\\eta_i\\rangle = \\frac{1}{\\sqrt{T+1}}\\sum\\limits^T_{t=0}U_t\\cdots U_1|j, 0\\rangle \\otimes |t\\rangle其中 $|j\\rangle$ 为前面 $m$ 个计算用的量子比特的基。接下来在 $\\mathcal S_{prop}$ 中应用投影引理，其中： H_1 = H_{out}|_{\\mathcal S_{prop}}, H_2 = J_{in}H_{in}|_{\\mathcal S_{prop}} 很显然地，$||H_1|| \\leqslant ||H_{out}|| = T+1 = \\text{poly}(N)$。而任意处于 $\\mathcal S_{in}^\\perp$ 中的 $H_2$ 的特征向量的特征值都至少是 $J_{in} / (T+1)$，因此，可以找到 $J_{in} = \\text{poly}(N)$ 使得 $\\lambda(H_1+H_2) \\geqslant \\lambda(H_{out}|_{\\mathcal S_{in}}) - \\frac 1 8$。 根据我们在上面给出的接受概率小于 $\\varepsilon$ 的假定，我们可以表明，$\\lambda(H_{out}|_{\\mathcal S_{in}}) &gt; 1-\\varepsilon$。综上所述，$\\lambda(H) \\geqslant 1-\\varepsilon - \\frac 2 8 = \\frac 3 4 - \\varepsilon$。","link":"/blog/dab644cb/"},{"title":"GDB 源码分析 03：函数前导代码分析（下）","text":"在上一次分析中，我们基本上通读了对前导代码分析实现通用的支持的代码，在这里，我们将针对多种不同架构的代码进行分析。 这篇文章将不可避免地要求对一些架构中汇编指令有一定的熟悉程度，但基本上只需要了解汇编指令的含义即可，可以参考相关的开发者指南。我们将首先从相对简单的 RISC-V 架构的分析开始。 RISC-V 架构的前导代码分析文件：riscv-tdep.c 在 RISC-V 架构下的前导代码分析只被用来在没有充足的 DWARF 调试信息的情况下跳过函数的前导代码1，因此它确实是比较简单的。但是，它仍然有保留栈帧缓存（frame cache）等相关功能，这使得它的代码还是不太简单。 核心函数定义如下： 1static CORE_ADDR riscv_scan_prologue(struct gdbarch *gdbarch, CORE_ADDR start_pc, CORE_ADDR end_pc, struct riscv_unwind_cache *cache); 这里面的大部分结构体应该都是在之前的分析中曾经见过的。唯一可能有点陌生的就是最后一个关于栈帧缓存的结构体，其形态如下： 1234567struct riscv_unwind_cache { int frame_base_reg; int frame_base_offset; trad_frame_saved_reg *regs; struct frame_id this_id; CORE_ADDR frame_base;} 其中前两个参数表明栈基地址寄存器和实际栈基地址到栈基地址寄存器的偏移量，后两个参数标识栈帧中存储的寄存器和栈帧的 id，在后面栈帧分析相关代码我们会仔细讨论；最后一个参数保存在进入此栈帧时的堆栈指针。 在进入函数时，首先进行了一系列初始化： 12CORE_ADDR cur_pc, next_pc, after_prologue_pc;CORE_ADDR end_prologue_addr = 0; 然后试图使用调试信息找到前导代码的上限，具体方法留待分析符号表的时候再讨论： 1234567after_prologue_pc = skip_prologue_using_sal (gdbarch, start_pc);if (after_prologue_pc == 0) { after_prologue_pc = start_pc + 100;}if (after_prologue_pc &lt; end_pc) { end_pc = after_prologue_pc;} 如果不能利用调试信息完成跳过，则我们给我们的代码起点加上一个足够大的数假装我们得到了一个合理的预测值；然后，如果这个预测值比原先传入的预测值更加精确，也就是说更小，那就采用这个预测值。 然后初始化寄存器并进行分析： 12345pv_t regs[RISCV_NUM_INTEGER_REGS];for (int regno = 0; regno &lt; RISCV_NUM_INTEGER_REGS; regno++) { regs[regno] = pv_register(regno, 0);}pv_area stack(RISCV_SP_REGNUM, gdbarch_addr_bit(gdbarch)); 然后我们根据 start_pc 和 end_pc 开始正式进行前导代码分析，这是一个大循环： 123for (next_pc = cur_pc = start_pc; cur_pc &lt; end_pc; cur_pc = next_pc) { /* Do something*/} 首先对代码进行解码，这一步是非常简单的，被隐藏的实现因为并不重要所以不多做分析，唯一需要说明的是指令长度不大于零表明解码失败： 1234struct riscv_insn insn;insn.decode(gdbarch, cur_pc);gdb_assert(insn.length() &gt; 0);next_pc = cur_pc + insn.length(); 接下来我们需要寻找用来调整栈的指令，这些指令大概有以下几种： addi 和 addiw 引起的，其目标寄存器和源寄存器都是 sp，这是用来调整栈的基地址的； sw 和 sd 引起的，其源寄存器为 fp 和 sp，这是用来向栈中保存寄存器的； addi 引起的，源寄存器为 sp 而目标寄存器为 fp 的以及 add 和 addw 引起的，源寄存器为 sp 和 zero，目标寄存器为 fp的，这是用来设置栈帧的； 其代码因为结构很简单但是较长所以不在此展示了，基本上就是利用中篇写好的各种工具，改寄存器的动数组，改内存的动 stack，除此之外做一下寄存器序号边界的检验即可。 接下来是对一些其他指令的处理，包括 auipc，lui，addi，add，ld，lw 和 mv，如果这一串代码到了无法再分析的地方，那就终止。终止时的 end_prologue_addr 设置为当前的 pc 值，即 curr_pc。 再往下就需要处理有 cache 的情形，这些会在我们讨论栈帧分析的时候再做讨论。 注意到，这里前导代码的含义似乎已经与最开始大相径庭。前导代码似乎包含了很多单纯的算术运算，对栈和寄存器的操作基本上都被置于其中。这一方面是因为更充分地分析函数运行时的数据流能够有助于后续的分析，另一方面是因为日渐复杂的编译器使得前导代码本身不再简单，因此必须以这种复杂的方式被加以分析——哪怕分析的内容是过度的，它也是“安全”的，这也是保守估计思想的一环。 i386 前导代码分析文件：i386-tdep.c 前面已经讲过，对于 RISC-V 架构代码的前导代码分析是相对简单的，哪怕循环中的结构看起来很长，它还是易懂且亲切的。但是对于 i386 架构代码来说，它的代码复杂度几乎让人望而却步。接下来，我们走出摇篮，开始直面这个不可名状的巨物，首先从第一根触手——啊不，第一个函数开始： 1234567891011static CORE_ADDRi386_analyze_prologue (struct gdbarch *gdbarch, CORE_ADDR pc, CORE_ADDR current_pc, struct i386_frame_cache *cache) { pc = i386_skip_endbr(pc); pc = i386_skip_noop(pc); pc = i386_follow_jump(gdbarch, pc); pc = i386_analyze_struct_return(pc, current_pc, cache); pc = i386_skip_probe(pc); pc = i386_analyze_stack_align(pc, current_pc, cache); pc = i386_analyze_frame_setup(gdbarch, pc, current_pc, cache); return i386_analyze_register_saves(pc, current_pc, cache);} 啊哈，确实有点吓人不是吗？别急，我们慢慢来，你可以喝一杯茶，或者先读读 Intel 的开发者手册，或者看看 CTFWiki 中关于 ROP 的段落……如果准备好了，那我们直接开始。 第一个函数是对 endbr 的处理——明白为什么要读读 ROP 的相关内容了嘛？如果你没读，那也没事，在这里简单解释一下花不了多少时间。当我们面对一个程序且需要绕过它的部分功能，比如注册机时，我们会操作函数的返回地址，也就是说，面向返回地址编程（return-oriented programming，ROP），而 endbr 就是 Intel 引入的 CET 技术的结果： 当一个跳转发生时，CPU 的状态机切换到 WAIT_FOR_ENDBRANCH 状态，它会确保跳转发生后的下一条指令一定是 endbr 指令。但是，这个指令本身没有意义，它不执行，只起到了缩小攻击面的作用。 所以，明白我们要做什么了吗？没错，直接跳过它。因为它是函数入口必不可少的东西，但是对我们来说，不能说是举足轻重，也可以说是毫无意义了。因此，这个函数就是这样： 1234567891011static CORE_ADDR i386_skip_endbr(CORE_ADDR pc) { static const gdb_byte endbr32[] = {0xf3, 0x0f, 0x1e, 0xfb}; gdb_byte buf[sizeof(endbr32)]; if (target_read_code(pc, buf, sizeof(endbr32))) { return pc; } if (memcmp(buf, endbr32, sizeof(endbr32))) { return pc; } return pc + sizeof(endbr32);} 如果读不出指令，终止；如果不是 endbr32，终止——这是因为 CET 是可选开启的，可以调整为不需要 endbr 也能正常执行；如果是，跳过它。 i386_skip_noop 函数当然也是类似的：跳过 nop 指令以及一些功能类似的指令，它的框架如下： 123456789101112static CORE_ADDR i386_skip_noop(CORE_ADDR pc) { gdb_byte op; int check = 1; if (target_read_code(pc, &amp;op, 1)) { return pc; } while (check) { check = 0; /* Do something*/ } return pc;} 也是一样的，如果不能成功读取指令，那就直接终止，接下来的 nop 指令，有几个算几个，全都跳过。 首先检查 nop 指令本体，这很简单： 1234567if (op == 0x90) { pc += 1; if (target_read_code(pc, &amp;op, 1)) { return pc; } check = 1;} else // ... 先在这里停一下，稍微注意一下，i386 的非定长特征赤裸裸地暴露在我们眼前了。nop 指令只有 1 个字节长，而 endbr32 则是 4 个字节，这使得很多处理方式不够优雅，而不太优雅的情形还不止于此，下面的操作，则是一个更加不优雅但有用的案例： 123456789101112/* cont. */ if (op == 0x8b) { if (target_read_code(pc + 1, &amp;op, 1)) { return pc; } if (op == 0xff) { pc += 2; if (target_read_code(pc + 1, &amp;op, 1)) { return pc; } check = 1; }} 哦，忘了说了，0x8bff 是一条 mov edi, edi 指令。事实上，0x89ff 也是——这是因为这两种编码都是成立的。当然，这里只检查了前一种情况，这是因为…… 因为 Windows 的系统动态链接库用到了这条指令！每个函数都会以五个 0x8bff 开头，在执行的时候，CPU 自然而然地将其解析为 nop 的等价指令，当然也就不会发生什么。而这五个指令可以被填充成五个短程跳转指令，这就使得热更新变得可能：只需要改变这几个空位，就可以改变函数的一些功能。 那么，在直面了这里的险恶之后，我们就要被迫思考这可能带来什么结果了。还记得吗，在前面的分析中，我们从来不考虑跳转这回事，但是这时候我们不得不考虑了——如果发生了跳转，它还是在函数最开头，在分析都没有开始的地方，而且对于 Windows 动态链接库函数这可能是普遍的，那么放弃就显得有些过于仓促了。所以，下一个函数就是要解决这个问题： 1234567891011121314151617181920212223242526272829static CORE_ADDR i386_follow_jump(struct) { enum bfd_endian byte_order = gdbarch_byte_order(gdbarch); gdb_byte op; long delta = 0; int data16 = 0; if (target_read_code(pc, &amp;op, 1)) { return pc; } if (op == 0x66) { data16 = 1; op = read_code_unsigned_integer(pc + 1, 1, byte_order); } switch (op) { case 0xe9: if (data16) { delta = read_memory_integer(pc + 2, 2, byte_order); delta += 4; } else { delta = read_memory_integer(pc + 1, 4, byte_order); delta += 5; } break; case 0xeb: delta = read_memory_integer(pc + data16 + 1, 1, byte_order); delta += data16 + 2; break; } return pc + delta;} 如果对 i386 不熟悉的话，想必这时候已经有点晕了。这段代码事实上就是把 pc 跳到跳转的目标地址，支持的是 jmp 指令，也就是 0xe9 和 0xeb，它的编码自己查手册应当不难。需要注意的是 0x66 前缀，如果直接查或许查不到，它的意思是 data16，也就是说偏移量按 16 bit 解读，在我们这里，它事实上没有带来任何本质上的不同。 下一个函数的名字可能并不太容易让人理解它在表达什么，首先要考虑的是一个问题，一个函数如何返回结构体或指针？很显然，它将不得不在栈中开辟空间用以填充这个指针，这是一段额外的代码： 123popl %eax 0x58xchgl %eax, (%esp) 0x87 0x04 0x24 or xchgl %eax, 0(%esp) 0x87 0x44 0x24 0x00 这个函数就是用来跳过这种原型的，经历了上面几个函数，我想或许不需要附上代码也能大致猜出它长什么样子了，不过保险起见，代码如下： 12345678910111213141516171819202122232425262728293031323334static CORE_ADDR i386_analyze_struct_return(CORE_ADDR pc, CORE_ADDR current_pc, struct i386_frame_cache cache) { static gdb_byte_proto1[3] = {0x87, 0x04, 0x24}; static gdb_byte_proto2[4] = {0x87, 0x44, 0x24, 0x00}; gdb_byte buf[4]; gdb_byte op; if (current_pc &lt;= pc) { return pc; } if (target_read_code(pc, &amp;op, 1)) { return pc; } if (op != 0x58) { return pc; } if (target_read_code(pc + 1, buf, 4)) { return pc; } if (memcmp(buf, proto1, 3) != 0 &amp;&amp; memcmp(buf, proto2, 4 != 0)) { return pc; } if (current_pc == pc) { cache-&gt;sp_offset += 4; return current_pc; } if (current_pc == pc + 1) { cache-&gt;pc_in_eax = 1; return current_pc; } if (buf[1] == proto1[1]) { return pc + 4; } else { return pc + 5; }} 里面当然涉及到一些 cache 的处理，今天也不做分析——再分析下去这篇文章就太长了，鉴于我们已经花了太多笔墨来考虑这些具体的乏味的代码，接下来我们要快进了。 i386_skip_probe，跳过最开始的一次对 _probe 函数的调用： 123pushl constantcall _probeaddl esp, 4 i386_analyze_stack_align 跳过用来对齐栈的代码： 123leal reg, esp[4]andl esp, -16(or -256)pushl reg[-4] 或者 1234pushl regleal reg, esp[8]andl esp, -16(or -256)pushl reg[-4] i386_analyze_frame_setup 跳过建立栈帧的代码，即这个系列最开始介绍的那一段代码，当然，中间可能插入了别的东西； i386_analyze_register_saves 跳过存储寄存器的代码，也就是一开始的一连串 push。 好，快进结束。反思一下，这个代码和分析 RISC-V 的部分显得极为不同。分析 RISC-V 时，我们根据的是抽象解读和逐一代码的分析，而分析 i386 时我们将其分块，因为编译器生成的代码的形式是完全一致的。新老架构在这里如此不同，我们或许可以窥见技术世界中年迈者的从容和无奈…… 1. 关于 DWARF 调试信息的内容，在以后读到相关代码时将详细解释，在这里，只需要理解它是一种用来描述整个程序功能的信息文件即可。 ↩","link":"/blog/38fef59d/"},{"title":"从 Kolmogorov 复杂度到 Martin Löf 随机性检验（上）","text":"Per Martin-Löf, The Definition of Random Sequences, Information and Control, 9, 602-619(1966) 考虑一个在有限字母表中所有字符串的集合。记字符串 $x = \\xi_1\\xi_2\\dots\\xi_n$ 的长度 $l(x) = n$。接下来我们需要考虑的问题是，这个序列有多复杂？ 很显然，直观地讲，一个“随机”（我们很快会回到这一点上来）字符串要比一个有规律的字符串复杂地多。同样很直观地，我们可以把一个字符串的复杂度定义为某种“最简单的描述方法的长度”，Kolmogorov 算术复杂度就是从这个视角出发做出的形式化定义。 “描述方法”和 Kolmogorov-Solomonoff 定理接下来我们需要反思，“描述方法”应该如何被形式化地定义出来？既然本文的 tag 中带有计算理论，很自然的一种想法就是，“描述方法”就是一种算法。 在这里，我们使用“算法”这个概念来表达一种从一个有限二进制序列到一个有限字母表上的单子的映射。算法概念的更精确的形式化定义可以采用递归论的定义模式或者其他等价的形式（本文中将不再展开），这并不影响后文的探讨。 记 $A$ 为一种算法，$A(p) = x$，$p$ 为一个有限二进制串，$x$ 为一个有限字母表上的字符串，我们称 $p$ 是在算法 $A$ 下对 $x$ 的描述。接下来，延续我们前面的直观认识，我们可以定义相对于算法 $A$ 字符串 $x$ 的复杂度为： K_A(x) = \\min\\limits_{A(p) = x}(l(p))那么，很自然的一个问题是，是否一定存在这样的二进制串 $p$ 来对任意字符串 $x$ 做出描述？答案是否定的。只要考虑一个平凡的算法 $A_0$，它将任意二进制串 $p$ 都映射到字符串 $x_0$，那么，其他的字符串就都不能在这个算法下做出描述。因此，为了定义的严谨性，我们需要补充说明： K_A(x) = + \\infty, \\mathrm{if}\\ \\forall p, A(p) \\neq x现在，我们剩下的一个问题就是，我们不能摆脱算法 $A$ 对我们的定义的限制。我们在考虑一个序列的复杂度时，很显然不是要考虑一个字符串“在某种描述方法下”的复杂度，我们需要使得它成为一个只与字符串 $x$ 有关的数。为了实现这个定义，就需要引入 Kolmogorov-Solomonoff 定理： 存在一个算法 $A$，使得对于任意算法 $B$， K_A(x) \\leqslant K_B(x) + c其中 $c$ 为一常数，且其只与 $A$ 和 $B$ 有关。 这个算法在 Kolmogorov 的著作中被称为渐进最优的（asymptotically optimal），在 Solomonoff 的著作中被称为通用的（universial），上述定理的证明如果我不鸽的话会丢进本文的附录或者其他文章里。总之，现在我们已经有了一个最好的算法来讨论如何描述字符串 $x$，于是，我们就可以定义这个字符串的复杂度 $K(x) = K_A(x)$，称其为 Kolmogorov 复杂度，或者简单地称为复杂度。 同样地，我们可以引入条件复杂度的概念。考虑一个两变量的算法 $A(p, x)$，$x$ 为一个有限字母表上的字符串，若 $A(p, x) = y$，则我们将其称为在 $x$ 的条件下对（可能不同于 $x$ 的）字母表上的字符串 $y$ 的描述。称在 $x$ 的条件下相对于算法 $A$ 的 $y$ 的复杂度为： K_A(y \\vert x) = \\min\\limits_{A(p, x) = y}(l(p))这个定义也是很自然的，同样与上面的讨论和我们的直观感受匹配。幸运地是，亲爱的 Kolmogorov 先生同样给出了一个与上面的定理相对应的定理： 存在一个算法 $A$，使得对于任意算法 $B$， K_A(y \\vert x) \\leqslant K_B(y \\vert x) + c其中 $c$ 为一常数，且其只与 $A$ 和 $B$ 有关。 因此，我们也记 $K(y\\vert x) = K_A(y \\vert x)$，并称其为在 $x$ 的条件下 $y$ 的条件复杂度。 很直观的，对于任意长度为 $n$ 的二进制串 $x$，都有 K(x \\vert n) \\leqslant n + c其中 $n$ 是用于描述这个字符串所用的最多比特数，在最坏情况下，我们需要将整个串都硬编码到代码当中，那么当然需要 $n$ 的长度。而 $c$ 则是机器相关的常数。 同时，我们可以尝试给出一个下界，满足 K(\\xi_1\\xi_2\\dots\\xi_n \\vert n) \\geqslant n - c的序列一共有 $(1-2^{-c})2^n$ 个。这个结论也很平凡，留给读者自证。 于是，我们发现，当 $n$ 很大的时候，会有很多字符串的复杂度的渐进上界是在 $O(n)$ 这个最大的级别的。Kolmogorov 指出，这可以使我们形式化地定义一个字符串的随机性。 一些评述看起来，对于二元算法的定义事实上意义不是很大，因为 $n$ 同样可以以 $\\log n$ 的复杂度编写到程序之中，这对于我们的结果并没有影响。事实上，很多较现代的论文提供了另外一种定义形式，例如 Peter D. Grünwald and Paul M. B. Vitányi, Algorithmic Information Theory 中给出的定义是： K(x) = \\min\\limits_{y, p: p(y) = x}(l(p) + l(y))这种定义形式似乎更加符合我们的预期。另外，也将一个对象的 Kolmogorov 编码定义为 $E*(x)$，是最短的能够打印 $x$ 然后停下的代码。 在这种定义形式下，我们就可以给出三种分类： 简单的对象：$O(\\log n)$，其原因已经在前面解释过了，因为 $n$ 需要被硬编码进去 完全偶然对象（completely random objects）：$n + O(\\log n)$，也是显然的 随机对象（stochastic objects）：$\\alpha n + o(n)$ 如果 $x_i$ 是一个随机变量 $X_i$ 的实现，其分布为 $P$，则这个对象是随机的，其中 $\\alpha &lt; 1$。比较常见的例子是二项分布，其 $\\alpha = H(p)$ 为二值熵（binary entrophy） H(p) = -p\\log p - (1-p) \\log(1-p)随机对象的情形可以类比于扔一个有缺陷的硬币，硬币的缺陷使得这个序列不再成为完全随机的，当然，很显然，$p = 0.5$ 时， $H(p) = 1$，序列还是完全偶然的。 另一个很遗憾的问题是，$K(x)$ 不是可计算的，在 [Li and Vitányi, 1997] 中，他们表明它是上半可计算的（upper semicomputable），或者简单地理解就是，它是可以被近似的，但近似算法很慢，且不能确定其终点。但是，也有一些方式来解决这个问题，比如通用编码（[Cover and Thomas, 1991]），最小描述长度原理（MDL, [Solomonoff, 1997]）等方式的近似。 Blum 和 Burgin 的公理系统对于这个领域来说也是相当重要的，他们给出了关于这些性质的普遍描述，如果不鸽的话大概也会专门开一篇文章来介绍他们的成果，嗯，如果不鸽的话。","link":"/blog/dd3e0497/"},{"title":"量子编程（Maksim Dimitrijev） Lecture 1","text":"在近二十年间，出现了两种量子计算的主要范式。一种是量子门编程模型（gate-based model of quantum computing），也叫通用量子计算（universal quantum computing）；另一种是量子退火方法（quantum annealing），也叫绝热量子计算（adiabatic quantum computing）。从数学角度上看，这两种模型具备同等的计算能力，但在实践上，两者有显著的不同。 前两次讲座主要介绍量子门编程模型，第一次讲座的内容包括量子比特（quantum bits, qubits）和量子门（quantum gates）、以及量子电路（quantum circuits）。 量子比特和量子门正如经典计算机一样，量子计算机也一样由门电路构成，不过其用来表示信息的单元为量子比特而非高低电平，其运算模块为量子门而非数字电路的逻辑门。 单个的量子比特单个量子比特为计算的基本单元，处在 $0$ 和 $1$ 的叠加状态（superposition）之中。我们可以使用类似描述振幅的方式去描述一个量子比特 $|\\psi\\rangle$： |\\psi\\rangle = \\psi_0|0\\rangle + \\psi_1|1\\rangle = \\big ( \\begin{matrix} \\psi_0 \\\\ \\psi_1 \\end{matrix} \\big )其中 $\\psi_0$ 和 $\\psi_1$ 均为复数。可以将 $|0\\rangle$ 和 $|1\\rangle$ 理解成一组基： |0\\rangle = \\begin{bmatrix} 1 \\newline 0 \\end{bmatrix} \\newline |1\\rangle = \\begin{bmatrix} 0 \\newline 1 \\end{bmatrix}这两个复数需要满足归一化条件： \\langle \\psi \\vert \\psi \\rangle = |\\psi_0|^2 + |\\psi_1|^2 = 1因此，我们可以表明， \\exists \\theta \\in [0, \\pi], \\mathrm{s. t.} |\\psi_0| = \\cos \\frac{\\theta}{2}, |\\psi_1| = \\sin \\frac{\\theta}{2}因为全局相位对其状态无影响，我们可以不失一般性地令 \\psi_0 = \\cos \\frac \\theta 2 \\newline \\psi_1 = e^{i\\phi}\\sin \\frac \\theta 2其中 $\\phi \\in [0, 2\\pi)$ 表示复系数之间的相对相位。 基于这些特性，我们可以在 Bloch 球面中表达一个量子比特，如下图： 其中的 $r^x, r^y, r^z$ 可以以投影的方式给出： \\mathbf r = \\begin{bmatrix} r^x \\newline r^y \\newline r^z \\end{bmatrix} = \\begin{bmatrix} \\sin\\theta\\cos\\phi \\newline \\sin\\theta\\sin\\phi \\newline \\cos\\theta \\end{bmatrix} = \\begin{bmatrix} \\langle\\psi|\\sigma^x|\\psi\\rangle \\newline \\langle \\psi|\\sigma^y|\\psi\\rangle \\newline \\langle \\psi|\\sigma^z|\\psi\\rangle \\newline \\end{bmatrix}其中 $\\langle\\psi|\\sigma|\\psi\\rangle = \\vert \\psi \\rangle^\\dagger \\sigma \\vert \\psi \\rangle$，其中 $A^\\dagger$ 表示 $A$ 的共轭转置，$\\sigma^x$，$\\sigma^y$，$\\sigma^z$ 为 Pauli 矩阵： \\sigma^x = \\begin{bmatrix} 0 & 1 \\newline 1 & 0 \\end{bmatrix}, \\sigma^y = \\begin{bmatrix} 0 & -i \\newline i & 0 \\end{bmatrix}, \\sigma^z = \\begin{bmatrix} 1 & 0 \\newline 0 & -1 \\end{bmatrix}关于 Bloch 球面，在第十讲中会有更详尽的叙述。需要注意，在进行测量时，一个量子比特会坍缩到 $0$ 或者 $1$ 的定态，其测量结果为 $0$ 的概率为 $|\\psi_0|^2$，为 $1$ 的概率为 $|\\psi_1|^2$。 量子门量子门可以看作是对 $|\\psi\\rangle$ 在 Bloch 球面上的旋转操作，事实上就是一些 $2 \\times 2$ 的酉矩阵。基于 Pauli 矩阵我们可以很方便的构造出绕坐标轴旋转的 Pauli 门： R^x(\\theta) = e^{-\\frac{i\\theta\\sigma^x}{2}} = \\begin{bmatrix} \\cos \\frac \\theta 2 & -i\\sin\\theta 2 \\newline -i\\sin \\frac\\theta 2 & \\cos \\frac \\theta 2 \\end{bmatrix} \\newline R^y(\\theta) = e^{-\\frac{i\\theta\\sigma^y}{2}} = \\begin{bmatrix} \\cos \\frac \\theta 2 & -\\sin\\theta 2 \\newline \\sin \\frac\\theta 2 & \\cos \\frac \\theta 2 \\end{bmatrix} \\newline R^z(\\theta) = e^{-\\frac{i\\theta\\sigma^z}{2}} = \\begin{bmatrix} e^{-\\frac{i\\theta}{2}} & 0 \\newline 0 & e^{\\frac{i\\theta}{2}} \\end{bmatrix} \\newline在实际的实现中，只会实现部分基本旋转，然后将其组合起来以实现真正的旋转门。例如，IBM Q 实现了 $R^x(\\frac\\theta 2)$ 和 $R^z(\\theta)$。 可以按照如下公式组合形成能够完成绕任意轴旋转的旋转门，其中 $\\vec n$ 是旋转轴的单位向量： R^{\\vec n}(\\theta) = e^{-\\frac{i\\theta\\vec n\\vec \\sigma}{2}} = I\\cos\\frac \\theta 2 - in_i\\sigma^i\\sin \\frac \\theta 2其中 $I$ 为恒等矩阵，$i = x, y, z$ 还有其他常用的门，罗列如下： X = \\sigma^x, Y = \\sigma^y, Z = \\sigma^z \\newline H = \\frac 1 {\\sqrt 2} \\begin{bmatrix} 1 & 1 \\newline 1 & -1 \\end{bmatrix}, T = \\begin{bmatrix} 1 & 0 \\newline 0 & e^{\\frac{i\\pi}{4}}\\end{bmatrix}, S = T^2 = \\begin{bmatrix} 1 & 0 \\newline 0 & i\\end{bmatrix}其中比较重要的是 $X$ 门，又称 $NOT$ 门、翻转门（flipping gate）；$H$ 门被称为 Hadamard 门，$T$ 和 $S$ 为相移门。 多个量子比特的情形在具备多个量子比特的情况下，只需要由 Kronecker 积（张量积，tensor product）将它们组合起来即可。我们首先构造这样一组基： |q_0q_1q_2\\dots q_{n-1}\\rangle = |q_0\\rangle \\otimes |q_1\\rangle \\otimes |q_2\\rangle \\otimes \\dots \\otimes|q_{n - 1}\\rangle其中 $q_i = 0, 1$，可以发现，$q_0q_1\\dots q_{n-1}$ 是一个大小在 $0$ 到 $2^n - 1$ 之间的数 $j$ 的二进制表示。记 $|q_0q_1…q_{n-1}\\rangle = |j\\rangle$，于是可以将一个 $n$ bit 的系统表示为 $\\sum\\limits_{j}\\psi_j|j\\rangle$。 涉及多个量子比特的门同样可以用 Kronecker 积表出，记 $U_i$ 为只对第 $i$ 个比特做 $U$ 操作的门，则： U_i = |q_0q_1\\dots q_{i-1}\\rangle U(q_i) |q_{i+1}q_{i+2}\\dots q_{n-1}\\rangle则很显然可以得出： U_i =\\underbrace{\\overbrace{I \\otimes I \\otimes \\cdots \\otimes I}^{i-1\\text{个}} \\otimes U \\otimes I \\otimes I \\cdots \\otimes I}_{n\\text{个}}当使用多个量子比特的系统时，很显然我们不只是想操作其中的某一个比特，而是要对其中的比特进行关联的操作，例如，当某比特为某状态时，对另一比特做某操作。受控的量子门（Controlled-U gate）就实现了这一点： CU_{i_1i_2}|q_0q_1\\dots q_{n-1}\\rangle = \\begin{cases} |q_0q_1\\dots q_{n-1}\\rangle, & q_{i_1} = 0 \\newline |U_{i_2}|q_0q_1\\dots q_{n-1}\\rangle, & q_{i_1} = 1 \\end{cases}一个暂时用不上的附注：在做 $CU$ 门运算之后，全局相位会变为相对相位。 常见的多比特门如下： CNOT = \\begin{bmatrix} 1&0&0&0\\newline 0&1&0&0\\newline 0&0&0&1\\newline 0&0&1&0 \\end{bmatrix}, CZ = \\begin{bmatrix} 1&0&0&0\\newline 0&1&0&0\\newline 0&0&1&0\\newline 0&0&0&-1 \\end{bmatrix}读者不难自证，它们之间存在如下联系： CNOT = (I\\otimes H)CZ(I \\otimes H)量子电路及其编程量子电路的结构类似下图： 上图表示的是量子电路中的一个 2 位半加器，最左边将几个比特初始化，从左向右依次由不同的量子门对每个比特进行处理，最终结果输出在右侧。 在实际用 Python 进行编程时，一般使用 qiskit 作为电路前端，后端模拟器有以下三种选择： qasm_simulator：初始值全 $0$，无噪声影响下的测量结果； statevector_simulator：在测量导致量子态坍缩之前的直接计算结果； unitary_simulator：给出计算过程的酉矩阵。","link":"/blog/dbb7e980/"},{"title":"量子编程（Maksim Dimitrijev） Lecture 2","text":"在上一次讲座中，简单地介绍了量子电路的基本概念和量子编程的方法。这一次课将分析几个量子电路的例子和代码实现，包括： 2 位半加器 量子近似优化算法（Quantum Approximation Optimization Algorithm, QAOA），以 Ising 问题为例 Grover 搜索 2 位半加器的实现2 位半加器的功能如下： |q_0q_1\\rangle|q_2q_3\\rangle \\rightarrow |q_0q_1\\rangle|q_0q_1 + q_2q_3\\rangle为了更好地理解其功能，我们给出几个例子： \\begin{array}{**lr**} |2\\rangle|1\\rangle \\rightarrow |2\\rangle|3\\rangle\\newline |2\\rangle \\dfrac{|0\\rangle + |1\\rangle}{\\sqrt{2}} \\rightarrow |2\\rangle \\dfrac{|2\\rangle + |3\\rangle}{\\sqrt{2}}\\newline \\dfrac{|0\\rangle + |1\\rangle}{\\sqrt{2}} \\dfrac{|0\\rangle + |1\\rangle}{\\sqrt{2}} \\rightarrow \\dfrac{|0\\rangle + |1\\rangle + |1\\rangle + |2\\rangle}{2} \\end{array}尤其值得关注的是第二行例子，我们似乎“并行地”做了加法，这种并行性有赖于量子电路的线性性，即对于电路 $U$，有 U(|\\psi_1\\rangle + |\\psi_2\\rangle) = U|\\psi_1\\rangle + U|\\psi_2\\rangle其电路实现已经在上一讲的例子中给出了，如下图： 从左往右，第一部分为初始化模块，这个模块通过一系列门电路将全 0 的初始值转化为我们所需要的值。 第二部分为量子傅里叶变换（Quantum Fourier Transform, QFT）模块，这个模块将信息从寄存器值转移到指数上来。我们不妨首先计算一下作用于 $q_2$ 和 $q_3$ 上的变换矩阵： \\begin{bmatrix} 1&1&1&1\\newline 1&-1&1&-1\\newline 1&i&-1&-i\\newline 1&-i&-1&i \\end{bmatrix} 未完待续","link":"/blog/42beb83a/"},{"title":"大概可能不难上手的 Vulkan 教程（1） 计算管线","text":"在这里，我们将使用 Rust 作为主要的编程语言开始对 Vulkan API (Home | Vulkan | Cross platform 3D Graphics) 的介绍。当然，许多其他语言都是可用的，例如更古老的 C++ + OpenGL/Vulkan 等等， Python + ModernGL 也是一个非常常见且易学的组合。这里之所以使用 Rust 是因为我喜欢，而且它的许多语言特性使得我们可以更少被语言本身的表达方式所限制，同时减少出错的可能。关于 Rust 的资料不多，可供参考的有 The Rust Programming Language - The Rust Programming Language (rust-lang.org) 以及 CS 110L: Safety in Systems Programming (stanford.edu) 等。在前几篇文章中，我们将首先使用 Vulkano 库 (vulkano - Rust (docs.rs)) 来引入对基本流程的介绍，并且完成两个小例子作为开胃小菜。然后，我们将从 Vulkano 库深挖下去，一方面分析它的原始代码，另一方面尝试引入更多图形学的概念来实现更加复杂的任务。前者更加偏向于底层架构，而后者更加偏向于上层的代码实现。 接下来，我们将假定读者已经对 Rust 的基础语法有所了解，并且已经完成了对 Vulkan SDK 的安装。我们使用的 rustc 版本为 1.66.2，Vulkano 版本为 0.32.3，Vulkan SDK 版本为 1.3.236，均为笔者写作时的最新版本。本文的主要参考资料来自 Vulkano 的官方文档：Vulkano。 作者也是初学者，所以出现错误在所难免，希望大家在看的过程中多参考其他资料，如果发现错误，敬请指正！另外，本文中涉及的专有名词很多事实上没有官方的翻译，看个乐就行，重要的地方应该都已经注上英文了。 下面是我们的主要流程，底图源于 Understanding Vulkan® Objects - GPUOpen。在这一批教程中，我们将涉及到其中的大部分——事实上，是所有画了蓝色方框的部分。 初始化 initialization我们首先使用 cargo 创建一个工程，称其为 compute_pipeline。为了使用 Vulkano 库，我们首先需要在 Cargo.toml 上添加相关的依赖： 12[dependencies]vulkano = &quot;0.32.3&quot; 接下来就可以在 main.rs 中使用相关的资源了。首先，为了使用 Vulkan API，我们需要载入对应的库。我们首先引入 vulkano::VulkanLibrary，这个类的构造函数会直接调用系统中已有的库并返回一个 Result 型对象。我们首先测试一下是否成功： 12345678use vulkano::VulkanLibrary;fn main() { let library = VulkanLibrary::new().expect(&quot;no local Vulkan library/DLL&quot;); for layer in library.layer_properties().unwrap() { println!(&quot;Available layer: {}&quot;, layer.name()); }} 这个例子在笔者这里输出如下： 1234567891011121314Available layer: VK_LAYER_NV_optimusAvailable layer: VK_LAYER_RENDERDOC_CaptureAvailable layer: VK_LAYER_VALVE_steam_overlayAvailable layer: VK_LAYER_VALVE_steam_fossilizeAvailable layer: VK_LAYER_OBS_HOOKAvailable layer: VK_LAYER_EOS_OverlayAvailable layer: VK_LAYER_EOS_OverlayAvailable layer: VK_LAYER_LUNARG_api_dumpAvailable layer: VK_LAYER_LUNARG_gfxreconstructAvailable layer: VK_LAYER_KHRONOS_synchronization2Available layer: VK_LAYER_KHRONOS_validationAvailable layer: VK_LAYER_LUNARG_monitorAvailable layer: VK_LAYER_LUNARG_screenshotAvailable layer: VK_LAYER_KHRONOS_profiles 这里我们测试的目标有两个：一是库的安装是否已经完成，如果没有，程序将会报错；二是现在的库中可用的层（layer）的种类，我们将在这里先引入并对这个概念稍作解释，在以后的深入分析中，我们才会更多地使用它。 我们知道，Vulkan 起到了应用程序（application）和硬件设备（device）的中间层的作用。作为这个中间层的重要组成部分，加载器（loader）是应用程序直接调用 Vulkan API 的入口点。这个加载器就是我们在创建 VulkanLibrary 对象时真正载入的东西，它因系统而异。对于 Windows 而言，它一般是一个叫做 vulkan-1.dll 的动态链接库；对于 Linux 而言，它则一般是 libvulkan.so.1；MacOS 和 Android 与之也不相同。这里我们的 Vulkano 库为我们封装简化了这个过程，使得我们可以简单地载入系统中已有的（事实上，是在环境变量中的）加载器文件。 而层则是由加载器控制的，在应用程序运行过程中注入的组成部分，它在加载器和硬件驱动（driver）之间，可以用来打断、改变或监测 Vulkan 函数调用的行为。因此，我们可以利用它做到很多事情，例如验证 API 的使用、追踪 API 的调用过程、协助完成调试过程、性能检测（profiling）等等。它们是动态载入，可以随时被打开和关闭的。我们在开发和调试过程中，往往会开启一些层来辅助开发，而在应用执行过程中，会将所有层关闭，从而避免对应用程序性能的过多影响。 距离硬件设备最近的一层则是驱动层。这一层通常是由硬件厂商提供的，将 Vulkan API 直接转化到硬件代码或者使用一些软件模拟来完成需要的过程的程序。加载器会识别设备上已有的驱动层程序，并且将代码的执行过程分派（dispatch）给对应的硬件。我们接下来要完成的就是对硬件的抽象。 在创建硬件抽象之前，我们首先需要创建一个 vulkano::Instance 类型的变量，它是一个 Vulkan 上下文（context），其中储存着一些在执行过程中使用的信息。如果你已经了解过 OpenGL，那么你可能对这个概念并不陌生，如果没有，也不要在意，我们会在后面反复看到这个概念的影子。在现在，我们只需要知道这提供了一个介于应用程序和加载器之间的联系。 接下来我们遍历输出所有找到的物理设备（physical device），总的代码如下： 123456789101112131415use vulkano::{VulkanLibrary, instance::{Instance, InstanceCreateInfo}};fn main() { let library = VulkanLibrary::new().expect(&quot;no local Vulkan library/DLL&quot;); let instance = Instance::new(library, InstanceCreateInfo::application_from_cargo_toml()).expect(&quot;create instance failed&quot;); let physical = instance .enumerate_physical_devices() .expect(&quot;could not enumerate devices&quot;); for dev in physical { println!(&quot;Device {:?} found&quot;, dev.properties().device_name); }} 你应当看到类似这样的输出： 12Device &quot;NVIDIA GeForce GTX 1650 Ti&quot; foundDevice &quot;Intel(R) UHD Graphics&quot; found 这就是在你的设备上所能找到的所有支持 Vulkan 的物理设备了。但是，为了更好地与物理设备完成通信，我们往往需要抽象出逻辑设备（logical device）来。它表达的是与一个物理设备之间开放的通信过程，这可能是 Vulkan API 中最为重要的概念之一。 执行模型 Execution Model我们前面说过，Vulkan 用逻辑设备（下面都仿照其 spec 称为设备，device）来抽象一个物理设备。每个设备都能暴露出一个或多个队列（queue），每个队列都可能与其他队列异步执行；这些队列被划分为一个个的族（family），每个族中的队列都有一些类似的特性。我们认为同一个族中的队列是相容的（compatible），为一族队列分配的工作可以被放在任何一个队列上执行。在规范中，我们定义一族队列可以支持的功能有：视频编码/解码（video encode/decode）、图像处理（graphics）、计算（compute）、传输（transfer）、稀疏内存管理（sparse memory management）。看起来有点复杂，是吧？简单地说，就是每个物理设备有一些队列，队列被划分为族，我们要拿一个族来创建逻辑设备。接下来看代码。 我们首先从 instance 暴露给我们的设备中选一个创建物理设备： 1234567let phy_device = instance .enumerate_physical_devices() .expect(&quot;could not enumerate devices&quot;) .next() .expect(&quot;device not found&quot;);println!(&quot;Device {:?} chosen&quot;, phy_device.properties().device_name); Rust 语言提醒：为什么不用 physical 直接去创建 phy_device ？因为我们使用了一个 for 循环遍历这个迭代器，隐式地调用了 into_iter() ，因此 physical 已经被借用走了~ 下一步我们去观察一下这个物理设备有哪些家族。也是一个遍历过程： 1234for family in phy_device.queue_family_properties() { println!(&quot;Find a queue family with {:?} queues with characteristic {:?}&quot;, family.queue_count, family.queue_flags);} 在笔者这里它的输出为： 123Find a queue family with 16 queues with characteristic QueueFlags { graphics: true, compute: true, transfer: true, sparse_binding: true, protected: false, video_decode: false, video_encode: false, _ne: NonExhaustive(()) }Find a queue family with 2 queues with characteristic QueueFlags { graphics: false, compute: false, transfer: true, sparse_binding: true, protected: false, video_decode: false, video_encode: false, _ne: NonExhaustive(()) }Find a queue family with 8 queues with characteristic QueueFlags { graphics: false, compute: true, transfer: true, sparse_binding: true, protected: false, video_decode: false, video_encode: false, _ne: NonExhaustive(()) } 就是这样，queue_count 指的是这个族中有几个队列，而 queue_flags 指的是这个族是否满足某些支持的功能。可能对于这些功能你暂且不太明白，但是没关系，我们后面会逐渐涉及。 接下来我们要从里面挑一个能跑图像操作的队列，也就是说 graphics 是 true 的队列。当然，我们已经看见了，事实上就是第一个，但不能这么赖皮，还是得写个搜索的： 12345678let queue_family_index = phy_device .queue_family_properties() .iter() .enumerate() .position(|(_, q)| q.queue_flags.graphics) .expect(&quot;couldn't find a graphical queue family&quot;) as u32;println!(&quot;Find queue family {:?} with graphics&quot;, queue_family_index); 想都不用想，结果当然是 0。拿到这个队列的序号之后，我们就要去创建对应的逻辑设备了。 123456789101112131415let (device, mut queues) = Device::new( phy_device, DeviceCreateInfo { queue_create_infos: vec![ QueueCreateInfo { queue_family_index, ..Default::default() }, ], ..Default::default() }).expect(&quot;fail to create device&quot;);println!(&quot;Device created on {:?}, with {:?} queues&quot;, device.physical_device().properties().device_name, queues.count()); 啊，当然，物理设备很明显，队列数也显然是 1。这里逻辑设备的队列和我们前面讲的物理设备的队列有一点差别，我们在后面会看到，我们是通过逻辑设备的队列向物理设备发放任务的，这个队列在物理设备上的意义暂且不必深究。为了方便我们就直接把它提取出来了： 1let queue = queues.next().unwrap(); Rust 语言提示：又是借用的问题，记得把上面那条 println 先注释掉哦 着色器 Shader好，我们现在有了一个告诉设备要干什么的方式，尽管我们还没说具体怎么做。那么先停下来考虑一下，我们要让设备来整点啥活。我们知道，GPU 最大的优势在于并行，因此，我们在这里首先完成一个看起来很符合它的要求的任务：绘制一个曼德博罗集（Mandelbrot set）的图像。 为什么说这个任务看起来符合它的要求呢？我们知道，并行性最重要的点在于各个线程（这里这样表述稍微有点不明确，参见下文）之间不能互相依赖，也就是说，A 线程不能用到 B 线程的运行结果，否则的话，A 就必须等到 B 执行完才能接着执行，这样会引起明显的资源上的浪费。而曼德博罗集是一个分形，但是我们不需要按照分型的方式去生成它。只需要检查每一个像素点对应的函数值在多次迭代之后是否发散就行。承此思路，具体的计算在写着色器代码的时候再解释。 接下来需要解释的是着色器代码。着色器代码语言常用的一般就是高级着色器语言（High-level shader language, HLSL）和 OpenGL 着色器语言（OpenGL shader language, GLSL），以及历久弥新的 Renderman 着色器语言（RenderMan shading language, RSL）。在这里我们用的是 Vulkan，它和 OpenGL 同样由 Khronos 这个公益组织维护，因此我们使用的也是 GLSL。这是一种（事实上这三种都是）类 C 的语言，它们的作用就可以看成是可以编译出 GPU 特定代码的编程语言。 当然，Vulkan 相对于 OpenGL 来说当然还是有两把刷子的。它引入了一个中间表示（intermediate representation）SPIR-V，首先在程序编译时就将 GLSL 代码编译成 SPIR-V 的中间表示，然后在运行时将其传给 Vulkan 来转化成架构特定代码并进行执行（其实这种表示在 OpenGL 中现在也获得了支持）。关于 SPIR-V 的指令集和编译过程等等细节我们将留待后面的文章具体讨论。顺便一提，另外两种在 GPU 代码生成过程中常用的中间语言是面向 NVIDIA 产品的 NVVM 和面向 AMD 产品的 ROCDL。 Talk is cheap. Let’s see the codes. 好，下面我们首先来写 GLSL 代码。这里我们要写的是一个计算着色器（compute shader），应该说这才是这篇文章的重点实现部分。但是我们不会过度深挖 GLSL 代码的写法，可以参考 OpenGL SuperBible 之类的参考书，当然，对于需要使用到的内容我们都会边写边做解释，如果不能插入正文的，会像上面对 Rust 的提示一样用小方块来做简要说明。 GLSL 代码的起手式就是要先表明它的版本。计算着色器是在 4.0 版本之后才被引入的，而为了符合时代要求，我们使用 4.60.7 的最新版本来进行介绍，它的版本号是 460，因此我们写： 1# version 460 core 跟在版本号后面的 core 可有可无，它与 compatibility 相对，后者会支持更多的过时的固定管线相关函数，关于管线（pipeline）的内容我们会在后面介绍。下面的代码基本上是抄的 vulkano 官方教程，接下来是布局（layout）的规定： 12layout(local_size_x = 8, local_size_y = 8, local_size_z = 1) in;layout(set = 0, binding = 0, rgba8) uniform writeonly image2D img; 这两句话解释起来稍微有点复杂，具体细节可能要在介绍 GPU 的体系结构之后才能讲清楚，在这里我们先做个概述。首先，我们知道，GPU 的前端是单指令多线程（single instruction multiple thread, SIMT）的模式，这里的每个线程（thread）具备自己的寄存器和 id，但是不具备独立的程序计数器（program counter, PC, 如果你是 Intel 爱好者可能会喜欢叫它指令指针 instruction pointer, IP, 但是这里我们会统一采用前者作为术语）或者栈，也不会被单独调度。要注意，这里的线程和一般操作系统里讲的是不一样的。其实这里更像 SIMD 架构下的车道（lane），指令和执行位置都是完全共享的，但前提和结果不是。 然后，我们把调度单元称为波面（wavefront, NVIDIA 的术语叫它封包 wrap, 我采用的是 AMD 的说法，因为它更形象，具体原因在讲 GPU 体系结构的时候会非常明显）。一个波面是一组线程的集合，每个波面具备自己的 PC，所以我们可以想象这样一个场景：不同的线程像水波一样同步向前，它们有一些具备相同的波面因而构成了一个连续的形体。 好，我们现在介绍了两层抽象，线程、波面，接下来是第三层，工作组（work groups, 或者线程组 thread groups，但是这容易造成混淆，因为它组织的事实上是波面而不是线程）。工作组的结构不像上面两者是硬件决定的，它事实上是由我们在着色器代码中定义的。但它也不是看起来那么随意，我们所有一个工作组中的波面都需要在同一个着色器单元（shader unit）中执行。因为工作组的一个特征是，它们具备共享内存（thread group shared memory, TGSM）。工作组的组织可以由局部大小（local size）来确定，为了编程的方便它有三个维度，最终每次执行的线程数是三者的乘积，我们的第一行语句设置的就是这个玩意。一个代码实现层面上的小提示是，事实上有默认值 1，所以上面的代码里 local_size_z 也是可以不用设置的。 第一行解释完了，绕晕了没？因为没有解释硬件实现，所以这些概念的引入显得非常生硬。在现在，实用起见我们只需要知道两件事： 每次调用一遍着色器代码都是一个线程； 我们在这里设定了同时执行并且具备共享内存的工作组规模。 接下来我们看下一行。计算着色器输出的硬件模型与后面要介绍的着色器有些不同，它是一个无序访存视图（unordered access view, UAV）。这个玩意长得就像是一块缓冲区，但是它带有一些额外的特征。第一，正如它的名字暗示的，它是无序访问的，也就是说，一旦一条指令要求访问它，它会被立刻访问。这意味着，当我们写 compute shader 时，必须保证各个线程之间的数据没有时间上的依赖关系，为什么这很重要呢？因为它具备的第二个特性，随机访存（random access）。各个线程都可以写到 UAV 的任意位置上去，这意味着我们的两个线程可以写在同一个位置。因此，如果我们有线程 A 和线程 B 往同一个位置上写不同的结果，那么最终出来的是谁的结果只有执行它的显卡才能知道。第三个特性是原子操作，我们会在后面讲架构的时候详细讨论。 这些事情和我们的第二行代码有什么关系呢？在现在看来，答案是没有关系。我们只定义了这个变量是 uniform writeonly image2D 类型的 img 也就是说一个全局、只写的二维图像以及它的描述子（descriptor）。关于描述子的内容我们放在下一节介绍，现在我们只要知道，我们是把它当成了程序的输出来用就行。注意，计算着色器本身没有输出，这只是被当成了一个输出。 然后我们进入了 main 函数，这是我们的入口点： 1234567891011121314151617void main() { vec2 norm_coordinates = (gl_GlobalInvocationID.xy + vec2(0.5)) / vec2(imageSize(img)); vec2 c = (norm_coordinates - vec2(0.5)) * 2.0 - vec2(1.0, 0.0); vec2 z = vec2(0.0, 0.0); float i; for (i = 0.0; i &lt; 1.0; i += 0.005) { z = vec2( z.x * z.x - z.y * z.y + c.x, z.y * z.x + z.x * z.y + c.y ); if (length(z) &gt; 4.0) { break; } } vec4 to_write = vec4(vec3(i), 1.0); imageStore(img, ivec2(gl_GlobalInvocationID.xy), to_write);} 啊，没啥好看的不是吗，代码应该都已经明明白白地写在这里了。这里需要解释的是： gl_GlobalInvocationID 是自带的传入参数，标志着这个线程在调用的哪个位置，在我们这个例子里，就是它调用的是哪个点； 我们把模长大于 4.0 作为发散的标准，不然算起来就永无止境了； 其他函数的含义应当很明确，可以参考 GLSL 4 - docs.gl 我们不会去解释每个变量和每一步操作，当然，否则的话这个教程就永远写不完了。着色器代码的写法可以参考很多已有的教程，反正基本上都能用。举几个简单的例子：WebGL2 Shaders and GLSL (webgl2fundamentals.org) 是基于 WebGL2 的介绍，LearnOpenGL - Shaders 是基于 OpenGL 的介绍，Creating a Context - ModernGL 5.8.0 documentation 基于 ModernGL，等等，我的收藏夹中有大量相关的内容，虽然用的 API 不同，但只要是 GLSL 的写法，都可以参考。最经典的纸质参考书的话，可能就是 OpenGL SuperBible 了（中译名为 OpenGL 超极宝典，超土不是吗）。 好了，下一步是把着色器代码引入到我们的程序中，为了偷懒我们这里使用 vulkano_shaders 这个库，先在 Cargo.toml 的 dependency 项中加入： 1vulkano-shaders = &quot;0.32.0&quot; 按照本教程的惯例，这当然也是写作时的最新版本。它的用法参考 vulkano_shaders - Rust (docs.rs)，我们这里就直接在程序里调用它了： 12345678mod cs { vulkano_shaders::shader!{ ty: &quot;compute&quot;, path: &quot;shaders/cshader.comp&quot; }}let shader = cs::load(device.clone()).expect(&quot;failed to create shader module&quot;); 当然，着色器代码被放在了 shaders/cshader.comp 下，这是为了避免代码过于脏乱，你也可以模仿示例把它嵌入在文件里面，这都无所谓。 管线 PipelinePipeline 这个词想必读者都不陌生。一般的翻译是流水线，或者装配链，不妨想象汽车厂的工人每个人都在一个固定位置上处理传送带上送来的工件。硬件上的流水线早在 CPU 上就已经被引入了，我们大概已经熟知它的四个典型阶段：取指（fetch），解码（decode），执行（execute），写回（write back）。其中前面两个阶段在 Intel 的术语里被称为前端（front end），后面两个阶段被称为后端（backend）。前端完成的操作就是取指令然后将指令解码成微指令（micro-operation, μop），过程中的重要部件是分支预测器（branch predictor）；后端则是将微指令分派（dispatch）到对应的执行单元，并完成读写寄存器堆和缓存的操作，其中主要的操作就是寄存器重命名（register renaming）和重排序（reorder）。 如果上面这些概念陌生的话或许得去参考一些体系结构的相关资料，毕竟我们在后面提到 GPU 的体系架构是会预设对前面这些内容有所了解。但是现在这都不是重点，我们只需要知道一点：我们在 GPU 上也有一些流水线，这就行了。 事实上，在图形学中我们更常把这种流水线称为是一个管线。它可以被想象成一个水管，指令像水流一样在里面流过——当然，这和上面波面的那个比喻很同意，这很好。接下来我们要做的是往这个管线里面灌水，也就是说，填充指令。因此，我们首先要创建一个流水线： 1234567let compute_pipeline = ComputePipeline::new( device.clone(), shader.entry_point(&quot;main&quot;).expect(&quot;shader entry point not found&quot;), &amp;(), None, |_| {}).expect(&quot;fail to create pipeline&quot;); 好了，弄完了，看起来很简单不是吗？我们要声明的东西就是，这个管线在哪个设备上，我们要往里面塞什么着色器代码，着色器代码的入口点是什么，剩下的几个参数在现在还不必理会。 我们这里要问，管线的布局（layout）是怎么指定的？因为我们知道，毕竟我们的管线是在 GPU 上的，它将不可避免地和 CPU 代码产生交互。这个交互就要求流水线知道自己要处理的数据长什么样，这就是我们所谓的布局。事实上，按照 Vulkan 的 spec，在创建流水线的时候是需要约定布局的，而且在约定的实现要求中，我们要求它和着色器的布局规定一致（VUID-VkComputePipelineCreateInfo-layout-00703）。这也给了我们一个提示，或许我们可以去着色器的编译结果中找答案。事实上，我们可以在这段的前面插入下面的代码： 12345let shader_entry = shader.entry_point(&quot;main&quot;).expect(&quot;shader entry point not found&quot;);let shader_layout_req = shader_entry.descriptor_requirements();for req in shader_layout_req { println!(&quot;Shader requirement {:?}&quot;, req);} 我们暂时不去深究 Vulkano 在这里实现的细节。但是我们注意到，它的输出是： 1Shader requirement ((0, 0), DescriptorRequirements { descriptor_types: [StorageImage], descriptor_count: Some(1), image_format: Some(R8G8B8A8_UNORM), image_multisampled: false, image_scalar_type: Some(Float), image_view_type: Some(Dim2d), sampler_compare: {}, sampler_no_unnormalized_coordinates: {}, sampler_no_ycbcr_conversion: {}, sampler_with_images: {}, stages: ShaderStages { vertex: false, tessellation_control: false, tessellation_evaluation: false, geometry: false, fragment: false, compute: true, raygen: false, any_hit: false, closest_hit: false, miss: false, intersection: false, callable: false, _ne: NonExhaustive(()) }, storage_image_atomic: {}, storage_read: {}, storage_write: {0} }) 这里提示我们，在编译完成之后，事实上着色器已经能给出描述子（descriptor）的细节了。这事实上也就是我们在创建流水线过程中所需的东西。在下面一节里，我们就会详细介绍这一部分内容。 数据迁移 Data Transfer看看我们现在有什么，一个 CPU，上面在跑 Rust 代码；一个 GPU，上面在跑（其实还没呢，是要跑）着色器代码。好，为了不让它们各自为政，接下来我们要做的一件事情是搭起 GPU 和 CPU 之间的桥梁。我们知道，GPU 上是具备一块内存的，它与 CPU 之间的通信是一个比较大的速度瓶颈。暂且不管速度或是什么，我们先让程序能跑起来，为了创建一个输入，我们需要引入缓冲区（buffer）的概念。从这里开始，我会更加专注于介绍编程模型而无视底层的硬件实现，因为这里的硬件实现复杂到绝对足以再为这篇文章扩大一倍的篇幅。并且，从这里开始，因为我们用的已经是更新版的 vulkano，我们的代码也会开始与他的标准教程有所不同（标准教程使用的 0.31.0 事实上没有引入设备内存这个概念）。 首先我们要引入设备内存（device memory）这个概念，它指的是设备可见的内存。我们说的设备内存可能由其与主从设备的关系不同产生分别，最常见的几种有仅限设备上的（device-local）、设备上主机可见的（device-local, host visible），主机上可见的（host-local, host visible），基本上都能顾名思义。注意，主机上可见的设备内存当然也是设备上可见的。 为了分配设备内存，我们首先需要一个内存分配器（memory allocator）： 1let mem_alloc = StandardMemoryAllocator::new_default(device.clone()); 很单纯，很友好。这里我们用了最标准的内存分配器，如果需要逐帧分配的话，这个分配器其实不够快，需要用 FastMemoryAllocator，但暂时嘛，能跑就行。 Rust 语言提示：猜猜为什么要用 clone 呢？当然又是借用问题~ 我们已经在着色器的部分提到过，我们实际上的输出是一个图像（image）。这个玩意当然应该是在设备内存上的。因此，我们首先来造一个图像： 12345678910let image = StorageImage::new( &amp;mem_alloc, ImageDimensions::Dim2d { width: 1024, height: 1024, array_layers: 1 }, Format::R8G8B8A8_UNORM, Some(queue.queue_family_index()),).expect(&quot;fail to create image&quot;); 但是我们要非常注意，我们不能保证图像一定是连续存储的。它的存储方式完全是设备特定的，在一些极端情况，如下图中，我们的 GPU 甚至也是有虚拟内存的（图片来自：GPUvm: GPU Virtualization at the Hypervisor, Yusuke Suzuki, Shinpei Kato, Hiroshi Yamada and Kenji Kono，这篇文章在后面介绍 GPU 体系结构时也会再次提及）。因此，我们绝对不能直接按照连续内存的方式让着色器去访问这样一张图像，我们需要添加一个中间层。 这个中间层就被称为图像视图（image view），我们创建它。当然，Vulkano 也已经为我们准备好了方便的接口： 1let view = ImageView::new_default(image.clone()).unwrap(); 然而，图像视图是这个图像在着色器眼中的样子。在主机眼中，它依然是一个不听话的家伙，没有那么容易访问。因此，我们要创建一个主机能访问的缓冲区，然后将它复制过来： 123456789let buf = CpuAccessibleBuffer::from_iter( &amp;mem_alloc, BufferUsage { transfer_dst: true, ..Default::default() }, false, (0..1024 * 1024 * 4).map(|_| 0u8),).expect(&quot;fail to create buffer&quot;); 这里我们设置了 transfer_dst 属性为 true。这意味着我们要把这块空间当成一次传输操作的目的地。现在可用的缓冲区一般有三种，这里我们用的 CpuAccessibleBuffer 指的是主机可见的；而第三个参数 host_cached 设为 false 则是指不在 CPU 访问的内存范围内缓存 GPU 上的数据；最后一项则是数据源，就这么大的全零数组。因此我们可以看出，这个缓冲区是用来存储输出的——对主机可见的一块等同于图像大小的内存。 好。我们已经把内存分配全部完成了。下一个问题是填上上一节挖下的坑：描述子。你可能会问，我们已经有了内存，有了代码，让代码直接运行不就完了吗？但是注意，在着色器代码里我们可没有声明内存，这块内存现在对于着色器而言还是未知的。因此，我们需要让计算流水线知道自己应该在哪做计算。为了完成这个任务，我们需要分配一个描述子集（descriptor set）。当然咯，首先我们要创建一个分配器，这个分配器的目的是更好地管理描述子池（descriptor pool），创建的方法和上面基本上差不多： 1let descriptor_set_alloc = StandardDescriptorSetAllocator::new(device.clone()); 然后再使用这个分配器去整一个描述子集： 1234567let layout = compute_pipeline.layout().set_layouts().get(0).unwrap();let set = PersistentDescriptorSet::new( &amp;descriptor_set_alloc, layout.clone(), [WriteDescriptorSet::image_view(0, view.clone())],).expect(&quot;fail to create persistent descriptor set&quot;); 因为偷懒，我们直接用了管线创建时的描述子集布局作为这里的布局。这是很正常而且没什么问题的，回忆一下管线创建，我们的布局来自于着色器，而这里的描述子集对应的也是着色器，嗯，完美。在这里我们暂时不会深挖描述子集到底是什么东西以及它的表达方式，只要知道，我们现在的 set 就存有管线所需的所有描述子集信息就行。 任务分派 Dispatch终于快结束了。在完成最后一步之前，让我们先看看最开始的那一张图。我们首先创建了实例（instance），找到了物理设备（physical device），然后从里面拿到了设备（device），从一族队列（queue family）中抽象出了队列（queue）。接下来我们写完了着色器模块（shader module），建立了一个管线（pipeline）并且为了通信从设备内存（device memory）中抽出了一个缓冲区（buffer）和一个图像（image）；为了图像被着色器访问我们又给出了一个图像视图（image view），再从描述子池（descriptor set）中分配了一个描述子集（descriptor set）。对比一下，我们现在还没有完成的东西就剩下从管线到队列这个看上去很漫长的过程了。 那么接下来要创建的东西就不用说了，是所谓的命令缓冲区（command buffer）。命令缓冲区是一片用来记录一连串需要提交给队列执行的操作的空间，从命令池（command pool）中分配。当然，每一个命令缓冲区都有自己的生命周期（lifecycle），下面这张来自 spec 的图把它表达的很明确了： 我们这里要做的过程就是：首先从命令池中把它分配出来（initial），然后向里面记录东西（recording），达到可执行状态（executable），接下来单次提交（one time submit），进入等待状态（pending），执行完成之后销毁（invalid），也就是说完成一个顺时针的循环。接下来我们来看代码，首先当然又是一个分配器： 123let command_alloc = StandardCommandBufferAllocator::new(device.clone(), StandardCommandBufferAllocatorCreateInfo { ..Default::default() }); 嗯，全是默认值。然后用它来创建一个命令缓冲区： 12345let mut builder = AutoCommandBufferBuilder::primary( &amp;command_alloc, queue.queue_family_index(), CommandBufferUsage::OneTimeSubmit ).expect(&quot;fail to create command buffer&quot;); 看最上面的图，我们要给它绑定管线和描述子集，然后往里面填东西。Vulkano 的解决方案是使用一个 builder 类来构造它： 12345678910111213141516builder.bind_pipeline_compute(compute_pipeline.clone()) .bind_descriptor_sets( PipelineBindPoint::Compute, compute_pipeline.layout().clone(), 0, set, ) .dispatch([1024 / 8, 1024 / 8, 1]) .unwrap() .copy_image_to_buffer(CopyImageToBufferInfo::image_buffer( image.clone(), buf.clone(), )).expect(&quot;fail to set up the command buffer&quot;);let command_buffer = builder.build().expect(&quot;fail to build the command buffer&quot;); 我们要再次提到前面说的工作组（忘了？在着色器那里）这个概念，在执行任务过程中需要告诉设备我们需要几个工作组，当然，这里就是 [1024 / 8, 1024 / 8, 1] 。然后我们指示在完成之后把图像复制到缓冲区去。最后我们把任务提交上去，丢给队列： 1234567let future = sync::now(device.clone()) .then_execute(queue.clone(), command_buffer) .unwrap() .then_signal_fence_and_flush() .unwrap();future.wait(None).unwrap(); 注意这里我们有用到一个同步策略。我们首先创建一个“现在”的设备，告诉它：接下来先在队列上执行命令缓冲区里的任务，然后等待所有任务完成（fence）并冲刷管线（flush），再发出示意信号（signal），再告诉“未来”的设备等到信号传来。这里的实现细节比较复杂，也涉及比较多同步和异步的知识，但是框架很统一，基本上所有程序都是这样来写的，所以暂时就这样形象地记住这段框架就行。 最后呢，我们要把图片存下来，我们用的是 0.24.5 的 image 库，就像这样： 123let buffer_content = buf.read().unwrap();let image = ImageBuffer::&lt;Rgba&lt;u8&gt;, _&gt;::from_raw(1024, 1024, &amp;buffer_content[..]).unwrap();image.save(&quot;image.png&quot;).unwrap(); 我们应当能够得到这样的图像： 总结 Conclusion到现在为止，我们完成了第一个 Vulkan 程序的书写，尽管依赖了 Vulkano 库，并且所有代码我都已经给出，但我认为这对于初学者而言依然会是一个不小的挑战，也能带来足够的成就感。如果你没能成功完成这部分代码，我已经将代码放在了 GitHub 上（vulkan-tutorial/compute_pipeline at main · FrightenedFoxCN/vulkan-tutorial (github.com)），可供对照参考。再次声明本文相关的代码均来自于 Vulkano 官方文档的例子的改编并感谢官方提供的好例子。在本文中，你应当已经了解了： Vulkan 程序的基本框架； 计算管线和计算着色器的写法； 在最开始的图中涉及的种种概念； 我在本文中挖的坑有： GPU 本身的体系结构，包括处理单元和内存模型； 着色器的编译流程和中间 IR SPIR-V； 硬件设备之间的同步和异步执行流； 在下一篇文章中，这些坑一个都不会被填上的（逃）。下一次，我们会完全偏离 Vulkano 官方教程的轨道，开始书写我们的第一个有交互的用户程序并且创建一个图形管线（graphics pipeline）。这意味着我们首先要把在最开始出现的流程图中的剩余部分讲完，并且引入大量关于图形管线的结构介绍，这应当是下一篇文章的重点——毕竟，静态的图像可不够吸引人，尽管它像曼德博罗集这样扣人心弦。至于这些坑，我们会在几篇开胃小菜性质的文章之后（大概是今年中旬）才会开始一点一点补上。 顺便一提，如果是初学者，最好先过一遍 Rust 的语法和 GLSL 的语法，毕竟这些在我的叙述中不会占有太大的篇幅，参考教程都已经在文中给出，你也可以到谷歌等搜索引擎获取更多的信息。或许我会写一个 Rust 的速通教程，但是谁知道呢？ 参考资料纸质材料OpenGL Superbible 7ed, Graham Sellers, Richard Wright, Nicholas Haemel (OpenGL SuperBible (github.com))General-Purpose Graphics Processor Architecture Tor M.Aamodt, Wilson Wai Lun Fung, Timothy G. Rogers（Morgan&amp;Claypool Publishers - Synthesis Lectures On Computer Architecture）Real-Time Rendering 4ed, Tomas Akenine-Möller, Eric Haines, Naty Hoffman, Angelo Pesce, Sebastien Hillaire, Michael Iwanicki 链接关于 Vulkan 的文档和现有的介绍：LunarXchange (lunarg.com)Introduction - Vulkan Tutorial (vulkan-tutorial.com)Introduction - Vulkan Tutorial (Rust) (kylemayes.github.io)Vulkan in 30 minutes (renderdoc.org)Vulkan Tutorial (github.com)[API without Secrets: Introduction to Vulkan Part 0: Preface (intel.com)](https://www.intel.com/content/www/us/en/developer/articles/training/api-without-secrets-introduction-to-vulkan-preface.html)*Developing Vulkan® applications - GPUOpen 关于 GLSL 和 SPIR-VThe OpenGL® Shading Language, Version 4.60.7 (khronos.org)SPIR Overview - The Khronos Group IncSPIR-V Specification (khronos.org)Shader Compilation - OpenGL Wiki (khronos.org)Compute Shader - OpenGL Wiki (khronos.org) 关于 GPU 的架构及其与系统的交互A trip through the Graphics Pipeline 2011, part 13 | The ryg blog (wordpress.com)Exploring the GPU Architecture | VMware1. GPU Architecture — Dive into Deep Learning Compiler 0.1 documentation (d2l.ai)Understanding the architecture of a GPU | by Vitality Learning | CodeX | MediumGPU segments - Windows drivers | Microsoft Learn这一部分介绍非常深入的并不多，最关键的还是第一份 blog，这是对图形管线最经典的介绍之一。其余的架构细节就只能参考上面提到的 Aamodt 的纸质书、生产商给的白皮书以及一些比较新的论文，在后面提到这一部分的时候我会重新充分引证。 以及所有文中所引用的资源。","link":"/blog/e453db9c/"},{"title":"量子计算与计算复杂度（M. N. Vyalyi） Lecture 1","text":"在之前（还没更新）的课程中，主要讨论了量子计算的几种范式，并对其进行了一些形式化。从这里开始，我们将尝试对量子计算的复杂性理论做出详细的解释和说明，并表明其与经典计算复杂度类的关系。首先，有必要快速回顾一下经典复杂性理论带给我们的一些结果。 经典的复杂性理论研究的对象是受到资源限制的计算。我们需要在“简单的”和“困难的”问题之间做出区分，并将这种区分形式化。首先，我们需要考虑计算所需的资源：时间、空间，还有什么吗？ 交互资源（interaction）也是一种非标准的资源，在我们后面的讨论中，我们会经常用到相关的概念。交互资源主要有两种，谕示机（oracle）和实验者（prover）。谕示机知道问题的答案，而且它是可信的；实验者也知道问题的答案，但它是不可信的，需要提供一个证明。 谕示机（oracle）一词本来被译为“先知、神谕”，事实上这个义项可以很好地理解其意图；实验者（prover）这个说法相当奇怪，按照其直白的含义，应该叫做受质询者之类，但 prover 这个词本身并没有这种含义，所以姑且将其作此翻译。 另，实验者这个词在标准的教科书中似乎并没有出现，在下文看来它似乎与验证者（verifier）相应，在此注明。1 既然我们要讨论量子计算，那么量子过程（quantum process）也是一种资源，我们将其归为自然法则（laws of nature）一类。这一类还包括概然过程（probabilistic process），其与量子过程的差别在后文中会进一步提及。 概然过程（probabilistic process）一词似乎看起来应该译为随机过程，但是随机过程（stochastic process）在概率论中为一个专有名词，为了避免发生混淆，在这里亦作别译。 接下来我们快速地回顾以下经典复杂度理论的成果。 基本概念为了讨论计算复杂度，我们需要从计算开始讲起。为了定义一种计算模型，我们需要给出构形空间（configuration space，可以理解为状态空间），即其所有可能的状态的总和。而一个基本的计算步骤（elementary computation step）可以被定义为一些从构形空间到其自身的函数，一个算法（algorithm）则是一串基本的计算步骤的序列。 在众多计算模型当中，我们选用图灵机（Turing machine）作为基本的计算模型。在这个模型中，构形为一个有限字母表上的序列、一个指针和一个控制态的三元组，一个基本的计算步骤为改变指针所指位置的字符、改变一格指针位置或者改变控制态。在这种情形下，我们可以很轻松地给出时间和空间复杂度的定义： 运行时间正比于一个算法所需的基本计算步骤的个数 所需花费的空间正比于任一构型中的字符串的长度 以 $\\vert x \\vert$ 作为字符串 $x$ 的长度，我们可以定义图灵机 $M$ 代表的算法的最坏时间复杂度为： t_M(n) = \\max\\limits_{\\vert x \\vert = n} T_M(x)其中 $T_M(x)$ 为输入 $x$ 时图灵机 $M$ 达到停机状态（halting state）所需的步数。 在复杂度理论的讨论中，我们感兴趣的是 $t_M(x)$ 的渐进界，尤其是渐进上界。一种很重要的情形是多项式时间的算法，我们形式化地定义其为： t(n) \\in \\mathrm{poly}(n),\\ \\mathrm{if}\\ \\exists c_1, c_2 \\in \\mathbb{Z}_+:\\forall n \\in \\mathbb{Z}_+ t(n) \\leqslant c_1n^{c_2}一般的，我们认为有多项式时间界的算法为足够快地或者足够高效的。 大部分计算模型都是多项式时间等价的（考虑扩展的 Church-Turing 论题），因此这种分类在一般的计算模型中都是普遍可靠的，而且，它使得我们可以使用更加有力的工具（图灵机）来给出复杂度的渐进界。2 从中，我们可以抽象出复杂度类（complexity class）的概念。一个复杂度类是一些可以被给定的资源解决的问题的集合。 为了下面讨论的便利，我们定义几种计算问题。 函数的计算。记 $A^\\ast$ 为一个有限集合 $A$ 上的字符串的总和，计算 $f:A^\\ast \\rightarrow A^\\ast$。以下的文本中，我们都将 $A$ 假定为 $\\{0, 1\\}$。 决策问题（decision problem）。我们称 $L \\subseteq A^\\ast$ 为一个语言（language），一个决策问题就是计算 $L$ 的示性函数（indicator function） $\\mathbb{1}_L(x)$ 。 承诺问题（promise problem）。考虑两个不交的语言 $L_0$ 和 $L_1$。给定 $x \\in L_0 \\cup L_1$，需要求解 $\\mathbb{1}_{L_1}(x)$ 。很显然，对语言 $L$ 的决策问题等价于 $(L, L)$ 的承诺问题。 一个很重要的复杂度类为 $\\mathsf{P}$ 类，我们称 $L \\in \\mathsf{P}$ ，若存在一个确定性算法使得其决策问题关于输入长度有多项式时间界。对于承诺问题，定义是类似的。 更广泛的，可以定义类 $\\mathsf{FP}$，如果一个函数 $f:A^\\ast \\rightarrow A^\\ast$ 可以在多项式时间内被计算，则称 $f \\in \\mathsf{FP}$ 。 1. 绿色方块标识的内容均为译注，下同。 ↩ 2. 蓝色方块标识的内容均为原注，下同。 ↩ 复杂度类 $\\mathsf{NP}$ 、规约和完全类对于复杂度 $\\mathsf{NP}$ 的定义，我们需要回到之前给出的交互资源的提法。直观地讲，在计算过程中，有验证者（verifier）和实验者。验证者只能解决 $\\mathsf{P}$ 类问题，而实验者可以解决任何问题。而实验者是不受信的，因此它必须向验证者给出证明。我们形式化这种直观，将其定义如下： 我们称 $L \\in \\mathsf{NP}$，若存在 $V(x, y) \\in \\mathsf{P}$ 和多项式 $p$ 满足以下条件： 完备性：若 $x \\in L$，则 $\\exists y$，使得 $V(x, y) = 1$ 且 $\\vert y \\vert \\leqslant p(\\vert x \\vert)$ 。 可靠性：若 $x \\not \\in L$，则 $\\forall y$，$V(x, y) = 0$ 。 其中 $V(x, y)$ 就是对验证者的形式化。为了证明答案 $x$ 是正确的，实验者需要给出证明 $y$ ，其长度（耗费的空间资源）也应该是多项式的，使得利用 $y$ 验证 $x$ 是一个 $\\mathsf{P}$ 类问题。而如果答案 $x$ 是错误的，无论给出什么证明，都不能使得验证者误认为 $x$ 是正确的。 很显然，$\\mathsf{P} \\subseteq \\mathsf{NP}$。我们下面不加证明地给出三个 $\\mathsf{NP}$ 问题的例子，它们可能是属于 $\\mathsf{NP} \\setminus \\mathsf{P}$ 的： $\\text{3-SAT}$: 给定一个三变量的合取范式，确定其是否可满足。 $\\text{3-COLOR}$: 给定一个图，确定其是否能被三种颜色染色。 $\\large\\text{P}\\small\\text{ARTITION}$: 给定一列正整数，确定是否存在一个平均分划。 考虑计算问题的复杂度的排序方式。很直观地，如果我们能够将某个问题化作另一个问题，这个问题肯定不会比另一个问题更难解决。这种直觉使我们能够定义规约（reduction）这个概念，它定义了复杂度类中的一种二元关系。 我们称一个承诺问题 $(L_0, L_1)$ 可以多项式地规约（polynomial reduction, 或者 Karp reduction）到一个问题 $(K_0, K_1)$ ，如果存在一个函数 $f \\in \\mathsf{FP}$，使得： x \\in L_1 \\iff f(x) \\in K_1, x \\in L_0 \\iff f(x) \\in K_0记作 $(L_0, L_1) \\leqslant_p (K_0, K_1)$ 。 很容易表明，这个二元关系具备传递性，只需观察到 $\\mathsf{FP}$ 类中的函数的复合还在 $\\mathsf{FP}$ 类中即可。这个二元关系引导我们定义出一个复杂度类中“最难”的问题，这些问题被称为完全问题（complete），因为它们事实上表征了整个复杂度类最难解的一些问题。 我们称一个问题 $P \\in \\mathcal{C}$ 为 $\\mathcal{C}$-完全的，如果对任意 $L \\in \\mathcal{C}$ ，有 $L \\leqslant_p P$ 成立。 $\\mathsf{NP}$ 类中同样包含一个完全集，这已经由 Cook 和 Levin 证明（通过 $\\text{3-SAT}$ 问题）。事实上，上面举出的三个例子均为 $\\mathsf{NP}\\text{-complete}$ 问题。 复杂度类的补、$\\text{co-}\\mathsf{NP}$ 类这里的考虑很好理解，既然我们定义了复杂度类，那么它自然而然地可以定义出一个补：设 $\\mathcal{C}$ 为一复杂度类，则 $\\text{co-}\\mathcal{C}$ 为其补，对任意 $L \\in \\mathcal{C}$，$\\bar L \\in \\text{co-}\\mathcal C$。 一个很显然的命题是，$\\mathsf{P} = \\text{co-}\\mathsf{P}$。我们只需要将算法的输出翻转一下，就可以得到 $\\text{co-}\\mathsf{P}$ 问题的解。 $\\text{co-}\\mathsf{NP}$ 和 $\\mathsf{NP}$ 的关系仍然悬而未决，我们在这里给出一个 $\\mathsf{NP} \\cap \\text{co-}\\mathsf{NP}$ 的问题： $\\large\\text{I}\\small\\text{NTEGER FACTORING}$\\输入：一个正整数 $N$ 的二进制表示\\输出：$N$ 的质因数分解 很显然，这不是一个决策问题。我们需要将其改写成决策问题： \\large\\text{B}\\small{\\text{IT}}\\large\\text{O}\\small{\\text{F}}\\large\\text{F}\\small\\text{ACTORING} = \\{\\langle N, i \\rangle : N \\text{ 的质因数分解第 } i \\text{ 位为 } 1\\}证明这个问题在 $\\mathsf{NP}$ 中是显然的，实验者只需要给出一个分解作为证明即可。 注意到，判断一个数是否为素数是可以在多项式时间内完成的，参考 AKS 素数测试。 证明其属于 $\\text{co-}\\mathsf{NP}$ 也并不困难，和上述方法是完全相同的。 我们还可以不加证明的给出一个 $\\mathsf{NP} \\cap \\text{co-}\\mathsf{NP}\\text{-complete}$ 的承诺问题的实例，决策问题的例子仍然是未知的。 $\\large\\text{O}\\small\\text{NE}\\large\\text{O}\\small\\text{F}\\large\\text{T}\\small\\text{WO}$\\输入：两个合取范式 $C_1$, $C_2$\\承诺：两者中必有一个是可满足的\\问题：表明 $C_1$ 是可满足的 概率性的计算，类 $\\mathsf{BPP}$ 和 $\\mathsf{PP}$如前所述，计算过程中，随机数也是一种资源。接下来我们探讨可以访问随机数的算法。在这里，我们将随机位认为是独立平均分布的，$0$ 和 $1$ 出现的概率各占一半。 对于一个涉及随机数的计算，出错也是难免的，因此，我们要提出的第一个问题是，如何定义一个成功的计算？ 直觉地，我们认为，一个计算如果正确的概率“过半”，那它就是成功的，设 $f(x)$ 为目标函数，$\\text{Res}(x)$ 为计算结果，我们可以形式化这个定义为： \\forall x, \\mathbf{P}[\\text{Res}(x) \\neq f(x)] < a \\leqslant \\frac 1 2注意到我们留了一个 $a$ 作为参量。这是因为，$a$ 是否等于 $\\frac 1 2$ 的情形是完全不同的，其区别的来源是一个算法能否被强化（amplify），即，经过多次重复这个算法（ $t$ 次），然后取出现最频繁的结果，这样似乎能够增加其准确性。事实上，可以证明以下命题成立： 若 $\\mathbf{P}[\\text{Res}(x) \\neq f(x)] &lt; \\frac 1 2 - \\epsilon$，则增强后的算法出错的概率为： \\Big(2\\sqrt{\\frac 1 4 - \\epsilon ^2}\\Big)^t也就是说，$a &lt; \\frac 1 2$ 时，算法出错的概率可以逐渐收敛到 $0$，若 $a = \\frac 1 2$，则我们并不能得出有用的结果。这样我们可以定义两个复杂度类： 若存在一个多项式 $p$ 和一个多项式时间内的确定性算法 $V(x, r)$ 满足以下条件，则称 $(L_0, L_1) \\in \\mathsf{BPP}$ ： 完备性：若 $x \\in L_1$，则 $\\mathbf{P}_{r \\leftarrow \\mathcal{U}_{p(|x|)}}[V(x, r) = 1] &gt; \\frac 2 3$ 可靠性：若 $x \\in L_0$，则 $\\mathbf{P}_{r \\leftarrow \\mathcal{U}_{p(|x|)}}[V(x, r) = 1] &lt; \\frac 1 3$ 其中 $\\mathcal{U}_m$ 是在长度 $m$ 的字符串上的均匀分布。 很显然，$\\mathsf{P} \\subseteq \\mathsf{BPP}$，只需要取 $p = 0$ 即可。 若存在一个多项式 $p$ 和一个多项式时间内的确定性算法 $V(x, r)$ 满足以下条件，则称 $(L_0, L_1) \\in \\mathsf{PP}$ ： 完备性：若 $x \\in L_1$，则 $\\mathbf{P}_{r \\leftarrow \\mathcal{U}_{p(|x|)}}[V(x, r) = 1] &gt; \\frac 1 2$ 可靠性：若 $x \\in L_0$，则 $\\mathbf{P}_{r \\leftarrow \\mathcal{U}_{p(|x|)}}[V(x, r) = 1] &lt; \\frac 1 2$ 其中 $\\mathcal{U}_m$ 是在长度 $m$ 的字符串上的均匀分布。 同样显然地，$\\mathsf{BPP} \\subseteq \\mathsf{PP}$，$\\mathsf{PP} = \\text{co-}\\mathsf{PP}$。 接下来，我们要表明另一个复杂度层级，$\\mathsf{NP} \\subseteq \\mathsf{PP}$，从而使我们的复杂度继承关系形如下图： 为了给出这个证明，我们需要引出一些额外的定义，并从另一个角度重新表述 $\\mathsf{NP}$ 和 $\\mathsf{PP}$ 的定义。 证明 $\\mathsf{NP} \\subseteq \\mathsf{PP}$：非确定性图灵机、$\\text#\\mathsf{P}$-函数和间隙函数非确定性图灵机（non-deterministic Turing machine, NTM）与确定性图灵机的差别在于，在每一步计算中，它可以在转移函数之间做出随机选择。因此，NTM 的计算过程是一棵含根的树，而非一列构形空间的序列。树的根部为起始状态，叶为停机状态，从根部到叶的一条路径称为一条计算路径（computation path），计算路径的最大值被定义为这个 NTM 的运行时间。 我们考虑停机状态有两种，接受（accept）和拒绝（reject）。记接受的路径的总数为 $\\text{acc}_M(x)$，其中 $M$ 为一图灵机，$x$ 为一输入。 接下来我们定义 $\\text#\\mathsf{P}$-函数和间隙函数的概念。称一函数 $f:\\{0, 1\\}^\\ast \\rightarrow \\mathbb{Z}_+$ 为一个 $\\text#\\mathsf{P}$-函数，若存在一个多项式运行时间的 NTM $M$ 使得 $\\forall x, f(x) = \\text{acc}_M(x)$。称一函数 $f:\\{0, 1\\}^\\ast \\rightarrow \\mathbb{Z}$ 为一个间隙函数（gap function），若存在两个 $\\text#\\mathsf{P}$-函数 $g, h$，使得 $\\forall x, f(x) = g(x) - h(x)$。 未完待续","link":"/blog/b5738fe7/"},{"title":"计算理论笔记 01A：有限自动机和正则语言","text":"从这里开始，我们要进入计算理论的世界。计算理论（theory of computation）研究的核心问题就是计算（computation）。我们需要思考的问题是：什么是计算；什么可以计算或不可以计算；如果可以计算，这个问题有多难计算；如果不可以计算，这个问题不可以计算的程度有多少。这个系列（前半部分的）主要框架来自于浙江大学计算理论课程毛宇尘老师的授课，其中插入了大量笔者个人的想法和来源于其他参考资料的结论和思路，应当完全覆盖并超越这门课程本身。这门课程事实上只部分地回答了前两个问题，对于第三个问题一带而过，而对于第四个问题几乎完全没有讨论，但是在这个系列里，我会尽可能完整地展现整个理论从新生到现代的全貌。笔者才疏学浅，如有疏漏，望指正。 为了讨论计算，我们首先需要有一个计算问题（problem）。为了定义计算问题，我们需要引入语言（language）的概念。而要解决这个问题，我们需要有一个计算设备（computation device），这又使得我们引入自动机（automata）的概念。在这一节中，我们讨论的对象是这两类中最简单的形式：正则语言（regular language）和有限状态机（finite automata）。本文作为 A 面讲的是基础部分，基本上已经完全覆盖了课程内容，以后可能会写的 B 面则更偏重更多理论发展和更现代化的理论。而在 B 面中，我们会假定读者已经具备了足够的数学能力，因为在那里我们会更多地使用代数（以及可能的拓扑）来解决计算理论相关的问题。 注意，在拓展内容中，因为笔者个人的兴趣以及体现理论整体美感的需要，哪怕在 A 面中也会涉及一些需要代数基础的内容，我将尽可能将其讲的足够明白；但是对于计算理论这门课程而言，能否理解这一部分应当是无足轻重的。 问题和语言 Problems &amp; Languages回忆一下我们迄今为止碰到过的计算问题。我们会问：一个图（graph）是否是一棵树（tree），一个逻辑表达式是否是可满足的（satisfiable）或者一个表达式（expression）是否是最简的（simplified）。我们将这种判断是或否的问题称为决策问题（decision problem）。这种问题是计算理论研究最主要的对象，也是在我们这个系列前半部分我们会主要讨论的问题。 是/否的判断事实上是一个取值为 0/1 的函数，我们可以将其视为一个集合的示性函数（characteristic function）： \\chi_S(x) = \\begin{cases} 0, x \\not\\in S\\\\ 1, x \\in S \\end{cases}这样的表示使得我们将一个问题转化为一个集合。下一个问题是，我们要怎样来表述这个集合？我们知道，我们可以用二进制表示一个整数，也可以用十进制表示一个整数，这些方法在我们判断一个数是奇数还是偶数的问题本身看来没有什么影响，但是它可能会影响计算过程。为了清晰地描述对一个问题的计算过程，我们需要定义某种“编码方式”作为讨论的对象，也就是说，我们要有一个字母表（alphabet）。字母表 $\\Sigma$ 是一个由有限个符号（symbol）组成的集合，在某个字母表上的字符串（string over an alphabet）$w$ 是有限个字母表中的符号组成的序列（sequence），这个字符串的长度就是字符串中符号的个数，记作 $\\vert w \\vert$。当然，这里我们似乎没有定义什么叫符号，因为它事实上没啥影响，你可以用 0, 1, 2, 3，也可以用甲乙丙丁，甚至如果你乐意的话，也可以用不同的 emoji 表情，这种等价性是显而易见的（我们确实可以用复杂的数学语言去描述它，但是在没有必要的情况下，不会有人喜欢这么干的吧，不会吧不会吧）。 需要额外说明，我们的字符串长度可以为 0，这个字符串被我们称为空字符串（empty string，或字，word，本文会混用这两个术语），在同一个字母表上，有且仅有一个空串，记作 $e$。当然，我们可以定义字符串上的运算，都很常规： 衔接（concatenation），$vw$ 就是前半段是 $v$ 的内容，后半段是 $w$ 的内容； 幂次（exponentiation），$w^i$ 就是把字符串 $w$ 重复 $i$ 次；我们定义 $\\forall w, w^0=e$； 翻转（reverse），$w^R$ 就是把字符串 $w$ 倒过来；我们定义 $e^R=e$； 注意看！衔接操作作为运算（operation），空串作为单位元（identity），我们可以从一个字母表生成一个自由幺半群（free monoid）。这是为后面的补充内容做铺垫：对于正则语言（regular language），它的句法幺半群（syntactic monoid）的结构可以直接导出判断一个语言是否正则的充要条件：Myhill-Nerode 定理。 在继续之前，我们再引入一些记号来表示从字母表到字符串集合的映射。 $\\Sigma^i$ 指字母表中所有长度为 $i$ 的字符串的集合； $\\Sigma^+$ 指字母表中所有非空字符串的集合：$\\Sigma^+ = \\cup_{i=1}^\\infty \\Sigma^i$； $\\Sigma^\\ast$ 指字母表中所有字符串的集合：$\\Sigma^\\ast = \\Sigma^+ \\cup \\{e\\}$，事实上这就是前面的生成的自由幺半群； 接下来对语言的定义也就顺理成章了：一个字母表 $\\Sigma$ 上的语言 $L$ 就是 $\\Sigma^\\ast$ 的一个子集。当然，它可以是全集，也可以是空集。在这种情况下，我们的问题就变成了：给定 $x \\in \\Sigma^\\ast$，计算 $\\chi_L(x)$ 的值。这样，我们就完成了从问题到语言的转化。语言之间的运算除了集合运算之外，常见的还有衔接、幂次和星（star）： $L_1L_2 = \\{uv: u\\in L_1, v\\in L_2\\}$； $L^i = \\{w_1w_2\\cdots w_i: w_j \\in L\\}$，$L^0 = \\{e\\}$； $L^\\ast = \\cup_{i=0}^\\infty L^i$； 我们可以明确地看到衔接和乘积之间的关系。但是，仅此而已吗？注意，我们还有集合的并操作，它可以在某种意义上被与一个加法等同，如果记其为 $L_1 + L_2$，那么，我们容易发现，$\\mathcal{P}(\\Sigma^\\ast)$ 上的所有元素在并和衔接（和交，但这不重要）下封闭，而且存在结合律： L(L_1 + L_2) = LL_1 + LL_2,\\quad(L_1 + L_2)L = L_1L+L_2L诶，看我们发现了什么？又是一个代数结构！我们在这上边定义了一个加法半群和一个乘法半群，两者都有单位元，加法乘法有结合律，加法有交换律，具备分配律，而且加法单位元 $\\varnothing$ 与所有其他元素的乘积都是 $\\varnothing$，不等于乘法单位元 $\\{e\\}$。好，这时候半环（semiring）就出现了。以下我们会混用 $0$ 和 $\\varnothing$ 以及 $1$ 和 $\\{e\\}$，前者一般会在代数视角中更常用，后者则直接地对应到语言上。这个时候我们自然而然地会想到想办法做一个商： u^{-1}L = \\{v\\in \\Sigma^\\ast:uv\\in L\\}, \\quad Lu^{-1} = \\{v \\in\\Sigma^\\ast:vu\\in L \\}很直观，是吧？这里我们很不幸不能用局部化（localization）的想法去做进一步的操作，因为加法同样也是半群，而且乘法不交换。但是我们一样可以整出语言之间的左商（left quotient）和右商（right quotient）： L_1^{-1}L_2 = \\bigcup_{u \\in L_1}u^{-1}L_2, \\quad L_1L_2^{-1} = \\bigcup_{u \\in L_2}L_1u^{-1}同样很和善，对吧？下面几个等式的证明很简单，留作读者练习： $(L_1 + L_2)^{-1}L = L_1^{-1}L + L_2^{-1}L$； $(L_1L_2)^{-1}L = L_2^{-1}(L_1^{-1}L)$； 你以为就到此为止了吗？难道你没有发现有什么东西被忘记了？是的，我们的星号还在苦苦等待。对于它我们最适合的归宿是先给出下面两个结论，同样留给读者自己证明： $(L_1 + L_2)^\\ast = L_1^\\ast(L_2 L_1^\\ast)^\\ast$：和-星规则（sum star rule）； $(L_1L_2)^\\ast = 1 + L_1(L_2L_1)^\\ast L_2$：积-星规则（product star rule）； 满足这两条规则的带星操作的半环被称为 Conway 半环。下面一个推论在所有 Conway 半环中都适用： x^\\ast = 1+ xx^\\ast被称为不动点定理（fixed point theorem），请读者自行证明之。注意，一个任意的 Conway 半环中的星操作只是一个满足这两条规则的从自身到自身的映射。 这些代数结构的引入在现在看来好像还有点突兀。但是，在后面，我们会尝试表明，Conway 半环的规则事实上对于 Kleene 定理来讲已经足够了。 我们最后给出一个语言的线性方程（linear equation）的定义。既然加法乘法都有了，那线性性看起来就是理所应当的了。我们暂时只考虑方程 $X = KX + L$，其中 $L, K$ 都已知，$X$ 未知，我们在某些情形下可以给出它的解，这个引理是属于 Arden 的：当 $e\\not \\in K$ 时，$X = KX+L$ 具备唯一解 $X = K^\\ast L$。这个结果直观地看上去很像线性方程组的结果，我们下面来严格证明一下。 首先，我们很容易表明这就是这个方程的解： K(K^\\ast L) + L = KK^\\ast L +L = (KK^\\ast + \\{e\\})L = K^\\ast L嘿！我们看到了不动点定理在这里发挥了作用。现在我们更加明白为什么它被叫做不动点了。因为我们事实上是表明了 $x^\\ast$ 是函数 $y = 1 + xt$ 的不动点，而 Arden 引理做的是对它进行了推广。 然后我们要表明唯一性，这一步看起来可能就会有点复杂了。依然是很直接的，我们假设它有两个不相同的解 $X_1, X_2$，接下来由于对称性，我们只需要表明一个单侧的包含关系 $X_1 \\subset X_2$ 就行，也就是说，$\\forall u \\in X_1, u \\in X_2$。我们通过对 $u$ 的长度做归纳来完成这个步骤。 如果 $\\vert u \\vert = 0$，$u$ 是空字且 $u \\in X_1$，那么 $u \\in KX_1 + L$。因为 $K$ 中不含空字，所以 $u \\in L$。这样的话，也就有 $u \\in X_2$。 接下来我们考虑长为 $n + 1$ 的字 $u$，假定 $X_1$ 中所有长度小于等于 $n$ 的字都在 $X_2$ 中。那么我们有两种选择：$u$ 要么在 $KX_1$ 中，要么在 $L$ 中。如果它在 $L$ 中，那它显然也就在 $X_2$ 中了。如果它在 $KX_1$ 中，那么存在 $k \\in K, x \\in X_1$ 使得 $kx=u$。因为 $k$ 不是空字，所以 $x$ 的长度一定小于等于 $n$，由归纳假设得 $x \\in X_2$，因此 $u = kx \\in KX_2$，$u \\in X_2$。 好，证完了，下面一个推论则留给读者了：如果 $K$ 中含有空字，那么这个方程的解就是 $K^\\ast M$ 的形式，且 $L \\subset M$。这事实上也表明，这个方程是有一个极小解（minimal solution）的。 接下来我们可以正式开始考虑线性方程组了。我们的方程组长这样： \\begin{array} \\\\X_1 &= K_{11}X_1 + K_{12}X_2 + \\cdots + K_{1n}X_n + L_1\\\\ X_2 &= K_{21}X_1 + K_{22}X_2 + \\cdots + K_{2n}X_n + L_2\\\\ \\vdots & \\vdots\\\\ X_n &= K_{n1}X_1 + K_{22}X_2 + \\cdots + K_{nn}X_n + L_n\\\\ \\end{array}当然咯，敏锐的读者应当已经发现了，这似乎也是一个求解不动点的问题，我们完全可以把它表示成 $\\boldsymbol X = \\boldsymbol K \\boldsymbol X + \\boldsymbol L$。我们接下来要表明，如果 $K_{ij}$ 均不含空字，则这个方程组也能给出一个唯一解。在 Arden 引理之后表明这一点是相当轻松的。我们只需要表明递归下去就可以了。由最后一条方程结合 Arden 引理，我们知道： X_n = K_{nn}^\\ast(K_{n1}X_1 + \\cdots + K_{n(n-1)}X_{n-1} + L_n)然后将其代入到倒数第二条方程中，一直递归到求解出每一个 $X_i$ 就完成了。 那么，这种方程组有什么含义呢？我们马上要引入的有限状态机事实上就是这样一种方程组的表现。 有限状态机 Finite Automata 注意：本节中会引入很多定义，与课本/课件中的定义有些不相同之处，但是它们都是等价的。我们这里的定义和课程中的定义表列如下： 本文中的有限状态机就是 NFA，DFA 是 NFA 的一个特殊情形，而 NFA-$e$ 则是 NFA 的推广情形；除了 DFA 以外，我们都不要求初始状态唯一。 课程中的有限状态机指的是我们的 DFA，课程中的 DFA 和本文中的 DFA 相同，课程中的 NFA 指的是本文中初始状态唯一的 NFA-$e$。 有限状态机的引入看起来就要容易地多。假设我们有一个门，它可以开，可以关。那么“开”和“关”就是它的两个状态，而开关的过程就是状态之间的转换，这很好理解。接下来我们要把这种想法形式化，于是我们给出这样的定义（注意，这里定义的引入和授课的顺序有所不同，笔者认为这是一个更方便建立形式化描述的思路，整体参考的是 Mathematical Foundations of Automata Theory, Jean-Éric Pin, Version of 18th Feb. 2022） 一个有限状态机（finite automata，或 finite state machine）是一个五元组 $(K, \\Sigma, \\Delta, S, F)$，其中： $K$ 是一个有限的集合，其中的元素被称为状态（states），因为状态数有限所以我们才称之为有限状态机； $\\Sigma$ 是一个字母表； $\\Delta$ 是 $K \\times \\Sigma \\times K$ 的子集，称为状态转移函数（transition function），记作 $q_1 \\stackrel a \\rightarrow q_2$；为什么将其称为一个函数呢？因为它可以被看成 $K \\times \\Sigma \\rightarrow \\mathcal{P}(K)$ 的一个函数，记作 $\\delta$。 $S$ 和 $F$ 都是 $K$ 的子集，称为起始状态（initial state）和终止状态（final state）； 看起来好像一头雾水，对吗？我们来看几个例子。前面提到了门，那就从这里开始吧。门的状态如上所述有两种，因此我们考虑它的状态集合 $K = \\{\\mathrm{Open}, \\mathrm{Closed}\\}$。当然，我们只有开关两种操作反复循环，所以我们约定 $0$ 表示开，$1$ 表示关，字母表 $\\Sigma = \\{0, 1\\}$ 组成的字符串就是我们对门的操作，比如 $1100$ 表示关、关、开、开。然后状态转移函数就很简单了：如果门是开着的，那开就是保持不变，关就是把它关上；如果门是关着的，那开就是把它打开，关就是保持不变，也就是说，我们有 \\Delta = \\{\\mathrm{Open}\\stackrel 0\\rightarrow \\mathrm{Open}, \\mathrm{Open}\\stackrel 1\\rightarrow \\mathrm{Closed}, \\mathrm{Closed}\\stackrel 0\\rightarrow \\mathrm{Open}, \\mathrm{Closed}\\stackrel 1\\rightarrow \\mathrm{Closed}\\}然后我们假设门开始是开着的，那就是说 $S = \\{\\mathrm{Open}\\}$，而门最终可以是关着的或者开着的，那就是说 $F = \\{\\mathrm{Open}, \\mathrm{Closed}\\}$。注意到在这里，我们的起始状态只有一个，而且对于任意一个 $q \\in K, a \\in \\Sigma$，最多只有一个 $q^\\prime \\in K$ 使得 $q\\stackrel a \\rightarrow q^\\prime \\in \\Delta$，也就是说，$\\text{Im }\\delta$ 中的元素全都是单元素集 。这种情况下，我们只要给定一串操作，我们得到的结果就是完全确定的。我们称这种状态机为确定性的有限状态机（deterministic finite automata, DFA）。 我们可以很容易地用图像来表明这个状态机： 嗯，手工画的有点难看就是了。我们用三角形指向某个状态表示起始状态，双圆圈表示终止状态，单圆圈表示一般的状态，箭头和箭头上的字母则表示状态转移函数。这是一个非常简单的 DFA，简单到不用解释就能看明白它在干什么。 继续，我们来把它弄复杂一点。现在我们这个自动机和语言看起来毫无关系，任何一个字符串都能让它从起始状态到达终止状态，现在，我们要规定，打开的门不能被再次打开，关掉的门也不能被再次关上，这样的限制看起来也是很合理的，不是吗？ 那么我们就要从我们的状态转移函数里删掉 $\\mathrm{Open}\\stackrel 0\\rightarrow \\mathrm{Open}$ 和 $\\mathrm{Closed}\\stackrel 1\\rightarrow \\mathrm{Closed}$。注意，这时候对于任意的 $q \\in K, a \\in \\Sigma$，不一定都有下一个状态了。我们把这种自动机称为不完备的（incomplete）自动机，原来那种则被称为完备的（complete）。接下来直觉上讲这个自动机不能随意接受字符串了。当然，我们还需要一些澄清来表明什么叫自动机接受一个字符串： 自动机中的一个路径（path）是一串状态转移函数 $\\{q_i \\stackrel {a_i} \\rightarrow q_i^\\prime\\}_{i=0}^n$，使得 $\\forall i, q_i^\\prime = q_{i+1}$；我们称 $a_i$ 组成的字符串是这个路径的一个标号（label）；称 $q_0$ 为路径的源（origin），$q_n^\\prime$ 为路径的尾（end）； 如果一个路径的源 $q_0 \\in S$，尾 $q_n^\\prime \\in F$，则称它是接受的/成功的（accepting/successful）； 如果一个字符串是至少一个成功的路径的标号，则称这个自动机接受（accept）这个字符串； 一个自动机接受的所有字符串构成的语言称为这个自动机接受的语言； 顺便补充一个说明，在这种定义下，不完备的自动机碰到没法走下去的地方当然就直接停机了，它不能接受这样的字符串。 好，接下来让我们看看这个自动机接受什么语言了。首先，因为它最开始是开着的，所以第一步操作一定是关上它，否则就不能继续了；然后，我们可以开、关、开、关发生任意次，但一定是交替进行的。也就是说，它接受的字符串是 $101\\cdots 010$ 或 $101\\cdots 01$。我们会在下一节讨论怎么方便的表示这种东西。 接下来看一个更加麻烦的例子。这下我们的自动机要不做门了，它给了一个非常自由放纵的状态图： 很诡异，很奇妙是吧？我们来看看它能接受怎样的语言。我们一开始在状态 $A$，然后只能接受 $0$，然后……等等！这个东西没有门那么友好了，它接受一个 $0$ 之后有两条路可以走：可以回到状态 $A$，也可以去往状态 $B$。 我们首先考虑第一种情形。如果它回到状态 $A$，那输入的下一个字符就是 $0$，否则它就不能继续下去，那我们记下来：任意前缀的 $0$ 个数都可以；甚至可以说，如果回到 $A$，那么它接下来就可以接受任意个 $0$。这是有限状态机的一个特点：接下来的执行过程只和当前状态和剩余的输入有关，与已经经历过的路径无关，因此它才被称为“状态”机。所以，我们可以这样来记它在中途的一个配置（configuration）和一步的运行过程：设自动机为 $\\mathcal{M}$，$q, q^\\prime \\in K$ 且 $w, w^\\prime \\in \\Sigma^\\ast$，则： (q, w) \\vdash_{\\mathcal{M}}(q^\\prime, w^\\prime)若 $\\exists a\\in \\Sigma$ 使得 $w = aw^\\prime$，且 $q\\stackrel a\\rightarrow q^\\prime\\in \\Delta$。如果是任意步转移（包括 $0$ 步），则记为 $(q, w) \\vdash_\\mathcal{M}^\\ast (q^\\prime, w^\\prime)$。 第二种情形，如果它到达状态 $B$，那么输入的下一个字符就是 $1$，然后它可以接受任意个数的 $1$ 之后终止或者回到 $A$。总结一下，我们现在有的信息是： 第一个字符必须是 $0$； 接下来可以接受任意个（包括 $0$ 个）$0$ 到达状态 $B$； 到达状态 $B$ 后可以结束，也可以接受任意个（包括 $0$ 个）$1$ 之后结束或者到达状态 $A$； 观察一下，这不就是第一个字符为 $0$ 的任意字符串吗？折腾了这么半天，我们发现它事实上并不是多么复杂的东西。它接受的语言是任意一个首字符为 $0$ 的字符串，不管后面是什么，它都能终止。 因此我们称上面定义的有限状态机为非确定性的有限状态机（non-deterministic finite automata, NFA），因为对于每一步输入，我们有可能获得不同的输出。注意，我们课程中提到的 NFA 是我在下面要讲的只有一个起始状态的 NFA-$e$。另一个说明是，在很多文献中，更常用的记号是 $\\varepsilon$ 而不是 $e$，因此下面所有带 $e$ 的名词都可以把它换成 $\\varepsilon$。 对于这个定义，我们可能还有很多顾虑。我们下面想要说明的事情是：对于这个定义的很多改造而言，它的“能力”都是不变的。当然，现在这个定义还是很难处理，看看上面的第二个例子就知道了，而我们又发现，DFA 往往是很好处理的。因此，我们下面的议程就是证明一大堆等价性： DFA 和 NFA 是等价的； NFA 和 NFA-$e$ 是等价的； 完备的 DFA 和 DFA 也是等价的； 完备的 NFA 和 NFA 也是等价的； 完备的 NFA-$e$ 和 NFA-$e$ 也是等价的； 只有一个初始状态的 NFA 和 NFA 等价； 只有一个初始状态的 NFA-$e$ 和 NFA-$e$ 等价； 当然，在此之前，我们先介绍等价的概念。我们说两个自动机等价，如果它们接受相同的语言。两类自动机等价，就是说 A 种自动机中的每个自动机都可以与 B 中自动机中的某个自动机等价。可以验证，它从集合的相等定义继承了自反性、传递性、对称性，是一种等价关系。 那么我们首先要证明的是第二条，因为我们用 NFA-$e$ 来处理问题会比用 NFA 来处理更方便一些，而且这也是我们课程中的做法。首先我们叙述一下 NFA-$e$ 的形式化定义： 一个 NFA-$e$ 是一个五元组 $(K, \\Sigma, \\Delta, S, F)$，其中： $K$ 是一个有限的集合，其中的元素被称为状态（states），因为状态数有限所以我们才称之为有限状态机； $\\Sigma$ 是一个字母表； $\\Delta$ 是 $K \\times (\\Sigma\\cup \\{e\\}) \\times K$ 的子集，称为状态转移函数（transition function），记作 $q_1 \\stackrel a \\rightarrow q_2$； $S$ 和 $F$ 都是 $K$ 的子集，称为起始状态（initial state）和终止状态（final state）； 注意到，我们的定义中它和 NFA 只差了一条，也就是说，我们在字母表中引进了空字 $e$，它的含义是，这里无需输入，可以直接发生转移。我们看一个例子： 看起来很眼熟对吧？这就是两个折磨了我们挺久的 NFA 的一个组合。我们来看看它接受什么语言。再次强调，状态机的特点就是只要管状态和接下来的输入是什么就行，所以只要到了 $S1$ 或者 $S3$，接下来会发生什么我们已经考虑完了。 我们首先进入 $S0$，现在我们碰到了一个 $e$-转移（$e$-transition，或 $e$-move），也就是说，我们可以不读入输入直接从 $S0$ 跳到 $S1$ 或者 $S3$。接下来就会读入： 任意一个 $0$ 开头的字符串（$S1$ 和 $S2$）； 任意一个 $1$ 开头的字符串（$S3$ 和 $S4$）； 诶，又绕回来了，我们现在这个 NFA-$e$ 接受的语言就是任意字符串……对吗？注意，这里有一个小细节，我们接受的是长度至少为 $1$ 的任意字符串，这在后面是极容易犯错的！我们在这里事实上看到了两个 NFA 是怎么被“并”起来的，它的语言也是两个语言的并。当然，在证明 NFA-$e$ 和 NFA 等价之前，我们还不能这么说。接下来我们就先直觉地说明这件事情，然后再给出规范的证明。 我们注意到，NFA 和 NFA-$e$ 的区别就在于状态转移函数。而且 NFA 可以看成一种特殊的 NFA-$e$，因此，我们只要考虑怎么把 NFA-$e$ 的状态转移函数“转换”成 NFA 的状态转移函数。我们注意到，NFA-$e$ 的状态转移函数事实上就是出现了一次无输入的跳跃，而 NFA 则要求每一次跳跃都有对应的输入。一个显而易见的想法是：为什么不把这一次转移和下一次转移合并到一起呢？例如，对于前面的例子，我们有以下几种可能的 $e$-转移及其下一步转移： $S0 \\stackrel e \\rightarrow S1 \\stackrel 0 \\rightarrow S1$ $S0 \\stackrel e \\rightarrow S1 \\stackrel 0 \\rightarrow S2$ $S0 \\stackrel e \\rightarrow S3 \\stackrel 1 \\rightarrow S3$ $S0 \\stackrel e \\rightarrow S3 \\stackrel 1 \\rightarrow S4$ 因此，我们把所有这两步操作合并成一步，这样我们的自动机就变成了这样： 看起来似乎完成了，是吧？在这个例子里，我们确实已经完成了改造，这个 NFA 显然不是最简单的，但是简化是我们在下一节才会去讨论的内容。但是，对于任意自动机，只要完成这样的合并就可以了吗？烦请读者再阅读一遍上面的叙述，思考三分钟，这里的叙述是有问题的，但是这个问题并不难补上。 想必机敏的读者现在已经发现了，首先，我们有可能发生连续的 $e$-转移。这使得我们不能简单合并两步，应该允许多步的合并，这个修补是很轻松的。 第二个问题更加关键：我们说的是把“这一次转移”和“下一次转移”合并在一起，但是没有确保说，一定存在下一次转移。如果我们的一次 $e$-转移把我们带到了终止状态，那么我们就找不到这样的下一次转移了。那么，怎么办？ 我们思考一下，如果我们的一次 $e$-转移能够将我们带到终止状态，这意味着在上一个状态我们随时可以终止，因为只要到了上一个状态，那么我们在任何时候都可以到下一个状态去宣告终止。这里我们第三次强调状态机的概念：状态，状态，还是状态！因此，一个自然而然的想法是，把它向前“传播”，让我们的上一个状态变成终止状态。这样，我们就可以保证它能够正确地终止。 这里留给读者一个思考题：为什么不把“上一次转移”和“这一次 $e$-转移”合并起来，然后特殊处理起始状态的情形？或者如同 Wikipedia(Nondeterministic finite automaton - Wikipedia) 上一样，只特殊处理起始状态和终止状态被 $e$-转移连接起来的情形？在我们的定义下，这事实上是可行的，但是对于以后说明“只有一个起始状态”和“具备多个起始状态”的 NFA 相同来说，这会带来额外的麻烦。在这里的构造我们其实已经保证：任何一个有唯一起始状态的 NFA-$e$ 都等价于一个有唯一起始状态的 NFA，希望你已经意识到了这点，如果没有，请看下面对这个证明的形式化叙述。 我们在这里涉及了两步操作：首先，找到一个状态通过 $e$-转移能到达的所有状态；然后，完成终止状态的传播和转移函数的重新构造。那么自然而然地，我们需要引入 $e$-闭包（$e$-closure）的概念。一个状态 $q$ 的 $e$-闭包 $E(q)$ 是指所有可以通过 $e$-转移抵达的那些状态的集合。也就是说，$E(q)$ 中的元素 $q^\\prime$ 满足：存在一条以 $q$ 为源、以 $q^\\prime$ 为尾的通路，使得其标号为空字符串。一个说明是，我们约定 $q \\in E(q)$，这会使得我们下面的证明看起来简单得多。 接下来我们来形式化后面的操作。首先是传播，我们需要拓展终止状态的集合。设原来的 NFA-$e$ 为 $\\mathcal{M} = (K, \\Sigma, \\delta, S, F)$，其等价的 NFA 为 $\\mathcal{M^\\prime} = (K^\\prime, \\Sigma, \\delta^\\prime, S, F)$。这里记成 $\\delta$ 的意思是我们要按照转移函数的函数视角来处理这个问题，这样书写的记号会简单一些。终止状态的拓展发生在一次 $e$-转移使得自动机达到了终止状态的情形下，也就是说： F^\\prime = F \\cup \\{q:E(q)\\cap F \\ne \\varnothing\\}接下来处理状态转移函数。对于任意一个 $q \\in K, a \\in \\Sigma$，我们给出： \\delta^\\prime(q, a) = \\bigcup_{r \\in E(q)}\\delta(r, a)也就是说，它是所有原来就有的状态转移函数可能转移到的状态与当前状态的 $e$-闭包中的状态经过这个字符之后可能转移到的状态之并。 然后我们要干的事情是验证这样的定义是正确的。首先我们很容易验证，这样得到的 NFA 中绝对没有 $e$-转移，因为我们在定义状态转移函数的时候根本没有引入 $e$；然后，我们需要验证，对于任意一个 NFA-$e$ 接受的字符串 $w$，NFA 都可以同样接受它，反之亦然。 我们对 $w$ 的长度做讨论： 如果 $w$ 是空串，说明在 NFA-$e$ 中，有一个标号为空字符串的接受路径。也就是说，存在一个起始状态 $s \\in S$，$E(s) \\cap F \\ne \\varnothing$。这意味着 $s \\in F^\\prime$，也就是说，在 NFA 中也有一个标号为空字符串的接受路径。反过来，如果在 NFA 中有一个标号为空字符串的接受路径，那么存在 $s \\in S$ 使得 $s \\in F^\\prime$。因此，$s$ 可能在原来的 $F$ 中，这时当然 NFA-$e$ 接受空串；它也有可能是一个使得 $E(s) \\cap F \\ne \\varnothing$ 的状态，那当然 NFA-$e$ 也接受空串。 如果字符串 $wa, a \\in \\Sigma$ 的长度至少为 $1$，在 NFA-$e$ 中有一个以其为标号的接受路径，那么一定存在状态 $q_1$ 使得 \\bigcup_{s \\in \\delta(q_1, a)}E(s) \\cap F \\ne \\varnothing这里的意思是，从 $q_1$ 可能读取一次 $a$ 到达状态 $s$，然后有可能经历一系列 $e$-转移最后到达一个终止状态。当然咯，这个式子就等价于： \\exists s \\in \\delta(q_1, a),E(s)\\cap F \\ne \\varnothing \\Rightarrow \\exists s \\in \\delta^\\prime(q_1, a),E(s)\\cap F \\ne \\varnothing这也就是说 $s \\in F^\\prime$，因此 $\\delta^\\prime(q_1, a) \\cap F^\\prime \\ne \\varnothing$，我们表明了这个 NFA 接受字符串 $wa$。反过来，如果这个 NFA 接受字符串 $wa$，我们也可以一步一步反推回去，因为每一步变换都是按照我们的构造方式等价的。 为了方便起见，我们以后记 $\\cup_{s \\in S}E(s) = E(S)$。如果我们容许对初始状态做出改变，那么我们的构造就会变得特别简单。只要让新的初始状态 $S^\\prime = E(S)$，新的状态转移函数 $\\delta^\\prime(q, a) = E(\\delta(q, a))$ 即可。这个过程的正确性应当不难验证。 折腾了半天我们证明完了第二个命题，接下来我们考虑第六个命题。这个过程就很简单了，对于一个 NFA 在前面添加一个状态，用 $e$-转移将其与所有原来的起始状态连接作为新的起始状态。然后，因为我们在上面的转化过程中没有改变 $S$，我们可以将这个只有一个起始状态的 NFA-$e$ 转化成只有一个起始状态的 NFA，它与原来的 NFA 等价。这里的形式化写法不复杂，留给读者自行补充。 下一步事实上才是我们的重点，证明第一个命题。这个命题要求我们证明 DFA 和 NFA 等价，和上面的第二个命题一样，因为我们知道 DFA 就是一种特殊的 NFA，所以我们要给出的就是把一个 NFA 转化成等价的 DFA 的方法。这个证明经常被称为 Rabin-Scott 幂集构造（Rabin-Scott powerset construction，或 Rabin-Scott 子集构造，Rabin-Scott subset construction）。这个构造应用非常广泛，在下一节以及以后对于更复杂的计算模型的类似构造过程中，我们都会反复看到它的影子。 基本的想法是这样的：我们知道，NFA 和 DFA 的区别就在于，一个 NFA 的运行过程是有可能出现“分支”的。也就是说，它有可能出现 $\\delta(q, a)$ 不是单元素集的情形。在这种情况下，我们的想法就是，去构造 NFA 的状态集合的一个幂集，使得这个 DFA 完美模拟 NFA 的运行。这次我们直接写形式化的结果：设 NFA $\\mathcal{M} = (K, \\Sigma, \\delta, S, F)$ 对应的 DFA 为 $\\mathcal{M^\\prime} = (K^\\prime, \\Sigma, \\delta^\\prime, S^\\prime, F^\\prime)$，我们首先写出状态集合： K^\\prime = \\mathcal{P}(K)也就是说，状态集合是它的幂集。这里不失一般性地我们沿用证明第二个命题的结果，令 $S$ 为单元素集，所以它就直接取成单元素集的集合： S^\\prime = \\{S\\}终止状态就是那些其中至少有一个状态是原来的终止状态的集合： F^\\prime = \\{q \\in K^\\prime:q\\cap F \\ne \\varnothing\\}而转移函数稍微复杂一点，它就是从一个集合 $q$ 经过输入 $a$ 之后所有其中的元素可能到达的状态的集合： \\delta^\\prime(q, a) = \\bigcup_{r \\in q}\\delta(r, a)好了，这样我们就成功构造出了一个 DFA。这样构造的 DFA 当然和原来的 NFA 等价，验证也与上面第二个命题的证明差不多，读者不妨自行完成。 但是我们考虑，这样一个 DFA 是不是足够好的？当然，这个构造很直接，作为证明而言很简洁，但是我们事实上经历了三个步骤：先转换到一个只有一个起始状态的 NFA-$e$，然后全局性地消除 $e$-转移，再消除不确定性。这样的过程直觉上会引入更多的冗余状态。因此，我们给出其他更加精巧的想法。 首先，我们可以注意到，转换到只有一个起始状态的 NFA-$e$ 不是必须的。我们只需要把初始状态定为所有可能的初始状态的集合就行。对于上面这个构造，我们甚至不用改变任何一个符号。这样，我们就得到了一个从 NFA 到 DFA 的直接方法。 但是，我们在将 NFA-$e$ 转化成 NFA 的过程中，需要做一次全局性的对 $e$-转移的消除。哪怕使用我们讲的不保持起始状态的构造方式，它直觉上似乎也并没有那么好。因此，我们下面介绍两种更加直接的从 NFA-$e$ 转化成 DFA 的方式，分别是逐集合计算 $e$-闭包和逐状态计算 $e$-闭包。首先是逐集合的方式，这要求我们把计算 $e$-闭包整合进计算转移函数的过程当中。 首先对起始状态稍作修改，这和第二个命题的证明过程中的第二种构造是类似的，使得： S^\\prime = E(S)这当然没什么问题。然后是在计算转移函数的过程中，引入同样类似的构造： \\delta^\\prime(q, a) = \\bigcup_{r \\in q} E(\\delta(r,a))这种方法事实上可以看成是那个构造的直接应用，不同的是，我们不再需要经过一个 NFA 作为中间产物了。而逐状态计算的想法则更加简单：我们事实上只需要计算 $K$ 中所有状态的闭包而不用计算 $\\mathcal{P}(K)$ 中所有状态的闭包，这样我们就能减少对闭包的计算，然后在转移函数的计算过程中再去取并集。 经过实验，我们发现逐状态计算的性能事实上是最差的，因为它计算并集的次数太多了；而逐集合计算往往在跳跃较多时有优势，全局计算往往在跳跃较少时有优势。 最后我们还需要注意到一个现象：在把 NFA 转变为 DFA 的过程中，我们的状态从 $n$ 个扩增到了 $2^n$ 个。这个过程被形象地称为确定性爆炸（deterministic blow-up）。这个界已被李雅普诺夫证明为最优的。我们可以给出一个很简单的例子：一个起始状态一个终止状态，而且字母表中只有两个字母的 NFA，它转变过去的 DFA 至少有 $2^n$ 个状态，这个例子是 F. R. Moore 的构造，证明其转变过去的 DFA 不能被简化将在下一节后留作读者练习。同时，也可以证明这个爆炸可以取到 $n$ 到 $2^n$ 中的任意一个整数，参见 J. Jirásek, G. Jirásková, and A. Szabari. Deterministic blow-up of minimal nondeterministic finite automata over a fixed alphabet. In J. Karhumäki and A. Lepistö, editors, Developments in Language Theory, volume 4588 of Lecture Notes in Comput. Sci., pages 254–265. Springer-Verlag, 2007. 这个 NFA 的字母表记为 $\\{0, 1\\}$，状态记作 ${1, 2, \\cdots, n}$，我们让起始状态为 $1$，终止状态为 $n$。当输入 $0$ 时，状态 $i &lt; n$ 转移到 $i + 1$，状态 $n$ 转移到 $1$ 或者 $2$；当输入 $1$ 时，状态 $1 &lt; i &lt; n$ 转移到 $i + 1$，状态 $1$ 转移到自身而状态 $n$ 不转移（指状态转移函数为空）。也就是说： \\delta(i, 0) = \\begin{cases} \\{i + 1\\}, i < n\\\\ \\{1, 2\\}, i = n \\end{cases},\\quad \\delta(i, 1) = \\begin{cases} \\{1\\}, i = 1\\\\ \\varnothing, i = n\\\\ \\{i + 1\\},\\text{ otherwise} \\end{cases}我们可以进一步将这种状态数抽象为状态复杂度（state complexity）的概念，它考察的是一个语言可以产生的最小 DFA 的状态数，以及状态机之间进行种类转变、各种操作对状态数造成的改变。这个方向的研究目前看来不是太多，我们将在介绍为整个经典的理论框架之后再做阐述。剩下的几个命题比较简单，留给读者自行完成。 现在，我们来填上在上一节最后埋下的一个坑：为什么说有限状态机就是一组这样的线性方程组？想必有些敏锐的同学已经发现了。如果没有的话，再思考几分钟，然后往下看。 在我们定义有限状态机接受的语言的过程中，大家应该已经注意到了，我们用的是路径（path）这个术语，这个术语很容易让人联想到图论（graph theory）。直觉地说，我们要找的可能的标号就是当前状态转移到终止状态的可能的字符串的标号。也就是说，如果我们当前的状态是 $q$，剩余的字串是 $X_q$，那么从当前状态出发能通过一条标号为 $X_q$ 中的元素的路径达到终点，因此，我们对中间状态做分类讨论： X_q = K_{q1}X_1 + K_{q2}X_2 +\\cdots + K_{qn}X_n + L_q这个式子的意思就是说，从 $q$ 出发，到达终点的所有路径的标号的集合为 $X_q$。其中 $K_{qi}$ 指的是从状态 $q$ 出发到状态 $i$ 的路径的标号，$X_i$ 指的是从 $i$ 出发到达终点的路径的标号的集合。因此，把所有经过状态 $i=1, 2, \\cdots, n$ 的从 $q$ 出发到达终点的路径并起来，就能得到所有从 $q$ 出发到达终点的路径。而如果 $q$ 自己就是终点，那当然 $L_q$ 要置为 $\\{e\\}$，这意味着不经历其他任何状态到达终点。注意，我们在这里用的是 NFA 来构造这个东西，如果要用 NFA-$e$，这里的处理会变得更加复杂。 那么这样的构造给出之后，$K_{pq}$ 也就不难定义了。我们定义 $K_{pq} = \\{a \\in \\Sigma: (p, a, q) \\in \\Delta\\}$。它就是每次转移过程要加上的字符串。最后一步是，对于 $L_q$，如果 $q$ 不是终止状态，那么置它为空集。这样我们就可以很明确地说，这个方程组已经被完整的定义了。 下一步是定义语言。很显然，语言就是 $\\sum_{q \\in S}X_q$，所有能从初始状态到达最终状态的字符串，这是良定的，因为我们已经表明 $K_{qn}$ 在 NFA 中是不含空串的，所以这个方程组是有唯一解的。在将正则语言和 NFA 进行转换的过程中，我们会再次用到这个结论。 对于 NFA-$e$ 往方程系统的转化，当然它显得就没那么容易了，因为 $K_{ij}$ 可能出现空串了。这时事实上我们的极小解就派上用场了，它是我们的 NFA-$e$ 得到的结果。这个结果的验证并不困难，不妨留作练习。我们还会留下一个问题：如何用方程系统表示 NFA 到 DFA 的转化？在这里请读者先作一番思索。 最后再挖一个坑：还记得幺半群吗？我们在讨论语言的时候已经提到了一个自由幺半群，而在自动机中，我们也能再抓到一个幺半群。下面我们记 $\\mu(w)$ 为所有令 $p, q$ 之间有一条标号为 $w$ 的路径的 $(p, q) \\in K\\times K$ 组成的集合。这是我们的核心命题： $\\mu(w)$ 是一个从 $\\Sigma^\\ast$ 到 $\\mathcal{P}(K \\times K)$ 的幺半群同态（monoid homomorphism）。 我们接下来慢慢解释这句话。首先，我们应该给 $\\mathcal{P}(K \\times K)$ 赋予一个乘法结构。当然，我们是有一个很自然的定义的，那就是将其中元素看成关系（relations），将乘法看成关系的复合。也就是说，$AB = \\{(p, r) \\in K \\times K: \\exists q \\in K,(p, q)\\in A \\wedge (q, r) \\in B\\}$。这是很自然的。 下一步是证明一个幺半群同态。幺半群同态的意思是一个保乘法、保单位元的映射。原来的单位元当然是空串，它映到平凡的关系 $\\{(a, a): a \\in K\\}$，很容易表明这是关系构成的幺半群的单位元。而保持乘法的性质也很简单：直观地，从 $p$ 到 $q$ 如果有一条标号 $v$ 的路径，从 $q$ 到 $r$ 有一条标号 $w$ 的路径，那么当然从 $p$ 到 $r$ 有一条标号 $vw$ 的路径，反之也一定能找到这样的 $q$ 使得前面的性质成立。也就是说 $\\mu(v)\\mu(w) = \\mu(vw)$。 接下来我们不加证明地声称，任何一个幺半群同态的像也是一个幺半群，任何一本代数课本都不可能没有介绍这个结论。我们称这样定义的幺半群为自动机的状态转移幺半群（transition monoid）。它在本章最后的高潮部分会得到淋漓尽致的应用，从而将语言、自动机和幺半群完美地融为一体。 简化 Minimization在上一届我们已经提到，不同的自动机可以表示相同的语言。这里我们要考虑怎么去简化一个自动机。在本文中，我们会主要考虑 DFA 的简化，介绍相应的几种方法并分析其复杂度，这并不困难。而 NFA 的简化相比起来要复杂得多。事实上，可以证明它是 $\\mathsf{PSPACE}$-完备的，目前已知的最快算法为 Kameda-Weiner 算法。关于这个问题复杂度的证明则会留在以后介绍了复杂度理论之后，作为 $\\mathsf{PSPACE}$-完备的一个例子来作叙述，暂且按下不表。 我们首先来直觉地讨论 DFA 简化的问题。第一步，我们要移除所有已经没法到达终点或者本身就不能被抵达的状态，这些状态当然是不可能抵达终点的，可以被删掉。第二步，我们需要把“相同”的状态删掉一个，因为这些状态删不删掉不影响最终的结果。 在继续之前，我们先必须确定，对于一个语言，在接受它的自动机当中，存在最简的一类自动机，它们在状态名字发生交换之后相同。这个结论的严格证明会在 Myhill-Nerode 定理之后给出，也就是本章最后。在这里，我们暂且承认这个结论。 好，我们继续。第一步是非常简单的步骤，基本上就是一个图论问题，在这里因为与主题关系不大就直接跳过了。第二步则是最关键的一步，各种算法也就是在这里发生了分歧。但是在继续之前，我们需要思考一个问题：什么样的状态是相同的状态？直观的想法就是，从这里出发向后取走，我们发现到达终点之前的路径的标号构成的集合完全一样了。这个时候我们只要把一个状态的历史复制一份，指向另一个状态，再把另一个状态删掉，那就完成了。 嗯，这样说可能还是有点抽象。我们还是要回到在上面已经强调了三次的状态机的概念。如果两个状态的未来相同，也就是说从这个状态往后的输入相同时它都能给出相同的结果，我们已经讲过，状态机没有“记忆”，决定它的运行的就是现在的状态和将来的输入，那这两个状态当然就是相同的。好，如果还不明白，看形式化的表述。 我们考虑状态 $q$，我们知道，我们可以很容易地定义出从它出发到达终点的路径的标号的集合，还记得吗，我们在方程组那里把它记成 $X_q$。这里我们为了强调它是一个语言，我们就将其记作 $L_q$，这都无关紧要。我们称这是它的未来（future，如果要看起来高级一点，也可以叫右语言 right language）。我们称一个自动机是最简的，如果 $\\forall p, q \\in K, L_p \\ne L_q$。当然，我们也可以定义一个状态的过去（past，左语言 left language），就是从起始状态到达这个状态经过的路径的字符串构成的集合，记作 $_qL$。 好了，定义很简单咯。接下来我们考虑怎么去求这个东西。看到那个不等号没？我们自然就要联想到等价关系了，在代数里这是一个非常常用的手段。我们定义状态之间的 Nerode 等价关系（Nerode equivalence）： p \\sim_N q \\iff L_p = L_q可能有人会说，这不是废话吗？你下一步是不是要说我们的最简自动机就是等价关系对于任意两个状态都不成立？是，但不完全是。等价关系最常用的一个操作是作商（quotient），一个集合对等价关系作商得到的是一组等价类（equivalence classes），它们不重不漏地把原来的集合划分成了一系列子集，在一个子集内部的所有元素等价，而不在同一个子集中的元素两两不等价。那么我们要求的极小自动机就是状态集合做的商，我们写成： \\mathcal{M}/\\sim_N = (K/\\sim_N, \\Sigma, \\delta^\\prime, q_0, F/\\sim_N)注意，我们特别强调了终止状态也要做一个商，因为我们要尽可能减少终止状态。状态转移函数则被定义为 $\\delta^\\prime([p], a) = [\\delta(p, a)]$，其中 $[p]$ 指包含状态 $p$ 的等价类。对一个集合，我们将它的一族不重不漏的子集称为它的一个划分（partition） 等价类会让人联想到什么？还记得我们上面做的语言的商吗？我们一直还没用它呢。这下它要派上用场了，因为我们知道作商在一般的代数结构中都能拿到一个等价类。这里我们稍微操作一下，将其转化成状态的语言：给定状态构成的集合 $P \\subset K$ 和字母 $a\\in \\Sigma$，我们记 $a^{-1}P = \\{q \\in K: \\delta(q, a) \\in P\\}$。再次强调一下，我们做的是 DFA，所以我们干脆就把 $\\delta$ 的取值定义在 $K$ 上了，这样来写能够避免很多麻烦事。 接下来我们看这是怎么拿到一个划分的。给定集合 $R \\subset K$，我们要通过 $(P, a)$ 将它划分成两个集合。很显然的划分方法就是：$\\{R\\cap a^{-1}P, R - a^{-1}P\\}$。其中 $-$ 表示集合之间作差。也就是： R\\cap a^{-1}P = \\{q \\in R: \\delta(q, a) \\in P\\}, \\quad R - a^{-1}P = \\{q \\in R: \\delta(q, a) \\not\\in P\\}这当然是个划分，也是个等价类，我们将其记为 $(P, a)\\vert R$。当然，$(P,a)\\vert R = (P^c, a) \\vert R$，其中 $P^c$ 表示 $P$ 在 $K$ 中的补。我们把 $(P,a)$ 称为分裂子（splitter），如果 $(P, a)\\vert R$ 中有两个元素，也就是说 $R\\cap a^{-1}P$ 和 $R - a^{-1}P$ 两者均不为空，则我们称 $(P, a)$ 分裂（splits）集合 $R$。当然，把 $a$ 推广到 $w \\in \\Sigma^\\ast$ 是自然的，我们很早就做出了关于商掉一个字的定义。 那么什么时候 $(P, a)$ 不能分裂集合 $R$ 呢？同样不难判断。我们记 $\\delta(R, a) = \\{\\delta(q, a): q \\in R\\}$，如果它被包含于 $P$ 或者 $P^c$，那当然 $(P, a)$ 就不能分裂 $R$ 了。接下来我们要说明的是下面这个定理，它将分裂子和 Nerode 等价关系联系在了一起： Nerode 等价关系对应的等价类是集合 $K$ 最粗糙的满足以下条件的划分 $\\mathcal{P}$：任意 $P, R \\in \\mathcal{P}, a \\in \\Sigma$，$(P, a)$ 不能分裂集合 $R$。 我们要说明一下什么叫一个划分比另一个划分粗糙（coarser）。给定集合 $K$ 的划分 $\\mathcal{P}$ 和 $\\mathcal{Q}$，我们称 $\\mathcal{P}$ 比 $\\mathcal{Q}$ 粗糙，如果任意一个 $\\mathcal Q$ 中的集合都被包含于 $\\mathcal P$ 中的一个集合。直观地说，这就像用刀切一块五花肉，如果是切成东坡肉，那当然要比把这些肉细细剁作臊子要来的粗糙。下面我们来快速描述一下这个结论怎么证明。 我们先思考，一个集合 $(P, a)$ 能不能分裂集合 $R$ 到底有什么意义？如果 $(P, a)$ 分裂集合 $R$，那么 $\\delta(R, a)$ 不包含于 $P$ 或者 $P^c$，也就是说，从 $R$ 出发经过 $a$ 之后不能确定它到达的是 $P$ 还是 $P^c$。如果能确定它到达 $P$ 或者 $P^c$，事实上我们就可以把它里面的元素对 $a$ 这个下一步输入看成相同的状态了，因为它们的下一步全都跑到了一个集合里去。更加严格的说法需要用到归纳法去证明，首先把终止状态的划分建立起来，然后再往前一层一层推进，严格的证明请读者自行完成。 好，乏味的理论时间到此为止，我们要开始讲算法了。最简单的一个算法是 Brzozowski 算法，准确的说，它是一个想法很简单、实现很简单的算法，唯一的缺点就是复杂度有可能很高。 首先，我们知道，把一个 DFA 起始终止状态换一下，然后翻转所有箭头可以得到一个 DFA，我们称之为一个翻转操作。要清理掉一个 DFA 或者 NFA 里面没用的状态很容易，只要从前往后进行一次遍历，从起始状态不能走到的地方都删掉，再从后往前进行一次遍历，从终止状态不能走到的状态都删掉。这也是很常规的套路，我们把它称作剪枝操作。然后，Brzozowski 的发现是，先剪枝、翻转，然后再通过幂集构造做成 DFA，再翻转，这样就能把原来的 DFA 最简化。神奇吗？ 神奇的东西背后往往会蕴藏着深刻的东西。比如在这个观察之中，我们会发现代数-余代数对偶性（algebra-coalgebra duality）的影子。别慌，我们在这里不打算把它详细讲清楚，这是我们在 B 面会讲的问题。我们先来看看怎么说明这个算法是对的。 下面我们要说明的是一个更强的版本：对于一个 NFA，我们可以用类似的方法做到其对应的 DFA 的简化。假定原来的 NFA 为 $\\mathcal{M}$，我们记 $\\mathcal{M}_D$ 为它通过标准的幂集构造转为 DFA 的结果，记 $\\mathcal{M}^R$ 为一个自动机翻转之后的结果，$\\mathcal{M}^\\sim$ 为一个自动机剪枝之后的结果，这三个操作就是我们算法所需要的全部了。 我们按照如下操作来完成转换：首先将其翻转得到 $\\mathcal{M}^R$，那么很自然地，我们知道它接受的语言就是原来的语言的翻转：里面的每个字都被反转了。然后我们将其转为 DFA 再剪枝，得到 $((\\mathcal{M}^R)_D)^\\sim$。然后再反转并转为 DFA，再剪枝，得到 $(((((\\mathcal{M}^R)_D)^\\sim)^R)_D)^\\sim$。括号很多，很烦，所以我们记 $M^\\dagger = ((\\mathcal{M}^R)_D)^\\sim$，得到的结果就是 $(M^\\dagger)^\\dagger$ 很显然，我们的剪枝和反转操作都不会影响自动机接受的语言，所以我们最终的这一个自动机就是一个与原来的自动机等价的自动机。 然后我们去想，为什么这个新的自动机就是最简的自动机了呢？我们需要追随它的每个状态的来源。我们的 $\\mathcal{M}^\\dagger$ 是一个确定性的自动机，这意味着它从起始状态出发，同一个词都会使得它到达同一个状态，也就是说，一次 $\\dagger$ 操作事实上就是把原来自动机的每个状态的未来转变为过去（past，或者左语言，与未来定义对应）并且唯一化了这种过去。然后我们的第二次 $\\dagger$ 操作使得唯一化的过去变成未来，并且又唯一化过去使得它变成一个真正的 DFA。 更加形式化一点，我们关注 $\\mathcal{M}^\\dagger$ 中的一个状态 $q$。它是一个 DFA 就意味着对于任意一个 $_qL$ 中的字符串 $s$，不存在状态 $p$ 使得 $s \\in\\ _pL$。当我们把它翻转过来之后，$_qL$ 变成 $L_q$，$_pL$ 变成 $L_p$，但是在翻转前没有唯一化的未来使得其中每个状态的过去不是唯一化的，因此我们要做第二次 $\\dagger$ 操作让它变成 DFA。准此思路可以完成严格的证明。我们可以说，这个过程事实上就是两次“时间反演”的操作。因为 NFA 到 DFA 的转化是对过去的唯一化，所以利用它就可以完成整个操作。 好，下面我们来看其他算法。它们看起来没有这么简单，但是复杂度会比上面的算法低。Brzozowski 算法的复杂度最坏情况下会到达指数级，因为它引入了两次倒霉的幂集构造，我想这不是什么难以发现的事情。而后面的几个算法则都是多项式级别的复杂度，但是要求更加精巧的想法和构造。 注意，在 Brzozowski 算法中我们没有用到 Nerode 等价关系，但是在这里，我们要开始利用它了。我们知道，等价类就是等价关系，那么我们要从一个最开始的等价类出发，逐步细化得 Nerode 等价关系对应的等价类，这就是我们的 Moore 算法的灵感来源。 我们要考虑，一个怎样的等价类既好算又能够逐渐趋近于 Nerode 等价关系对应的等价类呢？注意我们的 $L_q$，它是一个对所有从 $q$ 出发能够到达终止状态的字符串的刻画。因此我们不妨对其做一个限制：毕竟我们的输入是有限的，那就按照长度划分这样的字符串。我们定义一个在状态集 $K$ 上的等价关系，$k$ 阶的 Moore 等价关系 $\\sim_M^k$ 为： p \\sim_M^k q \\iff L_p^k = L_q^k其中 $L_q^k$ 指状态 $q$ 的未来中长度小于等于 $k$ 的字符串。下一步我们能够用 Moore 等价关系来表达 Nerode 等价关系：当 $\\sim_M^k = \\sim_M^{k+1}$ 时，$\\sim_N = \\sim_M^k$。根据上面提到的直觉，这是很显然的。我们称最小的这个值为 Moore 算法的深度（depth）。 因此，我们只要计算什么时候 $\\sim_M^k = \\sim_M^{k+1}$ 就行，而我们又可以很轻松地根据 $\\sim_M^k$ 计算出 $\\sim_M^{k+1}$： p \\sim_M^{k+1} q \\iff p \\sim_M^{k} q \\wedge \\forall a\\in \\Sigma,\\delta(p, a)\\sim_M^k \\delta(q, a)但是，我们要计算的是等价类。所以我们用等价类来表达等价关系，记 $\\mathcal{M}_k$ 为 $\\sim_M^k$ 对应的等价类，在考虑 $\\mathcal{M}_{k+1}$ 时，很直观的想法，也是上面这个式子告诉我们的就是，把 $\\mathcal{M}_k$ 中的东西继续细分。对于 $\\mathcal{M}_k$ 中的任意集合 $R$，取其中的任意集合 $P$ 和字母表中的任意字母 $a$，如果 $(P, a)$ 分裂 $R$，那就把分裂的结果放进 $\\mathcal{M}_{k+1}$ 中。这样，我们就构造完了 Moore 算法。 当然，我们的复杂度就是 $O(lkn)$，其中 $l$ 为 Moore 算法的深度，$k$ 为字母表的元素个数，$n$ 为状态的总数。这是因为在求解 $\\mathcal{M}_{k+1}$ 的过程中，我们每一步都可以使用一次基排序（radix sort）来使得已经完成的划分中，同一集合内的元素相互邻近。$l$ 的最坏结果，很容易发现就是 $n-2$。在最糟糕的情况下，我们会把所有的状态都分开，也就是说，发现它完全就已经是最简的了。这时，我们必须进行 $n-2$ 次计算才能得到循环终止的信号。我们称这样的自动机是对 Moore 算法慢的（slow for Moore）。 那么平均复杂度呢？我们这里不给出证明，因为这不是我们关注的重点。只要注意到，如果我们的输入在所有可能的 $n$ 状态完备 DFA 中均匀分布，那么 Moore 算法的平均复杂度是 $O(n\\log n\\log n)$。这个结论的证明可以参看 J. David. The average complexity of Moore’s state minimization algorithm is $o(n \\log \\log n)$. In P. Hlinený and A. Kucera, editors, Mathematical Foundations of Computer Science 2010, 35th International Symposium, MFCS 2010, volume 6281 of Lecture Notes in Comput. Sci., pages 318–329. Springer-Verlag, 2010。 好，下面我们给出最后一个经典算法：Hopcraft 算法。我们同样是通过逐步细化的方法来得到最终的结果，不同的是我们现在的出发点是 $\\{F, F^c\\}$，也就是说，先把终止不终止分成两堆。然后我们每一次把其中的一些一分为二。注意，在 Moore 算法中，我们不可能通过数据结构之类的优化降低它的复杂度了，为了做到更好，我们必须对这个构造动刀子。 我们首先观察一下 Moore 算法给出的构造模式。还记得怎么从 $\\sim_M^k$ 计算出 $\\sim_M^{k+1}$ 吗？根据这个式子，可以注意到 $\\sim_M^k \\ne \\sim_M^{k+1}$ 的条件就是： \\exists a \\in \\Sigma, p, q\\in K, p\\sim_M^k q \\wedge \\delta(p, a) \\sim_M^k \\delta(q, a)用划分的语言重新表述它： \\exists a \\in \\Sigma, P, Q \\in K/\\sim_M^k, \\delta(P, a) \\cap Q \\ne \\varnothing \\wedge \\delta(P, a)\\not \\subset Q也就是说，从 $\\sim_M^k$ 到 $\\sim_M^{k+1}$ 本质上就是一系列划分，把 $P$ 划分成 $P\\cap a^{-1}Q$ 和 $P - a^{-1}Q$。这是什么？没错这就是 $(Q, a)\\vert P$。因此，在这里我们需要对 $P$ 一分为二，裂开成它对于 $(Q, a)$ 这个分裂子的划分。 下面我们来描述一个基本的算法，这个算法本质上和 Moore 算法没什么差别，而且显得更加抽象了。但是它多了更重要的东西：优化空间。 我们的出发点是 $\\{F, F^c\\}$，那就意味着我们所有可能的分裂子就是 $\\{F, F^c\\} \\times \\Sigma$。现在要记下所有可能的分裂子，将其记作 $W$，有些文献里会叫它等待集（waiting set）。接下来，对于每个等待集中的分裂子，我们要将其能分裂的集合进行分裂。分裂完成之后，更新 $W$ 和已经做好的划分。如果被分裂的集合在 $W$ 中有一席之地，我们要更新它，如果没有，我们要把新的集合加入进去。 很简单，对吧？但是这里还需要一些澄清。下面我们就来规范地描述一下这个算法。 置 $R \\leftarrow \\{F, F^c\\}, W \\leftarrow R \\times \\Sigma$； 如果 $W$ 不为空，那么从中取出一个分裂子 $(P, a)$； 接下来拿着 $(P, a)$ 寻找 $R$ 中它可以分裂的集合，对于每个可以分裂的集合 $B$，在 $R$ 中将其替换成 $(P, a)\\vert B$ 中的两个元素； 再遍历字母表 $\\Sigma$，对其中的任意字母 $x$，如果 $(B, x) \\in W$，那么把 $(B, x)$ 换成 $(B^\\prime, x)$ 和 $(B^{\\prime\\prime}, x)$；否则将 $(B^\\prime, x)$ 和 $(B^{\\prime\\prime}, x)$ 添加到 $W$ 中； 回到步骤 2，直到 $W$ 为空。这个算法当然会停机，这是我们一开始从 Moore 等价关系推到这里就已经表明了的，那么它的复杂度呢？很显然，$W$ 中的元素最多有 $O(kn)$ 个，而且其中的每个元素都会带来 $k$ 重循环，所以现在我们的复杂度还是 $O(k^2n)$，这个结果和之前的 Moore 算法比起来只能说不分伯仲。 接下来我们要考虑怎么来优化这个玩意。直觉是这样的：只需要两刀就可以把一个蛋糕切成三块，但是在我们的算法里会出现三次切割。对于一个集合 $D \\in R$，我们使用 $(B, a)$ 切完之后，还可能会需要用 $(B^\\prime, a)$ 和 $(B^{\\prime\\prime}, a)$ 对这个集合做切割，最终的结果却是把它切成三个部分，所以里面有一步肯定多余了。 为什么这样说呢？我们按照上面的直觉给出这个引理：考虑 $D \\in R$，$(B, x) \\in W$，$B$ 可以被细分为 $B^\\prime$ 和 $B^{\\prime\\prime}$，那么用 $(B, x)$、$(B^\\prime, x)$ 和 $(B^{\\prime\\prime}, x)$ 三者之二分裂 $D$ 得到的结果是相同的。这个引理的证明很简单，因为 $D$ 总共能被分成的只有三块：去到 $B$ 之外的，去到 $B^\\prime$ 的，去到 $B^{\\prime\\prime}$ 的。这三部分会把 $D$ 细分为 $D_1, D_2, D_3$，而如上所述，切成这三块只需要两刀，这一步的形式化说明留给读者自行补全。 不仅如此，我们还可以表明，如果 $(B, x)$ 和 $(B^\\prime, x)$ 都不能分裂某个集合，那么 $(B^{\\prime\\prime}, x)$ 也不行，这也是很简单的切蛋糕问题，证明不妨留作练习。最后一个点在初始化的地方，用 $(F, x)$ 或 $(F^c, x)$ 去切得到的结果也是完全相同的，因此二选一就行。 在这里，我们碰到了好多次选择：二选一或者三选二。怎么去选是一个计算问题，按照 Hopcraft 的原始论文，我们选择的就是较小的那一个集合。 这样我们就拿到了一个更优化的算法流程： 置 $R \\leftarrow \\{F, F^c\\}, W \\leftarrow {\\min(F, F^c)} \\times \\Sigma$； 如果 $W$ 不为空，那么从中取出一个分裂子 $(P, a)$； 接下来拿着 $(P, a)$ 寻找 $R$ 中它可以分裂的集合，对于每个可以分裂的集合 $B$，在 $R$ 中将其替换成 $(P, a)\\vert B$ 中的两个元素； 再遍历字母表 $\\Sigma$，对其中的任意字母 $x$，如果 $(B, x) \\in W$，那么把 $(B, x)$ 换成 $(B^\\prime, x)$ 和 $(B^{\\prime\\prime}, x)$；否则将 $(\\min(B^\\prime, B^{\\prime\\prime}), x)$ 添加到 $W$ 中； 回到步骤 2，直到 $W$ 为空。其中 $\\min(P, Q)$ 表示集合 $P, Q$ 中较小的一个。 这个算法的最坏复杂度为 $O(kn\\log n)$，它的推导就比较复杂了。在 David 的那篇论文中，他还证明了如果我们的输入在所有可能的 $n$ 状态完备 DFA 中均匀分布，那么 Hopcraft 算法的平均复杂度也达到 $O(n\\log n\\log n)$。关于这个算法的复杂度推导，参看 Knuutila, T.: Re-describing an Algorithm by Hopcroft. Theoret. Computer Science 250 (2001) 333–363. 我们称一个自动机对于 Hopcraft 是慢的（slow for Hopcraft），如果 Hopcraft 算法不管怎么选择，在每一步执行的时候，当前的划分中只有一个集合被裂开了。 注意到，我们这里事实上还是要对字母表进行遍历。这是一个很麻烦的操作，因为我们可以往字母表里面塞很多没有用的东西。所以事实上我们可以说，Hopcraft 算法的复杂度界的前提是这个 DFA 是完备的。这实际上是一个很糟糕的假设。Antti Valmari 和 Petri Lehtinen 在 2008 年提出了一个不完备情形下的算法，它的时间复杂度是 $O(m\\log n)$，其中 $m$ 是状态转移函数的个数，但是代价是增加了一个保存状态转移函数的分划的数据结构，见 A. Valmari and P. Lehtinen. Efficient minimization of DFAs with partial transition. In S. Albers and P. Weil, editors, STACS 2008, Proc. 25th Symp. Theoretical Aspects of Comp. Sci., volume 08001 of Dagstuhl Seminar Proceedings, pages 645–656. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2008。同年，Marie-Pierre Béal 和 Maxime Crochemore 也提出了一个 $O(m\\log n)$ 的算法，它们主要的改进在推广了取较小集合的想法、标记状态出边以及弱排序（weak sorting），见 M.-P. Béal and M. Crochemore. Minimizing incomplete automata. In Workshop on Finite State Methods and Natural Language Processing 2008. Ispra, september 2008.。 那么能不能直接计算一个语言对应的最简 DFA 呢？其实也可以。但是我们会把它留在我们最后一节 Myhill-Nerode 定理之后再作介绍。这里我们先提示一下，作商的作用远不止这点，商的结果事实上就预示了这个语言能否被一个 DFA 所识别。 诶，在这里我们停一下。我们已经有了三个算法，除了 Brzozowski 有些奇怪之后，其他的算法都是尝试从粗到细地切蛋糕。然而，为什么我们不选择用融合的方法来处理这个问题呢？也就是说，我们尝试逐步把相同的状态切掉或者融合到一起。这种方法的探索绝非罕见，但是暂时我们只有对于一些特定种类的有限状态机有所进展。要么我们要考虑无环或者只有简单环路的自动机，例如著名的 Revuz 算法受限于无环；要么我们就考虑下面的条件：局部性（locality）。 *局部性 Locality在这一节中，我们会首先探索对局部自动机的定义及其简化，然后我们引入局部语言和线性表达式的概念。这将首次将我们引入到语言、自动机和表达式千丝万缕的等价关系中来，尽管这个等价关系早已在前面无数次被暗示了。 TODO 正则语言 Regular Language接下来我们介绍语言这一条线上最重要的模型之一：正则语言（regular language）。它又被称为有理语言（rational language），在强调代数阐释的地方，我会充分解释这个名字到底表达了什么以及它有什么意义。先看它的定义，我们用递归的方式去定义它： 任何有限的语言都是正则语言； 两个正则语言的并和衔接都是正则语言； 任意正则语言的星都是正则语言； 发现了什么？回忆一下在讨论语言那一节最后的定义，我们不难证明，正则语言事实上就是有限语言生成的 Conway 半环，也就是说，它是最小的包含所有有限语言的 Conway 半环。这当然是一个非常好的性质。注意，我们说的是两个正则语言的并和衔接，这意味着有限个正则语言的并和衔接都是正则语言，但不是可数个，也不是任意个，这是一个非常关键的点。 从这里出发，我们要去叙述正则语言的性质，但是现在我们先从最简单的地方开始，怎么去表达一个正则语言？从定义中很容易获得这样的灵感：只要表达出组成它的任何一个有限语言就可以了。而有限语言当然又可以看成单元素集的有限并，因此我们首先把一个单元素集表达出来，我们令 $\\{w\\} = w$，这样的记号应当不致混淆，如果我们坚持用小写字母表示字符串，大写字母表示语言的集合。然后我们用这样的方式往下进行，只使用并、衔接和星，我们可以得到类似这样的东西，我们称之为正则表达式（regular expression）： $0(0\\cup 1)^\\ast$ 所有以 $0$ 开头的字符串； $0(01)^\\ast$ 所有以 $0$ 开头，后面都是 $01$ 交替的字符串组成的字符串； $(0(0\\cup 1)^\\ast) \\cup (1(0\\cup 1)^\\ast)$ 至少有一个元素的零一字符串； $(0 \\cup 1)(0 \\cup 1)^\\ast$ 至少有一个元素的零一字符串； 注意，第三条和第四条表达的东西是一样的。这是因为我们前面对于一般语言已经给出了几条结论，它们在正则表达式的语境下依然成立： $a\\cup b = b \\cup a$；“加法交换律” $(a\\cup b)\\cup c = a \\cup (b\\cup c)$；“加法结合律” $ea = ae = a$；“乘法单位元” $(ab)c = a(bc)$ “乘法结合律” $a(b \\cup c) = ab \\cup ac$；“左分配律” $(a\\cup b)c = ac \\cup bc$；“右分配律” $(a + b)^\\ast = a^\\ast(ba^\\ast)^\\ast$；“和-星规则” $(ab)^\\ast = e + a(ba)^\\ast a$；“积-星规则” $x^\\ast = e + xx^\\ast$；“不动点定理” 看起来到此为止了？啊哈，没那么简单。我们要问的问题是：是否存在一组有限的、完备的公理系统，使得它们能够推出所有关于正则语言的恒等式？答案是否定的。例如上面这组规则就没法推出星运算的幂等性 $(a^\\ast)^\\ast = a^\\ast$。这是一个发端于 Conway 的研究，他和 Redko 分别证明了它的不存在性。可能的无限公理系统的构造也是困难重重的，在 1966 年，Salomma （不完整地）表明存在两条公理和一组公理；Pilling 简化修正了他的结果并给出了一个对交换的正则表达式完备的系统；Redko 成功构造了单字母字母表情形下的完备公理系统；Conway 提出了一个可能的通用公理体系，最后由 D. Krob 在 1988 年将其证明。这部分证明令人惊异地用到了有限单群相关的发展，我们把它暂且留到 B 面去细讲。 这里我们还要声明一点：正则表达式事实上不等于正则语言。虽然我们在上面和下面都会不断地混同使用这两个概念，但是这两个概念在本质上是不用的，正则表达式之于正则语言就像代数表达式之于数本身。这里最简单的例子就是，两个不同的正则表达式可能对应相同的正则语言，而且由上一段介绍的结论，我们没有什么能够被称为最简的正则表达式这一说。一个正则表达式事实上是一个由 $\\Sigma \\cup \\{e, \\varnothing, (, ), +,^\\ast\\}$ 构成的良定（well-formed）的式子，在 01B 我们提到逻辑（logic）和理论（theory）这些概念时，这种定义将称为我们手中的一根救命稻草。 然后我们考察正则语言对于各种操作的封闭性。还记得吗？在第一节我们就定义了语言的各种操作，接下来我们要考虑，正则语言对于这些操作是否封闭。但是，这个过程事实上是困难的，因为集合的操作很不直观而且容易导致混乱，有些构造的式子可能会很长。因此，我们将在完成 Kleene 定理的证明之后再来处理这个问题，届时我们将用有限自动机来构造出这些操作。在这里，我们只要记住，正则语言在交、补、商下都是封闭的即可。 Kleene 定理：语言对自动机 Kleene’s Theorem: Language versus Automata接下来我们就要进入本章的一个小高潮了。在前文中已经无数次提及 Kleene 定理，它的叙述事实上是非常简单的：所有可以被有限状态机识别的语言都是正则语言，所有正则语言都可以被有限状态机识别。 我们先来看前半句话，所有可以被有限状态机识别的语言都是正则语言。事实上证明现在已经非常简单了。还记得我们在前面引入的线性方程组吗？我们已经说了，NFA 就是一个线性方程组。对于这种线性方程组，我们同样已经找到了它的解。那么，因为我们方程组里的 $K_{ij}$ 和 $L_{ij}$ 都是有限的、线性方程组的个数是有限的、求解每个方程的过程中的操作数也是有限的，当然，我们的线性方程组的解就是正则语言，这是直接由定义获得的。这是由 NFA 计算它所识别的语言的一个很好办法。 后半句话的证明要相对麻烦一点点，但是只要利用 Conway 半环的性质，我们的证明也是理所应当的。在这里我们将列出一个大致思路，具体细节请读者自行填充。 首先我们知道，所有有限语言都可以被一个 NFA 识别（为什么？只需要构造一个无环的 NFA 就可以了）。然后，如果两个语言都能被 NFA 识别，它们的并也能被一个 NFA 识别（为什么？构造一个 NFA-$e$！），同样它们的衔接也能被一个 NFA 识别（为什么？自己构造一下！）。最后，如果一个语言能被 NFA 识别，它的星也能被 NFA 识别（也是构造 NFA-$e$！） 好，完成了。这里我们看起来讲的十分粗略，但其实它应该是挺顺理成章的。这些构造一个一个的都不困难也很直接，麻烦的反向证明则被我们用线性方程组解决掉了。所以，Kleene 定理的证明只占这么小的篇幅还是很合理的。 这样的话，上一节留下的问题也很快解决了。怎么构造交和补？这应该很显然。我们这里以商作为例子，因为课程中没讲这个。同样，我们也要展示一下怎么将这种构造性证明形式化地写出来。 设 $\\mathcal M = (K, \\Sigma, \\delta, q_0, F)$ 是一个识别某语言 $L$ 的 DFA，那么下面说明，对于任意 $u \\in \\Sigma^\\ast$，$u^{-1}L$ 可以被 $\\mathcal{M}^\\prime = (K, \\Sigma, \\delta, Q, F)$ 这个 NFA 识别。注意，我们既然已经讲了 DFA，NFA，NFA-$e$ 的等价性，自然我们也就可以直接取用和构造三种中的任意一种，只要方便就行。 好，接下来的想法就是怎么构造了。我们上面的定义已经说明了问题，令： Q=\\delta(q_0, u)问题就解决了。当然，其实这是一个 DFA 而不是 NFA，但假设的时候宽泛一点总没错。接下来我们要证明，如果还有任意 $\\Sigma$ 上的语言 $P$（它甚至不一定要正则，因为商特殊的不对称性），那么我们也能类似构造 $\\mathcal M^\\prime$ 识别 $P^{-1}L$。 这里的关键在于，我们不需要一台自动机来判定 $P$，因为 $P$ 是我们已经有的，它在构造的时候被“硬编码”进了新的自动机。想必大家已经有想法了，我们新的起始状态就是所有从 $q_0$ 出发经过一个标号为 $K$ 中的元素的路径到达的状态构成的集合。 我们既然声称这样的自动机识别语言 $P^{-1}L$，那么当然需要证明它。首先，任取 $u \\in K^{-1}L$，由商的定义，存在 $x \\in P$ 使得 $xu \\in L$。所以，在自动机 $\\mathcal M$ 中有一条接受路径的标号是 $xu$，这就意味着存在状态 $q$ 和终止状态 $r$，使得 $q_0 \\stackrel x \\rightarrow q \\stackrel u \\rightarrow r$。那么根据我们的构造，$q$ 是自动机 $\\mathcal{M}^\\prime$ 的起始状态，$r$ 是自动机 $\\mathcal M^\\prime$ 的终止状态，我们自然也就获得了一条 $\\mathcal M^\\prime$ 中的接受路径。 反过来，任取一个被 $\\mathcal M^\\prime$ 接受的字符串 $u$，它的接受路径为 $q\\stackrel u \\rightarrow r$。而根据我们对 $\\mathcal M^\\prime$ 的构造，我们知道 $\\exists x \\in P, q_0 \\stackrel x \\rightarrow q$。这就意味着 $u \\in P^{-1}L$。 我们给出最后一个封闭性相关的结论：一个正则语言被一个幺半群同态映过去的语言还是正则语言。我们的幺半群同态指的是 $\\Sigma^\\ast$ 到 $\\Sigma^{\\prime\\ast}$ 的同态，幺半群同态的概念在上面已经讲过了。但是在这里，我们还只能证明正则语言对同态的逆映射封闭，也就是说，我们将一个语言同态地映到另一个语言，如果映射的结果是正则语言，那原来的语言也是正则语言。这个证明可以由自动机完成，留给读者作为练习。 结束了。接下来我们用最后几句话来给这半部分收个尾：给定正则语言 $L$，我们根据它对应的自动机可以判断一个语言是否属于它，它是否为空，它是否有限或无限。在最后的一节中，我们将回收简化那一节最后的一个坑：如何将一个正则语言转化为识别它的最简自动机。 泵引理 Pumping Lemma这是我们课上讲的最后一部分内容了。这里要研究的是，怎么单从字符串的角度去识别一个语言是不是正则语言？在这里，事实上我们只能给出一个必要条件：我们知道正则语言一定长什么样，但我们不确定满足这个条件的一定是正则语言。 在这里我们需要提醒一件事情：哪怕只是研究字符串，我们也能得出很多好玩的结论，它们很多看起来比较显然，但可能也有很多不那么显然的东西。在这里我要举一些例子，它们在某种意义上可以看成泵定理的前身。但是为了简明起见，这里仅罗列结论，证明留给读者，应当都是“非常显然的”。 Levi 引理：字符串 $u, v, x, y \\in \\Sigma^\\ast$ 满足 $uv=xy$。若 $\\vert u \\vert \\geqslant \\vert x \\vert$，则存在 $t \\in \\Sigma^\\ast$ 使得 $u=xt$ 且 $y=tv$。若 $\\vert u \\vert &lt; \\vert x\\vert$，那么存在 $t \\in \\Sigma^+$ 使得 $x=ut$ 且 $v=ty$。 Lyndon-Schützenberger 第一定理：设 $y \\in \\Sigma^\\ast$，$x, z \\in \\Sigma^+$。则 $xy=yz$ 当且仅当存在 $u, v \\in \\Sigma^\\ast$ 和正整数 $e$ 使得 $x=uv, z=vu, y=(uv)^eu=u(vu)^e$。 Lyndon-Schützenberger 第二定理：设 $x, y \\in \\Sigma^+$，则以下三个条件等价： $xy=yx$； 存在正整数 $i, j$ 使得 $x^i = y^j$； 存在 $z \\in \\Sigma^+$ 和正整数 $k, l$ 使得 $x=z^k, y=z^l$。 Fine-Wilf 定理：设 $a, b$ 为两个无限长字符串，周期分别为 $m, n \\geqslant 1$，如果两者在长度 $m + n - \\text{gcd}(m, n)$ 的前缀上相同，那么 $a=b$。简单吧？看上去都是显然的结果，后面的内容不会显式地用到这些东西，毕竟我们这是计算理论而不是字符串学（stringology）。但是我们会发现，这些东西都会在我们后面的证明中如同草蛇灰线一般贯穿始终，当我们说某件事情显然时，往往就会用到其中的某些东西，它们可以被直觉所肯定，但是要寻求详细证明的话还得回到这里。 好，回归正题，我们来讨论泵引理。泵引理最重要的意义就在于它能告诉你正则语言到底长啥样。事实上，不仅正则语言如此，后面谈到 CFG 时也会有它的一个对应版本。设 $L$ 为一正则语言，那么存在正整数 $n$ 使得 $\\forall u \\in L, \\vert u \\vert \\geqslant n$ 都可以被表示成 $u=xyz$，其中 $x, y, z \\in \\Sigma^\\ast$，$\\vert xy\\vert \\leqslant n$，$y \\ne e$ 且 $\\forall k \\geqslant 0, xy^kz \\in L$。这个长度 $n$ 往往也被我们称为一个语言的泵长度（pumping length）。 很显然，对吧？嗯，在上一节的基础上这确实是显然的。因为上一节末尾我们说了，一个无限的语言在自动机中的表现是环路。我们只要找到第一个环路的地方，环路前面的串是 $x$，环路中的是 $y$，而出环之后的任意路径都可以为 $z$。当然，我们还可以很轻松搞出一个变体：找一个 $yz$ 长度受限的分解也是可以的嘛。诶，那为什么 $y$ 不能是空串呢？我们知道，对于无环的自动机，当然它对应的语言就是有限的，这时候它的泵长度就是这个语言中最长串的长度加一：没有这样的词也是可以的嘛。 注意，这个定理的核心是什么？它的核心就是有限状态机的有限性。也就是说，它不能有无穷多个状态。这个观点在 01B 中可以给我们另外一个判断一个语言是不是正则的标准：直接算它的单词的 Kolmogorov 复杂度界。 这个引理最常用的作用就是证明一个语言不是正则的。这当然是一个很常用的套路，也有一个很标准的例子：证明 $\\{0^n1^n:n\\geqslant 0\\}$ 不是正则的。 解决这种问题的方法也很简单，反证。假设它是正则的，我们去套泵引理。存在正整数 $n$ 使得 $\\forall u \\in L, \\vert u \\vert \\geqslant n$ 都可以被表示成 $u=xyz$，其中 $x, y, z \\in \\Sigma^\\ast$，$\\vert xy\\vert \\leqslant n$，$y \\ne e$ 且 $\\forall k \\geqslant 0, xy^kz \\in L$。既然它是任意的，我们就取一个 $u$ 来分析。最单纯的想法就是找个 $u=0^n1^n$，我们把它分解成 $xyz$。既然 $\\vert xy \\vert \\leqslant n$ 那么当然 $x$ 和 $y$ 都是一串 $0$。记 $x = 0^r, y=0^s$，$r$ 为非负数，$s$ 为正整数。注意，这里用了 $y \\ne e$ 的条件。然后呢，我们去构造 $xy^kz$。同样是任意的，那我们就取一个 $k$ 来分析。设 $k=2$，很显然 $0$ 的个数就比 $1$ 的个数多 $s$ 了，那就错了，$xy^2z \\not \\in L$，不满足泵引理，说明它不是正则的。 下面我们来举一个失败的例子：一个语言它符合泵引理，但是他不是正则的。我们的语言长这样：$L=\\{(ab)^nc^n:n&gt;0\\} \\cup ((a\\cup b\\cup c)^\\ast bb(a\\cup b\\cup c)^\\ast )\\cup ((a\\cup b\\cup c)^\\ast aa (a\\cup b\\cup c)^\\ast)$。呼诶，这玩意看上去不太好惹。我们这里只表明它符合泵引理，证明它不是正则要留到以后我们有了更趁手的工具之后再来干。直觉地说，证明它不是正则就是没有哪个自动机能保证我在一段任意长的单词之中都会有一个 $aa$ 或者 $bb$。 来吧，我们看看这玩意为什么不能用泵引理。从后面两个集合出发显然不是明智的，因为既然它有一段 $\\Sigma^\\ast$ 的东西，那么这段东西里什么都可以有。从第一个集合出发呢？很不幸，我们第一个集合出发构造任意长度的字符串都会被包含在后面两个字符串中。准此思路，我们可以对泵长度完成递归来证明随便取什么泵长度，我们都能找到这种分解，这个过程有些繁杂，不妨由读者来自行完成。 看起来这一节要结束了是吧？我们已经叙述完了泵引理，讲完了怎么用它证明一个语言不是正则的，并且说明了它不是个充要条件。接下来还能讲什么呢？ 到现在为止，我们真的只有这一个办法证明语言不是正则的吗？事实上不止。接下来的一个方法是，利用正则语言的定义。一个非常简单的例子是，证明由所有 $01$ 个数相同的 $01$ 字符串构成的语言不是正则的。很简单，很友好，和 $0^\\ast 1^\\ast$ 交一下，交出来的东西不是正则的，所以它就不是正则的。 还有一个很有用的封闭性，我们在上面讲到了反同态的封闭性，这个东西也是可以利用的。我们这里只能给一个看起来不那么漂亮的例子，它用泵引理也能证明：$\\{(01)^n 2^n:n&gt;0\\}$。我们把定义同态 $h(a)=01, h(b)=2$，然后得到反同态为 $\\{a^nb^n:n&gt;0\\}$，这个东西显然不是正则的。思考，怎么直接用泵引理证明这个东西不是正则的？ 最后，我们给出一个泵引理相关的充要条件，这是 J. Jeffe 在 1978 年提出的，不知道为什么现在还没有进入教科书……哦我们的教科书也是这个年代的，那没事了。 $L$ 为一正则语言当且仅当存在正整数 $n$ 使得 $\\forall u \\in L, \\vert u \\vert = n$ 都可以被表示成 $u=xyz$，其中 $x, y, z \\in \\Sigma^\\ast$，$y \\ne e$ 且 $\\forall k \\geqslant 0, v \\in \\Sigma^\\ast$，我们有： $uv \\in L \\Rightarrow xy^kzv\\in L$； $uv\\not \\in L \\Rightarrow xy^kzv \\not \\in L$。 很简单的条件，对吧？这个定理的证明甚至不需要用到 Myhill-Nerode 定理。很直觉的想法就是，依然把它切成第一个环和前面以及第一个环之后的部分，第一个环之后的部分需要我们做出更加细致的描述。第一个环无论重复多少遍都不影响它后面的任何改动，这又是状态机这个概念带给我们的自信。详细的证明留给读者，应当不难。 最后我们将罗列一些还没讲到的证明方式，我们会在本文后面讲到的有： Myhill-Nerode 定理：代数方法给出的充要条件； 生成函数法：分析组合学视角； 基于转导（transduction）的方法：更多的矩阵运算和交换图；在 01B 中会讲到的： Büchi-Elgot-Trakhtenbrot 定理、Rabin 树定理：一元二阶逻辑给出的充要条件； 基于准序（quasi-order）的方法； 基于 Kolmogorov 复杂度界的方法：信息论视角。以及以后 CFG 和线性语言（linear language）讲完之后会出现的新必要条件。 *正则语法 Regular Grammar有语言，那自然可以有语法。在泵定理一节中，我们已经探索了一种表达模式。接下来，我们将尝试形式化一个语言的“形成过程”。这一节更多地是为后面两三章的内容做一个简单的铺垫而不是后面几节，可以看成一个插曲，但是其中遇到的一些思想还是很有意思的。 TODO *无星语言 Star-free Language接下来我们要进入下半部分的讨论。我们上面讨论的正则语言对于开头来讲还是太复杂了，因此我们再将其简化，把星这个看起来很麻烦的运算去掉，得到无星语言（star-free language）的概念。然后我们将引入无周期性的幺半群（aperiodic monoid）的概念，最后由句法幺半群（syntactic monoid）这个我们铺垫了很久的概念来把它们联系起来，从而给出 Schützenberger 定理。 TODO *代数刻画 An Algebraic Approach这一节将会成为整篇文章最后的高潮。我们将会给出幺半群和自动机关系的完整刻画，进而给出 Myhill-Nerode 定理作为正则语言的一个充要条件，进而将语言、自动机、幺半群三个概念彻底融合在一起。最后，我们将引入句法代数（syntactic algebra）的概念并且讨论一些关于有理性（rationality）的结论，这将为我们在 01B 的讨论打下坚实的基础。 TODO 总结 Conclusion TODO 参考资料 References TODO：细化，分类，标准化，补充完整 Mathematical Foundations of Automata Theory, Jean-Éric Pin, Version of 18th Feb. 2022Verification of Reactive Systems: Formal Methods and Algorithms, Prof. Dr. Klaus Schneider (auth.), 2004, Springer, Texts in Theoretical Computer Science. An EATCS SeriesTreatment of $\\varepsilon$-Moves in Subset Construction, 2000, Van Noord, Gertjan, _Computational Linguistics_. 26 (1): 61–76.On the bounds for state-set size in the proofs of equivalence between deterministic, nondeterministic, and two-way finite automata, 1971, Moore, Frank R.Regular Algebra and Finite Machines, 1971, John H. Conway, Chapman &amp; HallOn rational series and rational languages, 1998, HuaXiong Wang, Theoretical Computer Science 205 (1998) 329-336Partial Conway and iteration semirings, 2007, S. L. Bloom, Z. Ésik, W. KuichSome remarks on semirings and their ideals, 2018, Peyman Nasehpour, Asian-European Journal of Mathematics 12(07)A complete system of B-rational identities,1990, Krob, D. In: Paterson, M.S. (eds) Automata, Languages and Programming. ICALP 1990. Lecture Notes in Computer Science, vol 443. Springer, Berlin, Heidelberg.Minimization of automata, 2011, Jean Berstel, Luc Boasson, Olivier Carton, Isabelle FagnotElements of automata theory, 2009, J. Sakarovitch, CambridgeAlgebra-Coalgebra Duality in Brzozowski’s Minimization Algorithm, 2014, Bonchi, Bonsangue, Rutten and Silva, ACM Transactions on Computational Logic 15, vol 1J. Jaffe, A necessary and sufficient pumping lemma for regular languages, _Sigact News - SIGACT_ 10 (1978) 48-49.Jean-Eric Pin, Jacques Sakarovitch. Some operations and transductions that preserve rationality. _6th GI Conference_, 1983, Berlin, Germany. pp.277-288.","link":"/blog/62bbbd2e/"}],"tags":[{"name":"数学","slug":"数学","link":"/blog/tags/%E6%95%B0%E5%AD%A6/"},{"name":"抽象代数","slug":"抽象代数","link":"/blog/tags/%E6%8A%BD%E8%B1%A1%E4%BB%A3%E6%95%B0/"},{"name":"群论","slug":"群论","link":"/blog/tags/%E7%BE%A4%E8%AE%BA/"},{"name":"源码阅读","slug":"源码阅读","link":"/blog/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"},{"name":"程序分析","slug":"程序分析","link":"/blog/tags/%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90/"},{"name":"范畴论","slug":"范畴论","link":"/blog/tags/%E8%8C%83%E7%95%B4%E8%AE%BA/"},{"name":"计算复杂性理论","slug":"计算复杂性理论","link":"/blog/tags/%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E6%80%A7%E7%90%86%E8%AE%BA/"},{"name":"量子计算","slug":"量子计算","link":"/blog/tags/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97/"},{"name":"计算理论","slug":"计算理论","link":"/blog/tags/%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA/"},{"name":"复杂度","slug":"复杂度","link":"/blog/tags/%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"name":"概率论","slug":"概率论","link":"/blog/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"},{"name":"信息论","slug":"信息论","link":"/blog/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"},{"name":"计算机图形学","slug":"计算机图形学","link":"/blog/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"categories":[{"name":"课程笔记","slug":"课程笔记","link":"/blog/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"},{"name":"源码阅读笔记","slug":"源码阅读笔记","link":"/blog/categories/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"name":"抽象代数续论","slug":"课程笔记/抽象代数续论","link":"/blog/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E6%8A%BD%E8%B1%A1%E4%BB%A3%E6%95%B0%E7%BB%AD%E8%AE%BA/"},{"name":"论文笔记","slug":"论文笔记","link":"/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},{"name":"GDB 源码分析","slug":"源码阅读笔记/GDB-源码分析","link":"/blog/categories/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/GDB-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"量子算法与编程暑期讲习班","slug":"课程笔记/量子算法与编程暑期讲习班","link":"/blog/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E9%87%8F%E5%AD%90%E7%AE%97%E6%B3%95%E4%B8%8E%E7%BC%96%E7%A8%8B%E6%9A%91%E6%9C%9F%E8%AE%B2%E4%B9%A0%E7%8F%AD/"},{"name":"计算理论","slug":"论文笔记/计算理论","link":"/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA/"},{"name":"量子编程","slug":"课程笔记/量子算法与编程暑期讲习班/量子编程","link":"/blog/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E9%87%8F%E5%AD%90%E7%AE%97%E6%B3%95%E4%B8%8E%E7%BC%96%E7%A8%8B%E6%9A%91%E6%9C%9F%E8%AE%B2%E4%B9%A0%E7%8F%AD/%E9%87%8F%E5%AD%90%E7%BC%96%E7%A8%8B/"},{"name":"Kempe(2005)","slug":"论文笔记/计算理论/Kempe-2005","link":"/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA/Kempe-2005/"},{"name":"Martin-Löf(1966)","slug":"论文笔记/计算理论/Martin-Lof-1966","link":"/blog/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA/Martin-Lof-1966/"},{"name":"没卵用的教程","slug":"没卵用的教程","link":"/blog/categories/%E6%B2%A1%E5%8D%B5%E7%94%A8%E7%9A%84%E6%95%99%E7%A8%8B/"},{"name":"量子计算复杂性","slug":"课程笔记/量子算法与编程暑期讲习班/量子计算复杂性","link":"/blog/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/%E9%87%8F%E5%AD%90%E7%AE%97%E6%B3%95%E4%B8%8E%E7%BC%96%E7%A8%8B%E6%9A%91%E6%9C%9F%E8%AE%B2%E4%B9%A0%E7%8F%AD/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E6%80%A7/"},{"name":"计算理论","slug":"计算理论","link":"/blog/categories/%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA/"},{"name":"Vulkan","slug":"没卵用的教程/Vulkan","link":"/blog/categories/%E6%B2%A1%E5%8D%B5%E7%94%A8%E7%9A%84%E6%95%99%E7%A8%8B/Vulkan/"}]}